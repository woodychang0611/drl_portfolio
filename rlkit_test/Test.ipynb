{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import rlkit.torch.pytorch_util as ptu\n",
    "from rlkit.data_management.env_replay_buffer import EnvReplayBuffer\n",
    "from rlkit.envs.wrappers import NormalizedBoxEnv\n",
    "from rlkit.launchers.launcher_util import setup_logger\n",
    "from rlkit.samplers.data_collector import MdpPathCollector\n",
    "from rlkit.torch.sac.policies import TanhGaussianPolicy, MakeDeterministic\n",
    "from rlkit.torch.sac.sac import SACTrainer\n",
    "from rlkit.torch.networks import ConcatMlp\n",
    "from rlkit.torch.torch_rl_algorithm import TorchBatchRLAlgorithm\n",
    "\n",
    "\n",
    "def experiment(variant):\n",
    "    print(\"Using GPU?: \" + str(ptu.gpu_enabled()))\n",
    "    expl_env = NormalizedBoxEnv(gym.make(\"MountainCarContinuous-v0\"))\n",
    "    eval_env = NormalizedBoxEnv(gym.make(\"MountainCarContinuous-v0\"))\n",
    "    obs_dim = expl_env.observation_space.low.size\n",
    "    action_dim = eval_env.action_space.low.size\n",
    "\n",
    "    M = variant['layer_size']\n",
    "    qf1 = ConcatMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        # First layer nn.Linear(input_size,hidden_sizes[0])\n",
    "        # 2nd layer nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        # Bias and initial Linear weights set with\n",
    "        #  ptu.fanin_init and .1\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    qf2 = ConcatMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    target_qf1 = ConcatMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    target_qf2 = ConcatMlp(\n",
    "        input_size=obs_dim + action_dim,\n",
    "        output_size=1,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    policy = TanhGaussianPolicy(\n",
    "        obs_dim=obs_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_sizes=[M, M],\n",
    "    )\n",
    "    eval_policy = MakeDeterministic(policy)\n",
    "    eval_path_collector = MdpPathCollector(\n",
    "        eval_env,\n",
    "        eval_policy,\n",
    "    )\n",
    "    expl_path_collector = MdpPathCollector(\n",
    "        expl_env,\n",
    "        policy,\n",
    "    )\n",
    "    replay_buffer = EnvReplayBuffer(\n",
    "        variant['replay_buffer_size'],\n",
    "        expl_env,\n",
    "    )\n",
    "    trainer = SACTrainer(\n",
    "        env=eval_env,\n",
    "        policy=policy,\n",
    "        qf1=qf1,\n",
    "        qf2=qf2,\n",
    "        target_qf1=target_qf1,\n",
    "        target_qf2=target_qf2,\n",
    "        **variant['trainer_kwargs']\n",
    "    )\n",
    "    algorithm = TorchBatchRLAlgorithm(\n",
    "        trainer=trainer,\n",
    "        exploration_env=expl_env,\n",
    "        evaluation_env=eval_env,\n",
    "        exploration_data_collector=expl_path_collector,\n",
    "        evaluation_data_collector=eval_path_collector,\n",
    "        replay_buffer=replay_buffer,\n",
    "        **variant['algorithm_kwargs']\n",
    "    )\n",
    "    algorithm.to(ptu.device)\n",
    "    algorithm.train()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:20:18.550910 UTC | Variant:\n",
      "2021-03-01 09:20:18.551866 UTC | {\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"version\": \"normal\",\n",
      "  \"layer_size\": 256,\n",
      "  \"replay_buffer_size\": 1000000,\n",
      "  \"algorithm_kwargs\": {\n",
      "    \"num_epochs\": 3000,\n",
      "    \"num_eval_steps_per_epoch\": 5000,\n",
      "    \"num_trains_per_train_loop\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"batch_size\": 256\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.0003,\n",
      "    \"qf_lr\": 0.0003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  }\n",
      "}\n",
      "Using GPU?: True\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 09:21:02.945961 UTC | [] Epoch 0 finished\n",
      "----------------------------------  --------------\n",
      "replay_buffer/size                  2000\n",
      "trainer/num train calls             1000\n",
      "trainer/QF1 Loss                       0.432287\n",
      "trainer/QF2 Loss                       0.432451\n",
      "trainer/Policy Loss                   -0.677446\n",
      "trainer/Q1 Predictions Mean            0.000107123\n",
      "trainer/Q1 Predictions Std             6.85158e-05\n",
      "trainer/Q1 Predictions Max             0.000292433\n",
      "trainer/Q1 Predictions Min             3.27804e-06\n",
      "trainer/Q2 Predictions Mean           -2.56457e-05\n",
      "trainer/Q2 Predictions Std             4.39631e-05\n",
      "trainer/Q2 Predictions Max             7.51978e-05\n",
      "trainer/Q2 Predictions Min            -0.000127115\n",
      "trainer/Q Targets Mean                 0.61744\n",
      "trainer/Q Targets Std                  0.226243\n",
      "trainer/Q Targets Max                  2.08988\n",
      "trainer/Q Targets Min                  0.172897\n",
      "trainer/Log Pis Mean                  -0.67747\n",
      "trainer/Log Pis Std                    0.231006\n",
      "trainer/Log Pis Max                   -0.267993\n",
      "trainer/Log Pis Min                   -1.76728\n",
      "trainer/policy/mean Mean              -8.03936e-06\n",
      "trainer/policy/mean Std                2.8132e-06\n",
      "trainer/policy/mean Max               -4.15323e-07\n",
      "trainer/policy/mean Min               -1.63604e-05\n",
      "trainer/policy/normal/std Mean         0.999237\n",
      "trainer/policy/normal/std Std          1.29855e-05\n",
      "trainer/policy/normal/std Max          0.999274\n",
      "trainer/policy/normal/std Min          0.9992\n",
      "trainer/policy/normal/log_std Mean    -0.000763697\n",
      "trainer/policy/normal/log_std Std      1.29955e-05\n",
      "trainer/policy/normal/log_std Max     -0.000726129\n",
      "trainer/policy/normal/log_std Min     -0.000800632\n",
      "trainer/Alpha                          1\n",
      "trainer/Alpha Loss                    -0\n",
      "exploration/num steps total         2000\n",
      "exploration/num paths total            4\n",
      "exploration/path length Mean         500\n",
      "exploration/path length Std          499\n",
      "exploration/path length Max          999\n",
      "exploration/path length Min            1\n",
      "exploration/Rewards Mean              -0.0385713\n",
      "exploration/Rewards Std                0.0308654\n",
      "exploration/Rewards Max               -6.1229e-08\n",
      "exploration/Rewards Min               -0.0997857\n",
      "exploration/Returns Mean             -19.2856\n",
      "exploration/Returns Std               19.193\n",
      "exploration/Returns Max               -0.092638\n",
      "exploration/Returns Min              -38.4787\n",
      "exploration/Actions Mean              -0.0123479\n",
      "exploration/Actions Std                0.620935\n",
      "exploration/Actions Max                0.997386\n",
      "exploration/Actions Min               -0.998928\n",
      "exploration/Num Paths                  2\n",
      "exploration/Average Returns          -19.2856\n",
      "evaluation/num steps total          4995\n",
      "evaluation/num paths total             5\n",
      "evaluation/path length Mean          999\n",
      "evaluation/path length Std             0\n",
      "evaluation/path length Max           999\n",
      "evaluation/path length Min           999\n",
      "evaluation/Rewards Mean               -5.81415e-12\n",
      "evaluation/Rewards Std                 7.18156e-13\n",
      "evaluation/Rewards Max                -4.3773e-12\n",
      "evaluation/Rewards Min                -7.36691e-12\n",
      "evaluation/Returns Mean               -5.80833e-09\n",
      "evaluation/Returns Std                 3.58295e-11\n",
      "evaluation/Returns Max                -5.76425e-09\n",
      "evaluation/Returns Min                -5.84201e-09\n",
      "evaluation/Actions Mean               -7.60986e-06\n",
      "evaluation/Actions Std                 4.71397e-07\n",
      "evaluation/Actions Max                -6.60653e-06\n",
      "evaluation/Actions Min                -8.59557e-06\n",
      "evaluation/Num Paths                   5\n",
      "evaluation/Average Returns            -5.80833e-09\n",
      "time/data storing (s)                  0.00509503\n",
      "time/evaluation sampling (s)           7.48808\n",
      "time/exploration sampling (s)          1.71648\n",
      "time/logging (s)                       0.0176752\n",
      "time/sac training (s)                 24.5117\n",
      "time/saving (s)                        0.00793365\n",
      "time/training (s)                      4.55361e-05\n",
      "time/epoch (s)                        33.747\n",
      "time/total (s)                        56.0237\n",
      "Epoch                                  0\n",
      "----------------------------------  --------------\n",
      "2021-03-01 09:21:39.516654 UTC | [] Epoch 1 finished\n",
      "----------------------------------  --------------\n",
      "replay_buffer/size                  3000\n",
      "trainer/num train calls             2000\n",
      "trainer/QF1 Loss                       0.00653928\n",
      "trainer/QF2 Loss                       0.00654269\n",
      "trainer/Policy Loss                   -2.7417\n",
      "trainer/Q1 Predictions Mean            2.2206\n",
      "trainer/Q1 Predictions Std             0.0417893\n",
      "trainer/Q1 Predictions Max             2.33096\n",
      "trainer/Q1 Predictions Min             2.11393\n",
      "trainer/Q2 Predictions Mean            2.22107\n",
      "trainer/Q2 Predictions Std             0.0417138\n",
      "trainer/Q2 Predictions Max             2.33207\n",
      "trainer/Q2 Predictions Min             2.11732\n",
      "trainer/Q Targets Mean                 2.20073\n",
      "trainer/Q Targets Std                  0.0852137\n",
      "trainer/Q Targets Max                  2.55002\n",
      "trainer/Q Targets Min                  1.99894\n",
      "trainer/Log Pis Mean                  -0.692267\n",
      "trainer/Log Pis Std                    0.147692\n",
      "trainer/Log Pis Max                   -0.546544\n",
      "trainer/Log Pis Min                   -2.03223\n",
      "trainer/policy/mean Mean               0.0101907\n",
      "trainer/policy/mean Std                0.000128028\n",
      "trainer/policy/mean Max                0.0105422\n",
      "trainer/policy/mean Min                0.00983877\n",
      "trainer/policy/normal/std Mean         0.846934\n",
      "trainer/policy/normal/std Std          0.0034145\n",
      "trainer/policy/normal/std Max          0.856676\n",
      "trainer/policy/normal/std Min          0.837066\n",
      "trainer/policy/normal/log_std Mean    -0.166141\n",
      "trainer/policy/normal/log_std Std      0.00403075\n",
      "trainer/policy/normal/log_std Max     -0.154696\n",
      "trainer/policy/normal/log_std Min     -0.177853\n",
      "trainer/Alpha                          0.740713\n",
      "trainer/Alpha Loss                    -0.50792\n",
      "exploration/num steps total         3000\n",
      "exploration/num paths total            6\n",
      "exploration/path length Mean         500\n",
      "exploration/path length Std          499\n",
      "exploration/path length Max          999\n",
      "exploration/path length Min            1\n",
      "exploration/Rewards Mean              -0.0343337\n",
      "exploration/Rewards Std                0.0291081\n",
      "exploration/Rewards Max               -2.7325e-08\n",
      "exploration/Rewards Min               -0.0992706\n",
      "exploration/Returns Mean             -17.1668\n",
      "exploration/Returns Std               17.1114\n",
      "exploration/Returns Max               -0.0554666\n",
      "exploration/Returns Min              -34.2782\n",
      "exploration/Actions Mean              -0.0206355\n",
      "exploration/Actions Std                0.585586\n",
      "exploration/Actions Max                0.996346\n",
      "exploration/Actions Min               -0.987967\n",
      "exploration/Num Paths                  2\n",
      "exploration/Average Returns          -17.1668\n",
      "evaluation/num steps total          9990\n",
      "evaluation/num paths total            10\n",
      "evaluation/path length Mean          999\n",
      "evaluation/path length Std             0\n",
      "evaluation/path length Max           999\n",
      "evaluation/path length Min           999\n",
      "evaluation/Rewards Mean               -1.03692e-05\n",
      "evaluation/Rewards Std                 2.65536e-08\n",
      "evaluation/Rewards Max                -1.03111e-05\n",
      "evaluation/Rewards Min                -1.04272e-05\n",
      "evaluation/Returns Mean               -0.0103588\n",
      "evaluation/Returns Std                 1.30071e-07\n",
      "evaluation/Returns Max                -0.0103586\n",
      "evaluation/Returns Min                -0.010359\n",
      "evaluation/Actions Mean                0.0101829\n",
      "evaluation/Actions Std                 1.30384e-05\n",
      "evaluation/Actions Max                 0.0102114\n",
      "evaluation/Actions Min                 0.0101544\n",
      "evaluation/Num Paths                   5\n",
      "evaluation/Average Returns            -0.0103588\n",
      "time/data storing (s)                  0.00496171\n",
      "time/evaluation sampling (s)           7.81589\n",
      "time/exploration sampling (s)          1.80934\n",
      "time/logging (s)                       0.0195678\n",
      "time/sac training (s)                 24.9603\n",
      "time/saving (s)                        0.0555122\n",
      "time/training (s)                      5.82598e-05\n",
      "time/epoch (s)                        34.6656\n",
      "time/total (s)                        92.5341\n",
      "Epoch                                  1\n",
      "----------------------------------  --------------\n",
      "2021-03-01 09:22:16.319880 UTC | [] Epoch 2 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                   4000\n",
      "trainer/num train calls              3000\n",
      "trainer/QF1 Loss                        0.00350468\n",
      "trainer/QF2 Loss                        0.00342908\n",
      "trainer/Policy Loss                    -4.20071\n",
      "trainer/Q1 Predictions Mean             3.81772\n",
      "trainer/Q1 Predictions Std              0.0407474\n",
      "trainer/Q1 Predictions Max              3.91934\n",
      "trainer/Q1 Predictions Min              3.69185\n",
      "trainer/Q2 Predictions Mean             3.82039\n",
      "trainer/Q2 Predictions Std              0.0411565\n",
      "trainer/Q2 Predictions Max              3.92037\n",
      "trainer/Q2 Predictions Min              3.69262\n",
      "trainer/Q Targets Mean                  3.83404\n",
      "trainer/Q Targets Std                   0.0734569\n",
      "trainer/Q Targets Max                   4.07195\n",
      "trainer/Q Targets Min                   3.63874\n",
      "trainer/Log Pis Mean                   -0.693449\n",
      "trainer/Log Pis Std                     0.136528\n",
      "trainer/Log Pis Max                    -0.575237\n",
      "trainer/Log Pis Min                    -1.74729\n",
      "trainer/policy/mean Mean                0.00315336\n",
      "trainer/policy/mean Std                 0.000207927\n",
      "trainer/policy/mean Max                 0.00384993\n",
      "trainer/policy/mean Min                 0.00274763\n",
      "trainer/policy/normal/std Mean          0.83557\n",
      "trainer/policy/normal/std Std           0.00187253\n",
      "trainer/policy/normal/std Max           0.840528\n",
      "trainer/policy/normal/std Min           0.830018\n",
      "trainer/policy/normal/log_std Mean     -0.179644\n",
      "trainer/policy/normal/log_std Std       0.00224204\n",
      "trainer/policy/normal/log_std Max      -0.173725\n",
      "trainer/policy/normal/log_std Min      -0.186308\n",
      "trainer/Alpha                           0.548729\n",
      "trainer/Alpha Loss                     -1.01632\n",
      "exploration/num steps total          4000\n",
      "exploration/num paths total             8\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0329662\n",
      "exploration/Rewards Std                 0.028905\n",
      "exploration/Rewards Max                -6.24492e-07\n",
      "exploration/Rewards Min                -0.0988379\n",
      "exploration/Returns Mean              -16.4831\n",
      "exploration/Returns Std                16.39\n",
      "exploration/Returns Max                -0.0931108\n",
      "exploration/Returns Min               -32.8731\n",
      "exploration/Actions Mean                0.00116828\n",
      "exploration/Actions Std                 0.57416\n",
      "exploration/Actions Max                 0.990498\n",
      "exploration/Actions Min                -0.994172\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns           -16.4831\n",
      "evaluation/num steps total          14985\n",
      "evaluation/num paths total             15\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -9.9224e-07\n",
      "evaluation/Rewards Std                  4.41869e-08\n",
      "evaluation/Rewards Max                 -9.11637e-07\n",
      "evaluation/Rewards Min                 -1.07681e-06\n",
      "evaluation/Returns Mean                -0.000991248\n",
      "evaluation/Returns Std                  5.89321e-07\n",
      "evaluation/Returns Max                 -0.000990616\n",
      "evaluation/Returns Min                 -0.00099197\n",
      "evaluation/Actions Mean                 0.0031492\n",
      "evaluation/Actions Std                  7.01436e-05\n",
      "evaluation/Actions Max                  0.00328148\n",
      "evaluation/Actions Min                  0.00301931\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.000991248\n",
      "time/data storing (s)                   0.00509225\n",
      "time/evaluation sampling (s)            7.78758\n",
      "time/exploration sampling (s)           1.81759\n",
      "time/logging (s)                        0.0186468\n",
      "time/sac training (s)                  25.0524\n",
      "time/saving (s)                         0.00691439\n",
      "time/training (s)                       5.51287e-05\n",
      "time/epoch (s)                         34.6883\n",
      "time/total (s)                        129.322\n",
      "Epoch                                   2\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:22:53.140493 UTC | [] Epoch 3 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                   5000\n",
      "trainer/num train calls              4000\n",
      "trainer/QF1 Loss                        0.00359836\n",
      "trainer/QF2 Loss                        0.00356717\n",
      "trainer/Policy Loss                    -5.19242\n",
      "trainer/Q1 Predictions Mean             4.91633\n",
      "trainer/Q1 Predictions Std              0.0376821\n",
      "trainer/Q1 Predictions Max              4.99248\n",
      "trainer/Q1 Predictions Min              4.77699\n",
      "trainer/Q2 Predictions Mean             4.91715\n",
      "trainer/Q2 Predictions Std              0.0381635\n",
      "trainer/Q2 Predictions Max              4.99603\n",
      "trainer/Q2 Predictions Min              4.77562\n",
      "trainer/Q Targets Mean                  4.90834\n",
      "trainer/Q Targets Std                   0.0692583\n",
      "trainer/Q Targets Max                   5.29834\n",
      "trainer/Q Targets Min                   4.72624\n",
      "trainer/Log Pis Mean                   -0.685075\n",
      "trainer/Log Pis Std                     0.127162\n",
      "trainer/Log Pis Max                    -0.566335\n",
      "trainer/Log Pis Min                    -1.84839\n",
      "trainer/policy/mean Mean                0.00731163\n",
      "trainer/policy/mean Std                 0.000877141\n",
      "trainer/policy/mean Max                 0.0109139\n",
      "trainer/policy/mean Min                 0.00629888\n",
      "trainer/policy/normal/std Mean          0.834373\n",
      "trainer/policy/normal/std Std           0.00512628\n",
      "trainer/policy/normal/std Max           0.843259\n",
      "trainer/policy/normal/std Min           0.816036\n",
      "trainer/policy/normal/log_std Mean     -0.181094\n",
      "trainer/policy/normal/log_std Std       0.00616402\n",
      "trainer/policy/normal/log_std Max      -0.170481\n",
      "trainer/policy/normal/log_std Min      -0.203296\n",
      "trainer/Alpha                           0.406545\n",
      "trainer/Alpha Loss                     -1.51667\n",
      "exploration/num steps total          5000\n",
      "exploration/num paths total            10\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0334007\n",
      "exploration/Rewards Std                 0.0293299\n",
      "exploration/Rewards Max                -9.88445e-08\n",
      "exploration/Rewards Min                -0.0975145\n",
      "exploration/Returns Mean              -16.7004\n",
      "exploration/Returns Std                16.6431\n",
      "exploration/Returns Max                -0.0572741\n",
      "exploration/Returns Min               -33.3435\n",
      "exploration/Actions Mean               -0.00420118\n",
      "exploration/Actions Std                 0.577918\n",
      "exploration/Actions Max                 0.987494\n",
      "exploration/Actions Min                -0.981846\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns           -16.7004\n",
      "evaluation/num steps total          19980\n",
      "evaluation/num paths total             20\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -5.06475e-06\n",
      "evaluation/Rewards Std                  3.57935e-07\n",
      "evaluation/Rewards Max                 -4.47054e-06\n",
      "evaluation/Rewards Min                 -5.90511e-06\n",
      "evaluation/Returns Mean                -0.00505969\n",
      "evaluation/Returns Std                  2.64303e-05\n",
      "evaluation/Returns Max                 -0.00502044\n",
      "evaluation/Returns Min                 -0.00509905\n",
      "evaluation/Actions Mean                 0.00711231\n",
      "evaluation/Actions Std                  0.000250054\n",
      "evaluation/Actions Max                  0.00768445\n",
      "evaluation/Actions Min                  0.00668625\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.00505969\n",
      "time/data storing (s)                   0.00515392\n",
      "time/evaluation sampling (s)            7.91148\n",
      "time/exploration sampling (s)           1.72432\n",
      "time/logging (s)                        0.0182992\n",
      "time/sac training (s)                  24.8122\n",
      "time/saving (s)                         0.00673476\n",
      "time/training (s)                       4.91571e-05\n",
      "time/epoch (s)                         34.4782\n",
      "time/total (s)                        166.128\n",
      "Epoch                                   3\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:23:30.041955 UTC | [] Epoch 4 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                   6000\n",
      "trainer/num train calls              5000\n",
      "trainer/QF1 Loss                        0.00221414\n",
      "trainer/QF2 Loss                        0.00227147\n",
      "trainer/Policy Loss                    -5.73894\n",
      "trainer/Q1 Predictions Mean             5.53349\n",
      "trainer/Q1 Predictions Std              0.044239\n",
      "trainer/Q1 Predictions Max              5.61117\n",
      "trainer/Q1 Predictions Min              5.29372\n",
      "trainer/Q2 Predictions Mean             5.53267\n",
      "trainer/Q2 Predictions Std              0.0452392\n",
      "trainer/Q2 Predictions Max              5.61469\n",
      "trainer/Q2 Predictions Min              5.29535\n",
      "trainer/Q Targets Mean                  5.5548\n",
      "trainer/Q Targets Std                   0.0601806\n",
      "trainer/Q Targets Max                   5.82308\n",
      "trainer/Q Targets Min                   5.30475\n",
      "trainer/Log Pis Mean                   -0.67534\n",
      "trainer/Log Pis Std                     0.0922267\n",
      "trainer/Log Pis Max                    -0.588824\n",
      "trainer/Log Pis Min                    -1.38255\n",
      "trainer/policy/mean Mean                0.0135192\n",
      "trainer/policy/mean Std                 0.00173702\n",
      "trainer/policy/mean Max                 0.0210206\n",
      "trainer/policy/mean Min                 0.011029\n",
      "trainer/policy/normal/std Mean          0.7963\n",
      "trainer/policy/normal/std Std           0.00737829\n",
      "trainer/policy/normal/std Max           0.811281\n",
      "trainer/policy/normal/std Min           0.771848\n",
      "trainer/policy/normal/log_std Mean     -0.227823\n",
      "trainer/policy/normal/log_std Std       0.00930956\n",
      "trainer/policy/normal/log_std Max      -0.209141\n",
      "trainer/policy/normal/log_std Min      -0.258968\n",
      "trainer/Alpha                           0.301238\n",
      "trainer/Alpha Loss                     -2.01016\n",
      "exploration/num steps total          6000\n",
      "exploration/num paths total            12\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0331032\n",
      "exploration/Rewards Std                 0.0282295\n",
      "exploration/Rewards Max                -1.57078e-07\n",
      "exploration/Rewards Min                -0.0997207\n",
      "exploration/Returns Mean              -16.5516\n",
      "exploration/Returns Std                16.4918\n",
      "exploration/Returns Max                -0.0597451\n",
      "exploration/Returns Min               -33.0434\n",
      "exploration/Actions Mean               -0.00736509\n",
      "exploration/Actions Std                 0.575306\n",
      "exploration/Actions Max                 0.99164\n",
      "exploration/Actions Min                -0.998603\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns           -16.5516\n",
      "evaluation/num steps total          24975\n",
      "evaluation/num paths total             25\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -1.74248e-05\n",
      "evaluation/Rewards Std                  8.09641e-07\n",
      "evaluation/Rewards Max                 -1.56165e-05\n",
      "evaluation/Rewards Min                 -1.96282e-05\n",
      "evaluation/Returns Mean                -0.0174074\n",
      "evaluation/Returns Std                  3.85135e-05\n",
      "evaluation/Returns Max                 -0.0173743\n",
      "evaluation/Returns Min                 -0.0174724\n",
      "evaluation/Actions Mean                 0.0131968\n",
      "evaluation/Actions Std                  0.000305657\n",
      "evaluation/Actions Max                  0.0140101\n",
      "evaluation/Actions Min                  0.0124966\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.0174074\n",
      "time/data storing (s)                   0.00526948\n",
      "time/evaluation sampling (s)            7.7945\n",
      "time/exploration sampling (s)           1.83147\n",
      "time/logging (s)                        0.0706641\n",
      "time/sac training (s)                  25.0537\n",
      "time/saving (s)                         0.00669939\n",
      "time/training (s)                       4.73112e-05\n",
      "time/epoch (s)                         34.7623\n",
      "time/total (s)                        203.068\n",
      "Epoch                                   4\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:24:07.048827 UTC | [] Epoch 5 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                   7000\n",
      "trainer/num train calls              6000\n",
      "trainer/QF1 Loss                        0.00165062\n",
      "trainer/QF2 Loss                        0.00160441\n",
      "trainer/Policy Loss                    -6.02508\n",
      "trainer/Q1 Predictions Mean             5.87078\n",
      "trainer/Q1 Predictions Std              0.0486111\n",
      "trainer/Q1 Predictions Max              5.9835\n",
      "trainer/Q1 Predictions Min              5.63757\n",
      "trainer/Q2 Predictions Mean             5.87273\n",
      "trainer/Q2 Predictions Std              0.0500581\n",
      "trainer/Q2 Predictions Max              5.97935\n",
      "trainer/Q2 Predictions Min              5.61801\n",
      "trainer/Q Targets Mean                  5.8874\n",
      "trainer/Q Targets Std                   0.0584695\n",
      "trainer/Q Targets Max                   6.14628\n",
      "trainer/Q Targets Min                   5.70335\n",
      "trainer/Log Pis Mean                   -0.669593\n",
      "trainer/Log Pis Std                     0.142744\n",
      "trainer/Log Pis Max                    -0.579359\n",
      "trainer/Log Pis Min                    -2.2545\n",
      "trainer/policy/mean Mean                0.0160529\n",
      "trainer/policy/mean Std                 0.00331272\n",
      "trainer/policy/mean Max                 0.0289325\n",
      "trainer/policy/mean Min                 0.0112026\n",
      "trainer/policy/normal/std Mean          0.784452\n",
      "trainer/policy/normal/std Std           0.0199069\n",
      "trainer/policy/normal/std Max           0.821993\n",
      "trainer/policy/normal/std Min           0.714971\n",
      "trainer/policy/normal/log_std Mean     -0.243096\n",
      "trainer/policy/normal/log_std Std       0.0256211\n",
      "trainer/policy/normal/log_std Max      -0.196023\n",
      "trainer/policy/normal/log_std Min      -0.335513\n",
      "trainer/Alpha                           0.223271\n",
      "trainer/Alpha Loss                     -2.50333\n",
      "exploration/num steps total          7000\n",
      "exploration/num paths total            14\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0316131\n",
      "exploration/Rewards Std                 0.0281657\n",
      "exploration/Rewards Max                -6.04109e-10\n",
      "exploration/Rewards Min                -0.0964525\n",
      "exploration/Returns Mean              -15.8065\n",
      "exploration/Returns Std                15.8045\n",
      "exploration/Returns Max                -0.00207182\n",
      "exploration/Returns Min               -31.611\n",
      "exploration/Actions Mean               -0.0162545\n",
      "exploration/Actions Std                 0.56202\n",
      "exploration/Actions Max                 0.966687\n",
      "exploration/Actions Min                -0.982102\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns           -15.8065\n",
      "evaluation/num steps total          29970\n",
      "evaluation/num paths total             30\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -2.40849e-05\n",
      "evaluation/Rewards Std                  2.23593e-06\n",
      "evaluation/Rewards Max                 -1.87922e-05\n",
      "evaluation/Rewards Min                 -3.05213e-05\n",
      "evaluation/Returns Mean                -0.0240608\n",
      "evaluation/Returns Std                  7.52228e-05\n",
      "evaluation/Returns Max                 -0.0239933\n",
      "evaluation/Returns Min                 -0.0242016\n",
      "evaluation/Actions Mean                 0.0155026\n",
      "evaluation/Actions Std                  0.000718909\n",
      "evaluation/Actions Max                  0.0174704\n",
      "evaluation/Actions Min                  0.0137085\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.0240608\n",
      "time/data storing (s)                   0.00539934\n",
      "time/evaluation sampling (s)            7.92028\n",
      "time/exploration sampling (s)           1.79637\n",
      "time/logging (s)                        0.066539\n",
      "time/sac training (s)                  24.9056\n",
      "time/saving (s)                         0.00663579\n",
      "time/training (s)                       4.73484e-05\n",
      "time/epoch (s)                         34.7009\n",
      "time/total (s)                        240.056\n",
      "Epoch                                   5\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:24:44.339067 UTC | [] Epoch 6 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                   8000\n",
      "trainer/num train calls              7000\n",
      "trainer/QF1 Loss                        0.00123556\n",
      "trainer/QF2 Loss                        0.00124058\n",
      "trainer/Policy Loss                    -6.11941\n",
      "trainer/Q1 Predictions Mean             6.00374\n",
      "trainer/Q1 Predictions Std              0.0703117\n",
      "trainer/Q1 Predictions Max              6.1403\n",
      "trainer/Q1 Predictions Min              5.66507\n",
      "trainer/Q2 Predictions Mean             6.00431\n",
      "trainer/Q2 Predictions Std              0.0710035\n",
      "trainer/Q2 Predictions Max              6.14277\n",
      "trainer/Q2 Predictions Min              5.69246\n",
      "trainer/Q Targets Mean                  6.00728\n",
      "trainer/Q Targets Std                   0.0637324\n",
      "trainer/Q Targets Max                   6.30329\n",
      "trainer/Q Targets Min                   5.69146\n",
      "trainer/Log Pis Mean                   -0.664721\n",
      "trainer/Log Pis Std                     0.186741\n",
      "trainer/Log Pis Max                    -0.50866\n",
      "trainer/Log Pis Min                    -2.08591\n",
      "trainer/policy/mean Mean                0.00939711\n",
      "trainer/policy/mean Std                 0.00235894\n",
      "trainer/policy/mean Max                 0.0237977\n",
      "trainer/policy/mean Min                 0.0075558\n",
      "trainer/policy/normal/std Mean          0.721951\n",
      "trainer/policy/normal/std Std           0.0225174\n",
      "trainer/policy/normal/std Max           0.7635\n",
      "trainer/policy/normal/std Min           0.62844\n",
      "trainer/policy/normal/log_std Mean     -0.326296\n",
      "trainer/policy/normal/log_std Std       0.0318053\n",
      "trainer/policy/normal/log_std Max      -0.269843\n",
      "trainer/policy/normal/log_std Min      -0.464514\n",
      "trainer/Alpha                           0.165542\n",
      "trainer/Alpha Loss                     -2.99405\n",
      "exploration/num steps total          8000\n",
      "exploration/num paths total            16\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.027884\n",
      "exploration/Rewards Std                 0.0254252\n",
      "exploration/Rewards Max                -2.46046e-08\n",
      "exploration/Rewards Min                -0.0979663\n",
      "exploration/Returns Mean              -13.942\n",
      "exploration/Returns Std                13.9386\n",
      "exploration/Returns Max                -0.00335217\n",
      "exploration/Returns Min               -27.8806\n",
      "exploration/Actions Mean                0.00681985\n",
      "exploration/Actions Std                 0.528009\n",
      "exploration/Actions Max                 0.989779\n",
      "exploration/Actions Min                -0.966294\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns           -13.942\n",
      "evaluation/num steps total          34965\n",
      "evaluation/num paths total             35\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -7.48292e-06\n",
      "evaluation/Rewards Std                  4.16358e-07\n",
      "evaluation/Rewards Max                 -6.82967e-06\n",
      "evaluation/Rewards Min                 -9.02686e-06\n",
      "evaluation/Returns Mean                -0.00747544\n",
      "evaluation/Returns Std                  9.21108e-05\n",
      "evaluation/Returns Max                 -0.00738306\n",
      "evaluation/Returns Min                 -0.00763636\n",
      "evaluation/Actions Mean                 0.00864716\n",
      "evaluation/Actions Std                  0.000236285\n",
      "evaluation/Actions Max                  0.00950104\n",
      "evaluation/Actions Min                  0.00826422\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.00747544\n",
      "time/data storing (s)                   0.0538671\n",
      "time/evaluation sampling (s)            8.03044\n",
      "time/exploration sampling (s)           1.70227\n",
      "time/logging (s)                        0.0168973\n",
      "time/sac training (s)                  25.1325\n",
      "time/saving (s)                         0.00600968\n",
      "time/training (s)                       4.43086e-05\n",
      "time/epoch (s)                         34.942\n",
      "time/total (s)                        277.283\n",
      "Epoch                                   6\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:25:21.215019 UTC | [] Epoch 7 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                   9000\n",
      "trainer/num train calls              8000\n",
      "trainer/QF1 Loss                        0.000645455\n",
      "trainer/QF2 Loss                        0.00066813\n",
      "trainer/Policy Loss                    -6.05499\n",
      "trainer/Q1 Predictions Mean             5.97416\n",
      "trainer/Q1 Predictions Std              0.0570416\n",
      "trainer/Q1 Predictions Max              6.10373\n",
      "trainer/Q1 Predictions Min              5.66932\n",
      "trainer/Q2 Predictions Mean             5.97339\n",
      "trainer/Q2 Predictions Std              0.0539147\n",
      "trainer/Q2 Predictions Max              6.09525\n",
      "trainer/Q2 Predictions Min              5.68432\n",
      "trainer/Q Targets Mean                  5.97543\n",
      "trainer/Q Targets Std                   0.0635361\n",
      "trainer/Q Targets Max                   6.16018\n",
      "trainer/Q Targets Min                   5.70321\n",
      "trainer/Log Pis Mean                   -0.652323\n",
      "trainer/Log Pis Std                     0.189314\n",
      "trainer/Log Pis Max                    -0.473137\n",
      "trainer/Log Pis Min                    -2.42034\n",
      "trainer/policy/mean Mean                0.00850894\n",
      "trainer/policy/mean Std                 0.00564747\n",
      "trainer/policy/mean Max                 0.0570598\n",
      "trainer/policy/mean Min                 0.0058697\n",
      "trainer/policy/normal/std Mean          0.708655\n",
      "trainer/policy/normal/std Std           0.0246369\n",
      "trainer/policy/normal/std Max           0.749437\n",
      "trainer/policy/normal/std Min           0.599251\n",
      "trainer/policy/normal/log_std Mean     -0.345016\n",
      "trainer/policy/normal/log_std Std       0.0358315\n",
      "trainer/policy/normal/log_std Max      -0.288432\n",
      "trainer/policy/normal/log_std Min      -0.512076\n",
      "trainer/Alpha                           0.122853\n",
      "trainer/Alpha Loss                     -3.46453\n",
      "exploration/num steps total          9000\n",
      "exploration/num paths total            18\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.026378\n",
      "exploration/Rewards Std                 0.0251728\n",
      "exploration/Rewards Max                -8.01814e-08\n",
      "exploration/Rewards Min                -0.0955695\n",
      "exploration/Returns Mean              -13.189\n",
      "exploration/Returns Std                13.1767\n",
      "exploration/Returns Max                -0.0122766\n",
      "exploration/Returns Min               -26.3657\n",
      "exploration/Actions Mean                0.00123381\n",
      "exploration/Actions Std                 0.513594\n",
      "exploration/Actions Max                 0.977596\n",
      "exploration/Actions Min                -0.970912\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns           -13.189\n",
      "evaluation/num steps total          39960\n",
      "evaluation/num paths total             40\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -5.49746e-06\n",
      "evaluation/Rewards Std                  3.70162e-07\n",
      "evaluation/Rewards Max                 -4.86281e-06\n",
      "evaluation/Rewards Min                 -6.9677e-06\n",
      "evaluation/Returns Mean                -0.00549196\n",
      "evaluation/Returns Std                  9.04824e-05\n",
      "evaluation/Returns Max                 -0.00543057\n",
      "evaluation/Returns Min                 -0.00566711\n",
      "evaluation/Actions Mean                 0.00741052\n",
      "evaluation/Actions Std                  0.000242377\n",
      "evaluation/Actions Max                  0.00834732\n",
      "evaluation/Actions Min                  0.0069734\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.00549196\n",
      "time/data storing (s)                   0.00532793\n",
      "time/evaluation sampling (s)            7.79242\n",
      "time/exploration sampling (s)           1.83771\n",
      "time/logging (s)                        0.017915\n",
      "time/sac training (s)                  24.9096\n",
      "time/saving (s)                         0.0547928\n",
      "time/training (s)                       4.75142e-05\n",
      "time/epoch (s)                         34.6178\n",
      "time/total (s)                        314.146\n",
      "Epoch                                   7\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:25:58.327578 UTC | [] Epoch 8 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  10000\n",
      "trainer/num train calls              9000\n",
      "trainer/QF1 Loss                        0.000917895\n",
      "trainer/QF2 Loss                        0.00101413\n",
      "trainer/Policy Loss                    -5.9092\n",
      "trainer/Q1 Predictions Mean             5.84993\n",
      "trainer/Q1 Predictions Std              0.0552764\n",
      "trainer/Q1 Predictions Max              5.96819\n",
      "trainer/Q1 Predictions Min              5.4182\n",
      "trainer/Q2 Predictions Mean             5.85271\n",
      "trainer/Q2 Predictions Std              0.0549471\n",
      "trainer/Q2 Predictions Max              5.96752\n",
      "trainer/Q2 Predictions Min              5.42577\n",
      "trainer/Q Targets Mean                  5.82665\n",
      "trainer/Q Targets Std                   0.0546908\n",
      "trainer/Q Targets Max                   5.94825\n",
      "trainer/Q Targets Min                   5.40599\n",
      "trainer/Log Pis Mean                   -0.632824\n",
      "trainer/Log Pis Std                     0.223816\n",
      "trainer/Log Pis Max                    -0.261563\n",
      "trainer/Log Pis Min                    -1.99801\n",
      "trainer/policy/mean Mean                0.00637675\n",
      "trainer/policy/mean Std                 0.0105617\n",
      "trainer/policy/mean Max                 0.11541\n",
      "trainer/policy/mean Min                 0.00376236\n",
      "trainer/policy/normal/std Mean          0.668402\n",
      "trainer/policy/normal/std Std           0.0342746\n",
      "trainer/policy/normal/std Max           0.736523\n",
      "trainer/policy/normal/std Min           0.499812\n",
      "trainer/policy/normal/log_std Mean     -0.404264\n",
      "trainer/policy/normal/log_std Std       0.0537608\n",
      "trainer/policy/normal/log_std Max      -0.305814\n",
      "trainer/policy/normal/log_std Min      -0.693524\n",
      "trainer/Alpha                           0.0912273\n",
      "trainer/Alpha Loss                     -3.90963\n",
      "exploration/num steps total         10000\n",
      "exploration/num paths total            20\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0248637\n",
      "exploration/Rewards Std                 0.0240927\n",
      "exploration/Rewards Max                -1.24049e-07\n",
      "exploration/Rewards Min                -0.0952598\n",
      "exploration/Returns Mean              -12.4318\n",
      "exploration/Returns Std                12.3915\n",
      "exploration/Returns Max                -0.0403366\n",
      "exploration/Returns Min               -24.8234\n",
      "exploration/Actions Mean                0.0236277\n",
      "exploration/Actions Std                 0.498075\n",
      "exploration/Actions Max                 0.976011\n",
      "exploration/Actions Min                -0.942548\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns           -12.4318\n",
      "evaluation/num steps total          44955\n",
      "evaluation/num paths total             45\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -1.63234e-06\n",
      "evaluation/Rewards Std                  9.381e-08\n",
      "evaluation/Rewards Max                 -1.49088e-06\n",
      "evaluation/Rewards Min                 -1.79809e-06\n",
      "evaluation/Returns Mean                -0.00163071\n",
      "evaluation/Returns Std                  3.19541e-06\n",
      "evaluation/Returns Max                 -0.00162565\n",
      "evaluation/Returns Min                 -0.00163417\n",
      "evaluation/Actions Mean                 0.00403856\n",
      "evaluation/Actions Std                  0.00011597\n",
      "evaluation/Actions Max                  0.00424041\n",
      "evaluation/Actions Min                  0.00386114\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.00163071\n",
      "time/data storing (s)                   0.00535369\n",
      "time/evaluation sampling (s)            7.82772\n",
      "time/exploration sampling (s)           1.80103\n",
      "time/logging (s)                        0.0194172\n",
      "time/sac training (s)                  25.0937\n",
      "time/saving (s)                         0.00681487\n",
      "time/training (s)                       5.22323e-05\n",
      "time/epoch (s)                         34.7541\n",
      "time/total (s)                        351.245\n",
      "Epoch                                   8\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:26:35.427550 UTC | [] Epoch 9 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  11000\n",
      "trainer/num train calls             10000\n",
      "trainer/QF1 Loss                        0.000251796\n",
      "trainer/QF2 Loss                        0.000207133\n",
      "trainer/Policy Loss                    -5.65121\n",
      "trainer/Q1 Predictions Mean             5.60407\n",
      "trainer/Q1 Predictions Std              0.0651883\n",
      "trainer/Q1 Predictions Max              5.72263\n",
      "trainer/Q1 Predictions Min              5.09398\n",
      "trainer/Q2 Predictions Mean             5.60515\n",
      "trainer/Q2 Predictions Std              0.0637025\n",
      "trainer/Q2 Predictions Max              5.72209\n",
      "trainer/Q2 Predictions Min              5.07245\n",
      "trainer/Q Targets Mean                  5.60239\n",
      "trainer/Q Targets Std                   0.0618806\n",
      "trainer/Q Targets Max                   5.7212\n",
      "trainer/Q Targets Min                   5.11164\n",
      "trainer/Log Pis Mean                   -0.636012\n",
      "trainer/Log Pis Std                     0.358565\n",
      "trainer/Log Pis Max                    -0.199615\n",
      "trainer/Log Pis Min                    -2.48828\n",
      "trainer/policy/mean Mean                0.0167242\n",
      "trainer/policy/mean Std                 0.0130063\n",
      "trainer/policy/mean Max                 0.121627\n",
      "trainer/policy/mean Min                 0.0121348\n",
      "trainer/policy/normal/std Mean          0.604982\n",
      "trainer/policy/normal/std Std           0.0317654\n",
      "trainer/policy/normal/std Max           0.672808\n",
      "trainer/policy/normal/std Min           0.475922\n",
      "trainer/policy/normal/log_std Mean     -0.504022\n",
      "trainer/policy/normal/log_std Std       0.0550247\n",
      "trainer/policy/normal/log_std Max      -0.396295\n",
      "trainer/policy/normal/log_std Min      -0.742502\n",
      "trainer/Alpha                           0.067872\n",
      "trainer/Alpha Loss                     -4.40109\n",
      "exploration/num steps total         11000\n",
      "exploration/num paths total            22\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0219057\n",
      "exploration/Rewards Std                 0.0221062\n",
      "exploration/Rewards Max                -7.00317e-08\n",
      "exploration/Rewards Min                -0.0922034\n",
      "exploration/Returns Mean              -10.9528\n",
      "exploration/Returns Std                10.9524\n",
      "exploration/Returns Max                -0.000404366\n",
      "exploration/Returns Min               -21.9053\n",
      "exploration/Actions Mean                0.0116275\n",
      "exploration/Actions Std                 0.46789\n",
      "exploration/Actions Max                 0.931809\n",
      "exploration/Actions Min                -0.960226\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns           -10.9528\n",
      "evaluation/num steps total          49950\n",
      "evaluation/num paths total             50\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -1.91013e-05\n",
      "evaluation/Rewards Std                  5.79314e-07\n",
      "evaluation/Rewards Max                 -1.76619e-05\n",
      "evaluation/Rewards Min                 -2.05258e-05\n",
      "evaluation/Returns Mean                -0.0190822\n",
      "evaluation/Returns Std                  1.28788e-05\n",
      "evaluation/Returns Max                 -0.0190603\n",
      "evaluation/Returns Min                 -0.0190982\n",
      "evaluation/Actions Mean                 0.0138192\n",
      "evaluation/Actions Std                  0.000209724\n",
      "evaluation/Actions Max                  0.0143268\n",
      "evaluation/Actions Min                  0.0132898\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.0190822\n",
      "time/data storing (s)                   0.0535222\n",
      "time/evaluation sampling (s)            8.01074\n",
      "time/exploration sampling (s)           1.82854\n",
      "time/logging (s)                        0.0185643\n",
      "time/sac training (s)                  24.8221\n",
      "time/saving (s)                         0.0072403\n",
      "time/training (s)                       7.94567e-05\n",
      "time/epoch (s)                         34.7408\n",
      "time/total (s)                        388.33\n",
      "Epoch                                   9\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:27:12.427536 UTC | [] Epoch 10 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  12000\n",
      "trainer/num train calls             11000\n",
      "trainer/QF1 Loss                        0.000750004\n",
      "trainer/QF2 Loss                        0.000675951\n",
      "trainer/Policy Loss                    -5.34466\n",
      "trainer/Q1 Predictions Mean             5.30799\n",
      "trainer/Q1 Predictions Std              0.105158\n",
      "trainer/Q1 Predictions Max              5.43042\n",
      "trainer/Q1 Predictions Min              4.55555\n",
      "trainer/Q2 Predictions Mean             5.30936\n",
      "trainer/Q2 Predictions Std              0.106637\n",
      "trainer/Q2 Predictions Max              5.42638\n",
      "trainer/Q2 Predictions Min              4.52674\n",
      "trainer/Q Targets Mean                  5.331\n",
      "trainer/Q Targets Std                   0.107216\n",
      "trainer/Q Targets Max                   5.46854\n",
      "trainer/Q Targets Min                   4.55341\n",
      "trainer/Log Pis Mean                   -0.519195\n",
      "trainer/Log Pis Std                     0.319516\n",
      "trainer/Log Pis Max                    -0.0212854\n",
      "trainer/Log Pis Min                    -2.39939\n",
      "trainer/policy/mean Mean               -0.00160516\n",
      "trainer/policy/mean Std                 0.0259849\n",
      "trainer/policy/mean Max                 0.0394078\n",
      "trainer/policy/mean Min                -0.239574\n",
      "trainer/policy/normal/std Mean          0.539064\n",
      "trainer/policy/normal/std Std           0.0361462\n",
      "trainer/policy/normal/std Max           0.613248\n",
      "trainer/policy/normal/std Min           0.397901\n",
      "trainer/policy/normal/log_std Mean     -0.620352\n",
      "trainer/policy/normal/log_std Std       0.0712219\n",
      "trainer/policy/normal/log_std Max      -0.488986\n",
      "trainer/policy/normal/log_std Min      -0.921551\n",
      "trainer/Alpha                           0.0506463\n",
      "trainer/Alpha Loss                     -4.53159\n",
      "exploration/num steps total         12000\n",
      "exploration/num paths total            24\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0188762\n",
      "exploration/Rewards Std                 0.019551\n",
      "exploration/Rewards Max                -9.60654e-13\n",
      "exploration/Rewards Min                -0.0861479\n",
      "exploration/Returns Mean               -9.4381\n",
      "exploration/Returns Std                 9.41009\n",
      "exploration/Returns Max                -0.028014\n",
      "exploration/Returns Min               -18.8482\n",
      "exploration/Actions Mean               -0.0263651\n",
      "exploration/Actions Std                 0.433667\n",
      "exploration/Actions Max                 0.928159\n",
      "exploration/Actions Min                -0.903755\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -9.4381\n",
      "evaluation/num steps total          54945\n",
      "evaluation/num paths total             55\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -7.17068e-07\n",
      "evaluation/Rewards Std                  8.50667e-07\n",
      "evaluation/Rewards Max                 -3.19744e-15\n",
      "evaluation/Rewards Min                 -3.94317e-06\n",
      "evaluation/Returns Mean                -0.000716351\n",
      "evaluation/Returns Std                  0.000474124\n",
      "evaluation/Returns Max                 -0.00035154\n",
      "evaluation/Returns Min                 -0.00162931\n",
      "evaluation/Actions Mean                 0.0016324\n",
      "evaluation/Actions Std                  0.00212273\n",
      "evaluation/Actions Max                  0.00627941\n",
      "evaluation/Actions Min                 -0.00460115\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.000716351\n",
      "time/data storing (s)                   0.00523364\n",
      "time/evaluation sampling (s)            7.88193\n",
      "time/exploration sampling (s)           1.83785\n",
      "time/logging (s)                        0.0170091\n",
      "time/sac training (s)                  25.0348\n",
      "time/saving (s)                         0.00646588\n",
      "time/training (s)                       4.21293e-05\n",
      "time/epoch (s)                         34.7833\n",
      "time/total (s)                        425.314\n",
      "Epoch                                  10\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:27:49.625932 UTC | [] Epoch 11 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  13000\n",
      "trainer/num train calls             12000\n",
      "trainer/QF1 Loss                        0.0830903\n",
      "trainer/QF2 Loss                        0.0842487\n",
      "trainer/Policy Loss                    -5.06437\n",
      "trainer/Q1 Predictions Mean             5.03219\n",
      "trainer/Q1 Predictions Std              0.146434\n",
      "trainer/Q1 Predictions Max              5.17423\n",
      "trainer/Q1 Predictions Min              4.31696\n",
      "trainer/Q2 Predictions Mean             5.03283\n",
      "trainer/Q2 Predictions Std              0.147719\n",
      "trainer/Q2 Predictions Max              5.1685\n",
      "trainer/Q2 Predictions Min              4.27491\n",
      "trainer/Q Targets Mean                  5.01607\n",
      "trainer/Q Targets Std                   0.343816\n",
      "trainer/Q Targets Max                   5.16886\n",
      "trainer/Q Targets Min                  -0.0195148\n",
      "trainer/Log Pis Mean                   -0.482384\n",
      "trainer/Log Pis Std                     0.428021\n",
      "trainer/Log Pis Max                     0.0760077\n",
      "trainer/Log Pis Min                    -2.74397\n",
      "trainer/policy/mean Mean               -0.0573094\n",
      "trainer/policy/mean Std                 0.0614037\n",
      "trainer/policy/mean Max                 0.0371712\n",
      "trainer/policy/mean Min                -0.594172\n",
      "trainer/policy/normal/std Mean          0.487515\n",
      "trainer/policy/normal/std Std           0.039001\n",
      "trainer/policy/normal/std Max           0.785735\n",
      "trainer/policy/normal/std Min           0.360054\n",
      "trainer/policy/normal/log_std Mean     -0.721622\n",
      "trainer/policy/normal/log_std Std       0.0802418\n",
      "trainer/policy/normal/log_std Max      -0.241135\n",
      "trainer/policy/normal/log_std Min      -1.0215\n",
      "trainer/Alpha                           0.0379611\n",
      "trainer/Alpha Loss                     -4.84916\n",
      "exploration/num steps total         13000\n",
      "exploration/num paths total            26\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0164066\n",
      "exploration/Rewards Std                 0.0177034\n",
      "exploration/Rewards Max                -2.88218e-08\n",
      "exploration/Rewards Min                -0.0784257\n",
      "exploration/Returns Mean               -8.20329\n",
      "exploration/Returns Std                 8.20312\n",
      "exploration/Returns Max                -0.000165251\n",
      "exploration/Returns Min               -16.4064\n",
      "exploration/Actions Mean               -0.0486638\n",
      "exploration/Actions Std                 0.402116\n",
      "exploration/Actions Max                 0.859248\n",
      "exploration/Actions Min                -0.885583\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -8.20329\n",
      "evaluation/num steps total          59940\n",
      "evaluation/num paths total             60\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -0.000240956\n",
      "evaluation/Rewards Std                  0.000110812\n",
      "evaluation/Rewards Max                 -7.98428e-05\n",
      "evaluation/Rewards Min                 -0.000581425\n",
      "evaluation/Returns Mean                -0.240716\n",
      "evaluation/Returns Std                  0.0216827\n",
      "evaluation/Returns Max                 -0.21846\n",
      "evaluation/Returns Min                 -0.275305\n",
      "evaluation/Actions Mean                -0.0478764\n",
      "evaluation/Actions Std                  0.010836\n",
      "evaluation/Actions Max                 -0.0282565\n",
      "evaluation/Actions Min                 -0.0762512\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.240716\n",
      "time/data storing (s)                   0.0536679\n",
      "time/evaluation sampling (s)            7.90648\n",
      "time/exploration sampling (s)           1.81486\n",
      "time/logging (s)                        0.0177886\n",
      "time/sac training (s)                  24.9798\n",
      "time/saving (s)                         0.00637043\n",
      "time/training (s)                       4.08404e-05\n",
      "time/epoch (s)                         34.779\n",
      "time/total (s)                        462.447\n",
      "Epoch                                  11\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:28:27.019179 UTC | [] Epoch 12 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  14000\n",
      "trainer/num train calls             13000\n",
      "trainer/QF1 Loss                        0.000228609\n",
      "trainer/QF2 Loss                        0.000234938\n",
      "trainer/Policy Loss                    -4.78999\n",
      "trainer/Q1 Predictions Mean             4.76876\n",
      "trainer/Q1 Predictions Std              0.116596\n",
      "trainer/Q1 Predictions Max              4.86344\n",
      "trainer/Q1 Predictions Min              4.06005\n",
      "trainer/Q2 Predictions Mean             4.76792\n",
      "trainer/Q2 Predictions Std              0.118567\n",
      "trainer/Q2 Predictions Max              4.87886\n",
      "trainer/Q2 Predictions Min              4.06444\n",
      "trainer/Q Targets Mean                  4.75949\n",
      "trainer/Q Targets Std                   0.119669\n",
      "trainer/Q Targets Max                   4.85331\n",
      "trainer/Q Targets Min                   4.01612\n",
      "trainer/Log Pis Mean                   -0.35011\n",
      "trainer/Log Pis Std                     0.492116\n",
      "trainer/Log Pis Max                     0.313784\n",
      "trainer/Log Pis Min                    -3.25316\n",
      "trainer/policy/mean Mean                0.0102016\n",
      "trainer/policy/mean Std                 0.0471439\n",
      "trainer/policy/mean Max                 0.151864\n",
      "trainer/policy/mean Min                -0.321257\n",
      "trainer/policy/normal/std Mean          0.414923\n",
      "trainer/policy/normal/std Std           0.0306262\n",
      "trainer/policy/normal/std Max           0.53215\n",
      "trainer/policy/normal/std Min           0.286695\n",
      "trainer/policy/normal/log_std Mean     -0.882623\n",
      "trainer/policy/normal/log_std Std       0.0787491\n",
      "trainer/policy/normal/log_std Max      -0.63083\n",
      "trainer/policy/normal/log_std Min      -1.24934\n",
      "trainer/Alpha                           0.0285772\n",
      "trainer/Alpha Loss                     -4.79984\n",
      "exploration/num steps total         14000\n",
      "exploration/num paths total            28\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0136423\n",
      "exploration/Rewards Std                 0.0147742\n",
      "exploration/Rewards Max                -5.25238e-08\n",
      "exploration/Rewards Min                -0.077817\n",
      "exploration/Returns Mean               -6.82114\n",
      "exploration/Returns Std                 6.79735\n",
      "exploration/Returns Max                -0.0237966\n",
      "exploration/Returns Min               -13.6185\n",
      "exploration/Actions Mean                0.000672062\n",
      "exploration/Actions Std                 0.369354\n",
      "exploration/Actions Max                 0.858967\n",
      "exploration/Actions Min                -0.882139\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -6.82114\n",
      "evaluation/num steps total          64935\n",
      "evaluation/num paths total             65\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -1.97572e-05\n",
      "evaluation/Rewards Std                  1.66184e-05\n",
      "evaluation/Rewards Max                 -2.23825e-11\n",
      "evaluation/Rewards Min                 -5.98558e-05\n",
      "evaluation/Returns Mean                -0.0197374\n",
      "evaluation/Returns Std                  0.00507167\n",
      "evaluation/Returns Max                 -0.0127286\n",
      "evaluation/Returns Min                 -0.027071\n",
      "evaluation/Actions Mean                 0.008341\n",
      "evaluation/Actions Std                  0.0113137\n",
      "evaluation/Actions Max                  0.0244654\n",
      "evaluation/Actions Min                 -0.0208837\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.0197374\n",
      "time/data storing (s)                   0.00549313\n",
      "time/evaluation sampling (s)            7.98126\n",
      "time/exploration sampling (s)           1.82991\n",
      "time/logging (s)                        0.0183361\n",
      "time/sac training (s)                  25.2667\n",
      "time/saving (s)                         0.00679536\n",
      "time/training (s)                       4.9524e-05\n",
      "time/epoch (s)                         35.1086\n",
      "time/total (s)                        499.827\n",
      "Epoch                                  12\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:29:04.617096 UTC | [] Epoch 13 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  15000\n",
      "trainer/num train calls             14000\n",
      "trainer/QF1 Loss                        0.000192355\n",
      "trainer/QF2 Loss                        0.000195927\n",
      "trainer/Policy Loss                    -4.49794\n",
      "trainer/Q1 Predictions Mean             4.47635\n",
      "trainer/Q1 Predictions Std              0.122587\n",
      "trainer/Q1 Predictions Max              4.58573\n",
      "trainer/Q1 Predictions Min              3.73746\n",
      "trainer/Q2 Predictions Mean             4.47707\n",
      "trainer/Q2 Predictions Std              0.122685\n",
      "trainer/Q2 Predictions Max              4.58873\n",
      "trainer/Q2 Predictions Min              3.75045\n",
      "trainer/Q Targets Mean                  4.46842\n",
      "trainer/Q Targets Std                   0.119271\n",
      "trainer/Q Targets Max                   4.56888\n",
      "trainer/Q Targets Min                   3.74832\n",
      "trainer/Log Pis Mean                   -0.247931\n",
      "trainer/Log Pis Std                     0.539373\n",
      "trainer/Log Pis Max                     0.338948\n",
      "trainer/Log Pis Min                    -3.45187\n",
      "trainer/policy/mean Mean                0.00670772\n",
      "trainer/policy/mean Std                 0.0635014\n",
      "trainer/policy/mean Max                 0.0753266\n",
      "trainer/policy/mean Min                -0.539679\n",
      "trainer/policy/normal/std Mean          0.351188\n",
      "trainer/policy/normal/std Std           0.0298737\n",
      "trainer/policy/normal/std Max           0.543532\n",
      "trainer/policy/normal/std Min           0.264194\n",
      "trainer/policy/normal/log_std Mean     -1.04989\n",
      "trainer/policy/normal/log_std Std       0.0824712\n",
      "trainer/policy/normal/log_std Max      -0.609666\n",
      "trainer/policy/normal/log_std Min      -1.33107\n",
      "trainer/Alpha                           0.0215984\n",
      "trainer/Alpha Loss                     -4.78599\n",
      "exploration/num steps total         15000\n",
      "exploration/num paths total            30\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0102703\n",
      "exploration/Rewards Std                 0.0118986\n",
      "exploration/Rewards Max                -3.75486e-09\n",
      "exploration/Rewards Min                -0.0713693\n",
      "exploration/Returns Mean               -5.13516\n",
      "exploration/Returns Std                 5.10524\n",
      "exploration/Returns Max                -0.0299227\n",
      "exploration/Returns Min               -10.2404\n",
      "exploration/Actions Mean                0.0129608\n",
      "exploration/Actions Std                 0.320211\n",
      "exploration/Actions Max                 0.844804\n",
      "exploration/Actions Min                -0.805412\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -5.13516\n",
      "evaluation/num steps total          69930\n",
      "evaluation/num paths total             70\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -4.66539e-05\n",
      "evaluation/Rewards Std                  3.00412e-05\n",
      "evaluation/Rewards Max                 -8.64588e-12\n",
      "evaluation/Rewards Min                 -0.000125255\n",
      "evaluation/Returns Mean                -0.0466073\n",
      "evaluation/Returns Std                  0.00365548\n",
      "evaluation/Returns Max                 -0.0443783\n",
      "evaluation/Returns Min                 -0.0538848\n",
      "evaluation/Actions Mean                 0.0186482\n",
      "evaluation/Actions Std                  0.0108988\n",
      "evaluation/Actions Max                  0.0353913\n",
      "evaluation/Actions Min                 -0.0232623\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.0466073\n",
      "time/data storing (s)                   0.00517756\n",
      "time/evaluation sampling (s)            8.09013\n",
      "time/exploration sampling (s)           1.89085\n",
      "time/logging (s)                        0.0180817\n",
      "time/sac training (s)                  25.2517\n",
      "time/saving (s)                         0.055031\n",
      "time/training (s)                       4.9755e-05\n",
      "time/epoch (s)                         35.311\n",
      "time/total (s)                        537.41\n",
      "Epoch                                  13\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:29:41.848367 UTC | [] Epoch 14 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  16000\n",
      "trainer/num train calls             15000\n",
      "trainer/QF1 Loss                        0.000146098\n",
      "trainer/QF2 Loss                        0.000111706\n",
      "trainer/Policy Loss                    -4.21916\n",
      "trainer/Q1 Predictions Mean             4.20073\n",
      "trainer/Q1 Predictions Std              0.0817881\n",
      "trainer/Q1 Predictions Max              4.2933\n",
      "trainer/Q1 Predictions Min              3.59889\n",
      "trainer/Q2 Predictions Mean             4.19996\n",
      "trainer/Q2 Predictions Std              0.0835756\n",
      "trainer/Q2 Predictions Max              4.2916\n",
      "trainer/Q2 Predictions Min              3.55736\n",
      "trainer/Q Targets Mean                  4.20369\n",
      "trainer/Q Targets Std                   0.0842499\n",
      "trainer/Q Targets Max                   4.30428\n",
      "trainer/Q Targets Min                   3.51287\n",
      "trainer/Log Pis Mean                   -0.19437\n",
      "trainer/Log Pis Std                     0.641582\n",
      "trainer/Log Pis Max                     0.584458\n",
      "trainer/Log Pis Min                    -5.3477\n",
      "trainer/policy/mean Mean                0.0144105\n",
      "trainer/policy/mean Std                 0.0434724\n",
      "trainer/policy/mean Max                 0.13285\n",
      "trainer/policy/mean Min                -0.11215\n",
      "trainer/policy/normal/std Mean          0.317759\n",
      "trainer/policy/normal/std Std           0.0276332\n",
      "trainer/policy/normal/std Max           0.466754\n",
      "trainer/policy/normal/std Min           0.221612\n",
      "trainer/policy/normal/log_std Mean     -1.14994\n",
      "trainer/policy/normal/log_std Std       0.0819026\n",
      "trainer/policy/normal/log_std Max      -0.761953\n",
      "trainer/policy/normal/log_std Min      -1.50683\n",
      "trainer/Alpha                           0.016388\n",
      "trainer/Alpha Loss                     -4.9103\n",
      "exploration/num steps total         16000\n",
      "exploration/num paths total            32\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.00797756\n",
      "exploration/Rewards Std                 0.009399\n",
      "exploration/Rewards Max                -5.29133e-08\n",
      "exploration/Rewards Min                -0.0597579\n",
      "exploration/Returns Mean               -3.98878\n",
      "exploration/Returns Std                 3.98769\n",
      "exploration/Returns Max                -0.00109182\n",
      "exploration/Returns Min                -7.97647\n",
      "exploration/Actions Mean                0.0118183\n",
      "exploration/Actions Std                 0.282198\n",
      "exploration/Actions Max                 0.773032\n",
      "exploration/Actions Min                -0.69619\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -3.98878\n",
      "evaluation/num steps total          74925\n",
      "evaluation/num paths total             75\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -7.79597e-05\n",
      "evaluation/Rewards Std                  4.76255e-05\n",
      "evaluation/Rewards Max                 -5.13012e-13\n",
      "evaluation/Rewards Min                 -0.000203759\n",
      "evaluation/Returns Mean                -0.0778817\n",
      "evaluation/Returns Std                  0.00441934\n",
      "evaluation/Returns Max                 -0.0736509\n",
      "evaluation/Returns Min                 -0.0857337\n",
      "evaluation/Actions Mean                 0.0243394\n",
      "evaluation/Actions Std                  0.0136817\n",
      "evaluation/Actions Max                  0.0451397\n",
      "evaluation/Actions Min                 -0.0337087\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.0778817\n",
      "time/data storing (s)                   0.00573014\n",
      "time/evaluation sampling (s)            7.9863\n",
      "time/exploration sampling (s)           1.83406\n",
      "time/logging (s)                        0.0687454\n",
      "time/sac training (s)                  25.3272\n",
      "time/saving (s)                         0.00583221\n",
      "time/training (s)                       3.85605e-05\n",
      "time/epoch (s)                         35.2279\n",
      "time/total (s)                        574.678\n",
      "Epoch                                  14\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:30:19.320458 UTC | [] Epoch 15 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  17000\n",
      "trainer/num train calls             16000\n",
      "trainer/QF1 Loss                        0.000426575\n",
      "trainer/QF2 Loss                        0.000319465\n",
      "trainer/Policy Loss                    -3.92917\n",
      "trainer/Q1 Predictions Mean             3.91683\n",
      "trainer/Q1 Predictions Std              0.106672\n",
      "trainer/Q1 Predictions Max              4.04124\n",
      "trainer/Q1 Predictions Min              3.34995\n",
      "trainer/Q2 Predictions Mean             3.91766\n",
      "trainer/Q2 Predictions Std              0.107018\n",
      "trainer/Q2 Predictions Max              4.0732\n",
      "trainer/Q2 Predictions Min              3.34858\n",
      "trainer/Q Targets Mean                  3.932\n",
      "trainer/Q Targets Std                   0.108724\n",
      "trainer/Q Targets Max                   4.08386\n",
      "trainer/Q Targets Min                   3.33477\n",
      "trainer/Log Pis Mean                   -0.0324891\n",
      "trainer/Log Pis Std                     0.635061\n",
      "trainer/Log Pis Max                     0.766632\n",
      "trainer/Log Pis Min                    -3.50846\n",
      "trainer/policy/mean Mean                0.0473946\n",
      "trainer/policy/mean Std                 0.0484923\n",
      "trainer/policy/mean Max                 0.127834\n",
      "trainer/policy/mean Min                -0.281935\n",
      "trainer/policy/normal/std Mean          0.258032\n",
      "trainer/policy/normal/std Std           0.0301659\n",
      "trainer/policy/normal/std Max           0.412734\n",
      "trainer/policy/normal/std Min           0.18355\n",
      "trainer/policy/normal/log_std Mean     -1.36076\n",
      "trainer/policy/normal/log_std Std       0.107656\n",
      "trainer/policy/normal/log_std Max      -0.884952\n",
      "trainer/policy/normal/log_std Min      -1.69527\n",
      "trainer/Alpha                           0.0125742\n",
      "trainer/Alpha Loss                     -4.51828\n",
      "exploration/num steps total         17000\n",
      "exploration/num paths total            34\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0053092\n",
      "exploration/Rewards Std                 0.00708442\n",
      "exploration/Rewards Max                -1.56675e-13\n",
      "exploration/Rewards Min                -0.0431257\n",
      "exploration/Returns Mean               -2.6546\n",
      "exploration/Returns Std                 2.65447\n",
      "exploration/Returns Max                -0.000129399\n",
      "exploration/Returns Min                -5.30907\n",
      "exploration/Actions Mean                0.0431081\n",
      "exploration/Actions Std                 0.226349\n",
      "exploration/Actions Max                 0.656702\n",
      "exploration/Actions Min                -0.648773\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -2.6546\n",
      "evaluation/num steps total          79920\n",
      "evaluation/num paths total             80\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -0.000319344\n",
      "evaluation/Rewards Std                  9.52188e-05\n",
      "evaluation/Rewards Max                 -1.0489e-06\n",
      "evaluation/Rewards Min                 -0.000501929\n",
      "evaluation/Returns Mean                -0.319025\n",
      "evaluation/Returns Std                  0.0199337\n",
      "evaluation/Returns Max                 -0.291091\n",
      "evaluation/Returns Min                 -0.339331\n",
      "evaluation/Actions Mean                 0.0555249\n",
      "evaluation/Actions Std                  0.0105083\n",
      "evaluation/Actions Max                  0.0708469\n",
      "evaluation/Actions Min                  0.00323873\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.319025\n",
      "time/data storing (s)                   0.00546101\n",
      "time/evaluation sampling (s)            7.93262\n",
      "time/exploration sampling (s)           1.79238\n",
      "time/logging (s)                        0.0177863\n",
      "time/sac training (s)                  25.2453\n",
      "time/saving (s)                         0.0067293\n",
      "time/training (s)                       4.4411e-05\n",
      "time/epoch (s)                         35.0003\n",
      "time/total (s)                        612.084\n",
      "Epoch                                  15\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:30:56.544603 UTC | [] Epoch 16 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  18000\n",
      "trainer/num train calls             17000\n",
      "trainer/QF1 Loss                        0.000158085\n",
      "trainer/QF2 Loss                        0.000150068\n",
      "trainer/Policy Loss                    -3.69202\n",
      "trainer/Q1 Predictions Mean             3.67298\n",
      "trainer/Q1 Predictions Std              0.107965\n",
      "trainer/Q1 Predictions Max              3.78501\n",
      "trainer/Q1 Predictions Min              3.00753\n",
      "trainer/Q2 Predictions Mean             3.67162\n",
      "trainer/Q2 Predictions Std              0.108745\n",
      "trainer/Q2 Predictions Max              3.77068\n",
      "trainer/Q2 Predictions Min              3.00592\n",
      "trainer/Q Targets Mean                  3.68006\n",
      "trainer/Q Targets Std                   0.110373\n",
      "trainer/Q Targets Max                   3.77913\n",
      "trainer/Q Targets Min                   3.01397\n",
      "trainer/Log Pis Mean                    0.207272\n",
      "trainer/Log Pis Std                     0.599239\n",
      "trainer/Log Pis Max                     0.733301\n",
      "trainer/Log Pis Min                    -2.83055\n",
      "trainer/policy/mean Mean                0.0136768\n",
      "trainer/policy/mean Std                 0.0496126\n",
      "trainer/policy/mean Max                 0.137042\n",
      "trainer/policy/mean Min                -0.276714\n",
      "trainer/policy/normal/std Mean          0.209983\n",
      "trainer/policy/normal/std Std           0.0200994\n",
      "trainer/policy/normal/std Max           0.333166\n",
      "trainer/policy/normal/std Min           0.166488\n",
      "trainer/policy/normal/log_std Mean     -1.56467\n",
      "trainer/policy/normal/log_std Std       0.0857863\n",
      "trainer/policy/normal/log_std Max      -1.09911\n",
      "trainer/policy/normal/log_std Min      -1.79283\n",
      "trainer/Alpha                           0.00971857\n",
      "trainer/Alpha Loss                     -3.67327\n",
      "exploration/num steps total         18000\n",
      "exploration/num paths total            36\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.00390897\n",
      "exploration/Rewards Std                 0.00499731\n",
      "exploration/Rewards Max                -5.86192e-09\n",
      "exploration/Rewards Min                -0.0344403\n",
      "exploration/Returns Mean               -1.95449\n",
      "exploration/Returns Std                 1.94196\n",
      "exploration/Returns Max                -0.0125225\n",
      "exploration/Returns Min                -3.89645\n",
      "exploration/Actions Mean                0.0198638\n",
      "exploration/Actions Std                 0.196711\n",
      "exploration/Actions Max                 0.586858\n",
      "exploration/Actions Min                -0.541204\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -1.95449\n",
      "evaluation/num steps total          84915\n",
      "evaluation/num paths total             85\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -7.00853e-05\n",
      "evaluation/Rewards Std                  3.56015e-05\n",
      "evaluation/Rewards Max                 -3.55271e-16\n",
      "evaluation/Rewards Min                 -0.00018346\n",
      "evaluation/Returns Mean                -0.0700152\n",
      "evaluation/Returns Std                  0.00264398\n",
      "evaluation/Returns Max                 -0.0660945\n",
      "evaluation/Returns Min                 -0.0723745\n",
      "evaluation/Actions Mean                 0.0234417\n",
      "evaluation/Actions Std                  0.0123021\n",
      "evaluation/Actions Max                  0.0428323\n",
      "evaluation/Actions Min                 -0.0393999\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.0700152\n",
      "time/data storing (s)                   0.00542227\n",
      "time/evaluation sampling (s)            8.00253\n",
      "time/exploration sampling (s)           1.88298\n",
      "time/logging (s)                        0.0165171\n",
      "time/sac training (s)                  24.9168\n",
      "time/saving (s)                         0.0056652\n",
      "time/training (s)                       3.91323e-05\n",
      "time/epoch (s)                         34.83\n",
      "time/total (s)                        649.293\n",
      "Epoch                                  16\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:31:33.412649 UTC | [] Epoch 17 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  19000\n",
      "trainer/num train calls             18000\n",
      "trainer/QF1 Loss                        0.0483481\n",
      "trainer/QF2 Loss                        0.0483123\n",
      "trainer/Policy Loss                    -3.4568\n",
      "trainer/Q1 Predictions Mean             3.44038\n",
      "trainer/Q1 Predictions Std              0.104271\n",
      "trainer/Q1 Predictions Max              3.57212\n",
      "trainer/Q1 Predictions Min              2.87588\n",
      "trainer/Q2 Predictions Mean             3.44148\n",
      "trainer/Q2 Predictions Std              0.105457\n",
      "trainer/Q2 Predictions Max              3.54743\n",
      "trainer/Q2 Predictions Min              2.84916\n",
      "trainer/Q Targets Mean                  3.43575\n",
      "trainer/Q Targets Std                   0.24004\n",
      "trainer/Q Targets Max                   3.56005\n",
      "trainer/Q Targets Min                  -0.00369045\n",
      "trainer/Log Pis Mean                    0.29203\n",
      "trainer/Log Pis Std                     0.707651\n",
      "trainer/Log Pis Max                     0.851848\n",
      "trainer/Log Pis Min                    -3.8333\n",
      "trainer/policy/mean Mean                0.0346763\n",
      "trainer/policy/mean Std                 0.0622608\n",
      "trainer/policy/mean Max                 0.166421\n",
      "trainer/policy/mean Min                -0.200908\n",
      "trainer/policy/normal/std Mean          0.192438\n",
      "trainer/policy/normal/std Std           0.0180279\n",
      "trainer/policy/normal/std Max           0.274203\n",
      "trainer/policy/normal/std Min           0.132527\n",
      "trainer/policy/normal/log_std Mean     -1.65196\n",
      "trainer/policy/normal/log_std Std       0.0872523\n",
      "trainer/policy/normal/log_std Max      -1.29389\n",
      "trainer/policy/normal/log_std Min      -2.02097\n",
      "trainer/Alpha                           0.00748313\n",
      "trainer/Alpha Loss                     -3.46559\n",
      "exploration/num steps total         19000\n",
      "exploration/num paths total            38\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.00336793\n",
      "exploration/Rewards Std                 0.00434253\n",
      "exploration/Rewards Max                -2.33559e-09\n",
      "exploration/Rewards Min                -0.0313859\n",
      "exploration/Returns Mean               -1.68397\n",
      "exploration/Returns Std                 1.68365\n",
      "exploration/Returns Max                -0.000315038\n",
      "exploration/Returns Min                -3.36762\n",
      "exploration/Actions Mean                0.0377536\n",
      "exploration/Actions Std                 0.179594\n",
      "exploration/Actions Max                 0.560231\n",
      "exploration/Actions Min                -0.438779\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -1.68397\n",
      "evaluation/num steps total          89910\n",
      "evaluation/num paths total             90\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -0.000193929\n",
      "evaluation/Rewards Std                  7.7201e-05\n",
      "evaluation/Rewards Max                 -1.2451e-10\n",
      "evaluation/Rewards Min                 -0.00050463\n",
      "evaluation/Returns Mean                -0.193735\n",
      "evaluation/Returns Std                  0.00244607\n",
      "evaluation/Returns Max                 -0.190502\n",
      "evaluation/Returns Min                 -0.198062\n",
      "evaluation/Actions Mean                 0.0426016\n",
      "evaluation/Actions Std                  0.0111532\n",
      "evaluation/Actions Max                  0.0710373\n",
      "evaluation/Actions Min                 -0.0175065\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.193735\n",
      "time/data storing (s)                   0.0535734\n",
      "time/evaluation sampling (s)            7.92714\n",
      "time/exploration sampling (s)           1.7191\n",
      "time/logging (s)                        0.0172929\n",
      "time/sac training (s)                  24.7133\n",
      "time/saving (s)                         0.0548871\n",
      "time/training (s)                       4.57726e-05\n",
      "time/epoch (s)                         34.4854\n",
      "time/total (s)                        686.148\n",
      "Epoch                                  17\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:32:10.330048 UTC | [] Epoch 18 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  20000\n",
      "trainer/num train calls             19000\n",
      "trainer/QF1 Loss                        0.000521736\n",
      "trainer/QF2 Loss                        0.000491798\n",
      "trainer/Policy Loss                    -3.27031\n",
      "trainer/Q1 Predictions Mean             3.25741\n",
      "trainer/Q1 Predictions Std              0.095351\n",
      "trainer/Q1 Predictions Max              3.34592\n",
      "trainer/Q1 Predictions Min              2.68357\n",
      "trainer/Q2 Predictions Mean             3.25645\n",
      "trainer/Q2 Predictions Std              0.0955555\n",
      "trainer/Q2 Predictions Max              3.34553\n",
      "trainer/Q2 Predictions Min              2.66518\n",
      "trainer/Q Targets Mean                  3.2379\n",
      "trainer/Q Targets Std                   0.0953487\n",
      "trainer/Q Targets Max                   3.32594\n",
      "trainer/Q Targets Min                   2.69713\n",
      "trainer/Log Pis Mean                    0.382744\n",
      "trainer/Log Pis Std                     0.662943\n",
      "trainer/Log Pis Max                     1.05357\n",
      "trainer/Log Pis Min                    -4.16491\n",
      "trainer/policy/mean Mean                0.0184491\n",
      "trainer/policy/mean Std                 0.0587395\n",
      "trainer/policy/mean Max                 0.168313\n",
      "trainer/policy/mean Min                -0.28981\n",
      "trainer/policy/normal/std Mean          0.170382\n",
      "trainer/policy/normal/std Std           0.0119105\n",
      "trainer/policy/normal/std Max           0.221821\n",
      "trainer/policy/normal/std Min           0.124163\n",
      "trainer/policy/normal/log_std Mean     -1.77205\n",
      "trainer/policy/normal/log_std Std       0.0677249\n",
      "trainer/policy/normal/log_std Max      -1.50588\n",
      "trainer/policy/normal/log_std Min      -2.08616\n",
      "trainer/Alpha                           0.00584321\n",
      "trainer/Alpha Loss                     -3.17423\n",
      "exploration/num steps total         20000\n",
      "exploration/num paths total            40\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0025446\n",
      "exploration/Rewards Std                 0.00343384\n",
      "exploration/Rewards Max                -1.17551e-09\n",
      "exploration/Rewards Min                -0.0254881\n",
      "exploration/Returns Mean               -1.2723\n",
      "exploration/Returns Std                 1.27029\n",
      "exploration/Returns Max                -0.00201471\n",
      "exploration/Returns Min                -2.54259\n",
      "exploration/Actions Mean                0.0134235\n",
      "exploration/Actions Std                 0.158952\n",
      "exploration/Actions Max                 0.504857\n",
      "exploration/Actions Min                -0.444228\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -1.2723\n",
      "evaluation/num steps total          94905\n",
      "evaluation/num paths total             95\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -6.25769e-05\n",
      "evaluation/Rewards Std                  4.00613e-05\n",
      "evaluation/Rewards Max                 -5.54312e-11\n",
      "evaluation/Rewards Min                 -0.000291105\n",
      "evaluation/Returns Mean                -0.0625144\n",
      "evaluation/Returns Std                  0.00410781\n",
      "evaluation/Returns Max                 -0.0584215\n",
      "evaluation/Returns Min                 -0.0689041\n",
      "evaluation/Actions Mean                 0.0227744\n",
      "evaluation/Actions Std                  0.0103487\n",
      "evaluation/Actions Max                  0.0539542\n",
      "evaluation/Actions Min                 -0.0321831\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.0625144\n",
      "time/data storing (s)                   0.0052637\n",
      "time/evaluation sampling (s)            7.81796\n",
      "time/exploration sampling (s)           1.85884\n",
      "time/logging (s)                        0.0165449\n",
      "time/sac training (s)                  24.838\n",
      "time/saving (s)                         0.00576127\n",
      "time/training (s)                       4.68064e-05\n",
      "time/epoch (s)                         34.5424\n",
      "time/total (s)                        723.05\n",
      "Epoch                                  18\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:32:47.348710 UTC | [] Epoch 19 finished\n",
      "----------------------------------  ---------------\n",
      "replay_buffer/size                  21000\n",
      "trainer/num train calls             20000\n",
      "trainer/QF1 Loss                        0.000279674\n",
      "trainer/QF2 Loss                        0.000281616\n",
      "trainer/Policy Loss                    -3.0698\n",
      "trainer/Q1 Predictions Mean             3.05419\n",
      "trainer/Q1 Predictions Std              0.0827818\n",
      "trainer/Q1 Predictions Max              3.13839\n",
      "trainer/Q1 Predictions Min              2.68581\n",
      "trainer/Q2 Predictions Mean             3.05451\n",
      "trainer/Q2 Predictions Std              0.0836968\n",
      "trainer/Q2 Predictions Max              3.14173\n",
      "trainer/Q2 Predictions Min              2.69397\n",
      "trainer/Q Targets Mean                  3.03962\n",
      "trainer/Q Targets Std                   0.0834194\n",
      "trainer/Q Targets Max                   3.12299\n",
      "trainer/Q Targets Min                   2.67342\n",
      "trainer/Log Pis Mean                    0.498177\n",
      "trainer/Log Pis Std                     0.632251\n",
      "trainer/Log Pis Max                     1.11955\n",
      "trainer/Log Pis Min                    -3.56808\n",
      "trainer/policy/mean Mean                0.0127437\n",
      "trainer/policy/mean Std                 0.0434466\n",
      "trainer/policy/mean Max                 0.201003\n",
      "trainer/policy/mean Min                -0.108154\n",
      "trainer/policy/normal/std Mean          0.149657\n",
      "trainer/policy/normal/std Std           0.00728905\n",
      "trainer/policy/normal/std Max           0.184359\n",
      "trainer/policy/normal/std Min           0.120207\n",
      "trainer/policy/normal/log_std Mean     -1.90058\n",
      "trainer/policy/normal/log_std Std       0.0481996\n",
      "trainer/policy/normal/log_std Max      -1.69087\n",
      "trainer/policy/normal/log_std Min      -2.11854\n",
      "trainer/Alpha                           0.00458311\n",
      "trainer/Alpha Loss                     -2.7025\n",
      "exploration/num steps total         21000\n",
      "exploration/num paths total            42\n",
      "exploration/path length Mean          500\n",
      "exploration/path length Std           499\n",
      "exploration/path length Max           999\n",
      "exploration/path length Min             1\n",
      "exploration/Rewards Mean               -0.0020301\n",
      "exploration/Rewards Std                 0.00288308\n",
      "exploration/Rewards Max                -1.32061e-09\n",
      "exploration/Rewards Min                -0.0213001\n",
      "exploration/Returns Mean               -1.01505\n",
      "exploration/Returns Std                 1.01262\n",
      "exploration/Returns Max                -0.00242696\n",
      "exploration/Returns Min                -2.02767\n",
      "exploration/Actions Mean                0.0115603\n",
      "exploration/Actions Std                 0.142012\n",
      "exploration/Actions Max                 0.450579\n",
      "exploration/Actions Min                -0.46152\n",
      "exploration/Num Paths                   2\n",
      "exploration/Average Returns            -1.01505\n",
      "evaluation/num steps total          99900\n",
      "evaluation/num paths total            100\n",
      "evaluation/path length Mean           999\n",
      "evaluation/path length Std              0\n",
      "evaluation/path length Max            999\n",
      "evaluation/path length Min            999\n",
      "evaluation/Rewards Mean                -2.696e-05\n",
      "evaluation/Rewards Std                  3.07423e-05\n",
      "evaluation/Rewards Max                 -2.40163e-11\n",
      "evaluation/Rewards Min                 -0.000240163\n",
      "evaluation/Returns Mean                -0.026933\n",
      "evaluation/Returns Std                  0.00772749\n",
      "evaluation/Returns Max                 -0.0188989\n",
      "evaluation/Returns Min                 -0.0369146\n",
      "evaluation/Actions Mean                 0.0123124\n",
      "evaluation/Actions Std                  0.010863\n",
      "evaluation/Actions Max                  0.0490065\n",
      "evaluation/Actions Min                 -0.0327904\n",
      "evaluation/Num Paths                    5\n",
      "evaluation/Average Returns             -0.026933\n",
      "time/data storing (s)                   0.053443\n",
      "time/evaluation sampling (s)            8.02621\n",
      "time/exploration sampling (s)           1.89477\n",
      "time/logging (s)                        0.0696067\n",
      "time/sac training (s)                  24.8685\n",
      "time/saving (s)                         0.00636406\n",
      "time/training (s)                       3.9475e-05\n",
      "time/epoch (s)                         34.9189\n",
      "time/total (s)                        760.108\n",
      "Epoch                                  19\n",
      "----------------------------------  ---------------\n",
      "2021-03-01 09:33:24.637776 UTC | [] Epoch 20 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   22000\n",
      "trainer/num train calls              21000\n",
      "trainer/QF1 Loss                         0.000146349\n",
      "trainer/QF2 Loss                         0.000180546\n",
      "trainer/Policy Loss                     -2.85798\n",
      "trainer/Q1 Predictions Mean              2.84509\n",
      "trainer/Q1 Predictions Std               0.0810088\n",
      "trainer/Q1 Predictions Max               2.99445\n",
      "trainer/Q1 Predictions Min               2.48187\n",
      "trainer/Q2 Predictions Mean              2.84346\n",
      "trainer/Q2 Predictions Std               0.0816948\n",
      "trainer/Q2 Predictions Max               3.00993\n",
      "trainer/Q2 Predictions Min               2.47976\n",
      "trainer/Q Targets Mean                   2.85227\n",
      "trainer/Q Targets Std                    0.0810456\n",
      "trainer/Q Targets Max                    2.98986\n",
      "trainer/Q Targets Min                    2.4784\n",
      "trainer/Log Pis Mean                     0.587129\n",
      "trainer/Log Pis Std                      0.64174\n",
      "trainer/Log Pis Max                      1.42924\n",
      "trainer/Log Pis Min                     -2.68825\n",
      "trainer/policy/mean Mean                 0.0394028\n",
      "trainer/policy/mean Std                  0.0500071\n",
      "trainer/policy/mean Max                  0.234769\n",
      "trainer/policy/mean Min                 -0.0757336\n",
      "trainer/policy/normal/std Mean           0.13733\n",
      "trainer/policy/normal/std Std            0.00990385\n",
      "trainer/policy/normal/std Max            0.175649\n",
      "trainer/policy/normal/std Min            0.0880432\n",
      "trainer/policy/normal/log_std Mean      -1.98824\n",
      "trainer/policy/normal/log_std Std        0.077937\n",
      "trainer/policy/normal/log_std Max       -1.73927\n",
      "trainer/policy/normal/log_std Min       -2.42993\n",
      "trainer/Alpha                            0.00363349\n",
      "trainer/Alpha Loss                      -2.31933\n",
      "exploration/num steps total          22000\n",
      "exploration/num paths total             44\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.00189542\n",
      "exploration/Rewards Std                  0.00253978\n",
      "exploration/Rewards Max                 -3.64708e-09\n",
      "exploration/Rewards Min                 -0.0251143\n",
      "exploration/Returns Mean                -0.947712\n",
      "exploration/Returns Std                  0.944029\n",
      "exploration/Returns Max                 -0.00368296\n",
      "exploration/Returns Min                 -1.89174\n",
      "exploration/Actions Mean                 0.035124\n",
      "exploration/Actions Std                  0.133119\n",
      "exploration/Actions Max                  0.501142\n",
      "exploration/Actions Min                 -0.341818\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.947712\n",
      "evaluation/num steps total          104895\n",
      "evaluation/num paths total             105\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -9.35785e-05\n",
      "evaluation/Rewards Std                   4.36772e-05\n",
      "evaluation/Rewards Max                  -1.51202e-09\n",
      "evaluation/Rewards Min                  -0.000326998\n",
      "evaluation/Returns Mean                 -0.0934849\n",
      "evaluation/Returns Std                   0.00295886\n",
      "evaluation/Returns Max                  -0.0896579\n",
      "evaluation/Returns Min                  -0.0976911\n",
      "evaluation/Actions Mean                  0.0295964\n",
      "evaluation/Actions Std                   0.00773541\n",
      "evaluation/Actions Max                   0.0571837\n",
      "evaluation/Actions Min                  -0.00867666\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0934849\n",
      "time/data storing (s)                    0.00593634\n",
      "time/evaluation sampling (s)             8.12567\n",
      "time/exploration sampling (s)            1.87806\n",
      "time/logging (s)                         0.0159165\n",
      "time/sac training (s)                   25.0079\n",
      "time/saving (s)                          0.00561236\n",
      "time/training (s)                        4.04697e-05\n",
      "time/epoch (s)                          35.0392\n",
      "time/total (s)                         797.329\n",
      "Epoch                                   20\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:34:01.838618 UTC | [] Epoch 21 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   23000\n",
      "trainer/num train calls              22000\n",
      "trainer/QF1 Loss                         7.94621e-05\n",
      "trainer/QF2 Loss                         0.000107813\n",
      "trainer/Policy Loss                     -2.67934\n",
      "trainer/Q1 Predictions Mean              2.66458\n",
      "trainer/Q1 Predictions Std               0.0943795\n",
      "trainer/Q1 Predictions Max               2.82476\n",
      "trainer/Q1 Predictions Min               2.29484\n",
      "trainer/Q2 Predictions Mean              2.66381\n",
      "trainer/Q2 Predictions Std               0.094174\n",
      "trainer/Q2 Predictions Max               2.84199\n",
      "trainer/Q2 Predictions Min               2.28792\n",
      "trainer/Q Targets Mean                   2.66624\n",
      "trainer/Q Targets Std                    0.0931516\n",
      "trainer/Q Targets Max                    2.82488\n",
      "trainer/Q Targets Min                    2.27984\n",
      "trainer/Log Pis Mean                     0.730625\n",
      "trainer/Log Pis Std                      0.615231\n",
      "trainer/Log Pis Max                      1.49546\n",
      "trainer/Log Pis Min                     -3.36256\n",
      "trainer/policy/mean Mean                 0.0386091\n",
      "trainer/policy/mean Std                  0.045554\n",
      "trainer/policy/mean Max                  0.263557\n",
      "trainer/policy/mean Min                 -0.0753405\n",
      "trainer/policy/normal/std Mean           0.122769\n",
      "trainer/policy/normal/std Std            0.0115078\n",
      "trainer/policy/normal/std Max            0.162056\n",
      "trainer/policy/normal/std Min            0.0779966\n",
      "trainer/policy/normal/log_std Mean      -2.10216\n",
      "trainer/policy/normal/log_std Std        0.0993065\n",
      "trainer/policy/normal/log_std Max       -1.81981\n",
      "trainer/policy/normal/log_std Min       -2.55109\n",
      "trainer/Alpha                            0.00287046\n",
      "trainer/Alpha Loss                      -1.57673\n",
      "exploration/num steps total          23000\n",
      "exploration/num paths total             46\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.00167411\n",
      "exploration/Rewards Std                  0.00231888\n",
      "exploration/Rewards Max                 -6.49281e-09\n",
      "exploration/Rewards Min                 -0.0177046\n",
      "exploration/Returns Mean                -0.837057\n",
      "exploration/Returns Std                  0.836452\n",
      "exploration/Returns Max                 -0.00060524\n",
      "exploration/Returns Min                 -1.67351\n",
      "exploration/Actions Mean                 0.0337236\n",
      "exploration/Actions Std                  0.124915\n",
      "exploration/Actions Max                  0.420768\n",
      "exploration/Actions Min                 -0.341769\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.837057\n",
      "evaluation/num steps total          109890\n",
      "evaluation/num paths total             110\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -0.00010888\n",
      "evaluation/Rewards Std                   5.26717e-05\n",
      "evaluation/Rewards Max                  -9.24061e-11\n",
      "evaluation/Rewards Min                  -0.000375616\n",
      "evaluation/Returns Mean                 -0.108771\n",
      "evaluation/Returns Std                   0.00377823\n",
      "evaluation/Returns Max                  -0.104529\n",
      "evaluation/Returns Min                  -0.113375\n",
      "evaluation/Actions Mean                  0.031892\n",
      "evaluation/Actions Std                   0.0084677\n",
      "evaluation/Actions Max                   0.0612875\n",
      "evaluation/Actions Min                  -0.0015793\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.108771\n",
      "time/data storing (s)                    0.00494821\n",
      "time/evaluation sampling (s)             8.20544\n",
      "time/exploration sampling (s)            1.87308\n",
      "time/logging (s)                         0.0172101\n",
      "time/sac training (s)                   24.7709\n",
      "time/saving (s)                          0.00623976\n",
      "time/training (s)                        4.59105e-05\n",
      "time/epoch (s)                          34.8779\n",
      "time/total (s)                         834.413\n",
      "Epoch                                   21\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:34:38.742019 UTC | [] Epoch 22 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   24000\n",
      "trainer/num train calls              23000\n",
      "trainer/QF1 Loss                         0.000119066\n",
      "trainer/QF2 Loss                         0.000109834\n",
      "trainer/Policy Loss                     -2.51184\n",
      "trainer/Q1 Predictions Mean              2.49748\n",
      "trainer/Q1 Predictions Std               0.0851086\n",
      "trainer/Q1 Predictions Max               2.664\n",
      "trainer/Q1 Predictions Min               2.12994\n",
      "trainer/Q2 Predictions Mean              2.4976\n",
      "trainer/Q2 Predictions Std               0.085528\n",
      "trainer/Q2 Predictions Max               2.67484\n",
      "trainer/Q2 Predictions Min               2.11823\n",
      "trainer/Q Targets Mean                   2.50394\n",
      "trainer/Q Targets Std                    0.084602\n",
      "trainer/Q Targets Max                    2.66568\n",
      "trainer/Q Targets Min                    2.13234\n",
      "trainer/Log Pis Mean                     0.767555\n",
      "trainer/Log Pis Std                      0.677275\n",
      "trainer/Log Pis Max                      1.6859\n",
      "trainer/Log Pis Min                     -2.39076\n",
      "trainer/policy/mean Mean                 0.0110957\n",
      "trainer/policy/mean Std                  0.0459227\n",
      "trainer/policy/mean Max                  0.175606\n",
      "trainer/policy/mean Min                 -0.239701\n",
      "trainer/policy/normal/std Mean           0.113376\n",
      "trainer/policy/normal/std Std            0.011531\n",
      "trainer/policy/normal/std Max            0.149967\n",
      "trainer/policy/normal/std Min            0.0702557\n",
      "trainer/policy/normal/log_std Mean      -2.18282\n",
      "trainer/policy/normal/log_std Std        0.111096\n",
      "trainer/policy/normal/log_std Max       -1.89734\n",
      "trainer/policy/normal/log_std Min       -2.65561\n",
      "trainer/Alpha                            0.00230578\n",
      "trainer/Alpha Loss                      -1.41148\n",
      "exploration/num steps total          24000\n",
      "exploration/num paths total             48\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.00125514\n",
      "exploration/Rewards Std                  0.00181114\n",
      "exploration/Rewards Max                 -1.05103e-09\n",
      "exploration/Rewards Min                 -0.0135393\n",
      "exploration/Returns Mean                -0.627572\n",
      "exploration/Returns Std                  0.627369\n",
      "exploration/Returns Max                 -0.000203218\n",
      "exploration/Returns Min                 -1.25494\n",
      "exploration/Actions Mean                 0.0137432\n",
      "exploration/Actions Std                  0.111187\n",
      "exploration/Actions Max                  0.351778\n",
      "exploration/Actions Min                 -0.367958\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.627572\n",
      "evaluation/num steps total          114885\n",
      "evaluation/num paths total             115\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.6905e-05\n",
      "evaluation/Rewards Std                   1.47402e-05\n",
      "evaluation/Rewards Max                  -2.27374e-12\n",
      "evaluation/Rewards Min                  -0.000137541\n",
      "evaluation/Returns Mean                 -0.0168881\n",
      "evaluation/Returns Std                   0.00321117\n",
      "evaluation/Returns Max                  -0.0141255\n",
      "evaluation/Returns Min                  -0.0226822\n",
      "evaluation/Actions Mean                  0.011414\n",
      "evaluation/Actions Std                   0.00622655\n",
      "evaluation/Actions Max                   0.0370865\n",
      "evaluation/Actions Min                  -0.0241858\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0168881\n",
      "time/data storing (s)                    0.00510082\n",
      "time/evaluation sampling (s)             7.99345\n",
      "time/exploration sampling (s)            1.8861\n",
      "time/logging (s)                         0.0740622\n",
      "time/sac training (s)                   24.914\n",
      "time/saving (s)                          0.00658331\n",
      "time/training (s)                        6.64257e-05\n",
      "time/epoch (s)                          34.8794\n",
      "time/total (s)                         871.307\n",
      "Epoch                                   22\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:35:15.450499 UTC | [] Epoch 23 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   25000\n",
      "trainer/num train calls              24000\n",
      "trainer/QF1 Loss                         0.000144386\n",
      "trainer/QF2 Loss                         0.000142477\n",
      "trainer/Policy Loss                     -2.36862\n",
      "trainer/Q1 Predictions Mean              2.3547\n",
      "trainer/Q1 Predictions Std               0.0702331\n",
      "trainer/Q1 Predictions Max               2.45871\n",
      "trainer/Q1 Predictions Min               1.98689\n",
      "trainer/Q2 Predictions Mean              2.35489\n",
      "trainer/Q2 Predictions Std               0.0704651\n",
      "trainer/Q2 Predictions Max               2.47631\n",
      "trainer/Q2 Predictions Min               1.99191\n",
      "trainer/Q Targets Mean                   2.36335\n",
      "trainer/Q Targets Std                    0.069656\n",
      "trainer/Q Targets Max                    2.48556\n",
      "trainer/Q Targets Min                    1.98664\n",
      "trainer/Log Pis Mean                     1.0836\n",
      "trainer/Log Pis Std                      0.595038\n",
      "trainer/Log Pis Max                      1.96107\n",
      "trainer/Log Pis Min                     -2.163\n",
      "trainer/policy/mean Mean                 0.00815082\n",
      "trainer/policy/mean Std                  0.0378453\n",
      "trainer/policy/mean Max                  0.260838\n",
      "trainer/policy/mean Min                 -0.0948066\n",
      "trainer/policy/normal/std Mean           0.0903358\n",
      "trainer/policy/normal/std Std            0.00894613\n",
      "trainer/policy/normal/std Max            0.109585\n",
      "trainer/policy/normal/std Min            0.0559327\n",
      "trainer/policy/normal/log_std Mean      -2.40985\n",
      "trainer/policy/normal/log_std Std        0.110288\n",
      "trainer/policy/normal/log_std Max       -2.21106\n",
      "trainer/policy/normal/log_std Min       -2.88361\n",
      "trainer/Alpha                            0.00190333\n",
      "trainer/Alpha Loss                       0.523703\n",
      "exploration/num steps total          25000\n",
      "exploration/num paths total             50\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000806429\n",
      "exploration/Rewards Std                  0.00117504\n",
      "exploration/Rewards Max                 -7.32271e-11\n",
      "exploration/Rewards Min                 -0.00818478\n",
      "exploration/Returns Mean                -0.403214\n",
      "exploration/Returns Std                  0.403065\n",
      "exploration/Returns Max                 -0.000149923\n",
      "exploration/Returns Min                 -0.806279\n",
      "exploration/Actions Mean                 0.000538978\n",
      "exploration/Actions Std                  0.0897998\n",
      "exploration/Actions Max                  0.286091\n",
      "exploration/Actions Min                 -0.274996\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.403214\n",
      "evaluation/num steps total          119880\n",
      "evaluation/num paths total             120\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -3.94028e-06\n",
      "evaluation/Rewards Std                   9.92357e-06\n",
      "evaluation/Rewards Max                  -5.97211e-13\n",
      "evaluation/Rewards Min                  -8.59425e-05\n",
      "evaluation/Returns Mean                 -0.00393634\n",
      "evaluation/Returns Std                   0.00451878\n",
      "evaluation/Returns Max                  -0.000361425\n",
      "evaluation/Returns Min                  -0.0126101\n",
      "evaluation/Actions Mean                  0.00138006\n",
      "evaluation/Actions Std                   0.00612358\n",
      "evaluation/Actions Max                   0.0293159\n",
      "evaluation/Actions Min                  -0.0275719\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.00393634\n",
      "time/data storing (s)                    0.053956\n",
      "time/evaluation sampling (s)             7.92641\n",
      "time/exploration sampling (s)            1.81844\n",
      "time/logging (s)                         0.0683323\n",
      "time/sac training (s)                   24.3051\n",
      "time/saving (s)                          0.00590227\n",
      "time/training (s)                        5.38714e-05\n",
      "time/epoch (s)                          34.1782\n",
      "time/total (s)                         907.995\n",
      "Epoch                                   23\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:35:52.641452 UTC | [] Epoch 24 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   26000\n",
      "trainer/num train calls              25000\n",
      "trainer/QF1 Loss                         0.0189817\n",
      "trainer/QF2 Loss                         0.0190362\n",
      "trainer/Policy Loss                     -2.2221\n",
      "trainer/Q1 Predictions Mean              2.20732\n",
      "trainer/Q1 Predictions Std               0.0813704\n",
      "trainer/Q1 Predictions Max               2.29082\n",
      "trainer/Q1 Predictions Min               1.85207\n",
      "trainer/Q2 Predictions Mean              2.20853\n",
      "trainer/Q2 Predictions Std               0.0801446\n",
      "trainer/Q2 Predictions Max               2.29105\n",
      "trainer/Q2 Predictions Min               1.86075\n",
      "trainer/Q Targets Mean                   2.201\n",
      "trainer/Q Targets Std                    0.159191\n",
      "trainer/Q Targets Max                    2.29022\n",
      "trainer/Q Targets Min                   -0.00878289\n",
      "trainer/Log Pis Mean                     1.08351\n",
      "trainer/Log Pis Std                      0.723671\n",
      "trainer/Log Pis Max                      1.81485\n",
      "trainer/Log Pis Min                     -4.29467\n",
      "trainer/policy/mean Mean                 0.0248537\n",
      "trainer/policy/mean Std                  0.0337134\n",
      "trainer/policy/mean Max                  0.176059\n",
      "trainer/policy/mean Min                 -0.110235\n",
      "trainer/policy/normal/std Mean           0.0848191\n",
      "trainer/policy/normal/std Std            0.010353\n",
      "trainer/policy/normal/std Max            0.128129\n",
      "trainer/policy/normal/std Min            0.0589102\n",
      "trainer/policy/normal/log_std Mean      -2.47418\n",
      "trainer/policy/normal/log_std Std        0.116167\n",
      "trainer/policy/normal/log_std Max       -2.05472\n",
      "trainer/policy/normal/log_std Min       -2.83174\n",
      "trainer/Alpha                            0.00179208\n",
      "trainer/Alpha Loss                       0.528165\n",
      "exploration/num steps total          26000\n",
      "exploration/num paths total             52\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000671187\n",
      "exploration/Rewards Std                  0.000904044\n",
      "exploration/Rewards Max                 -7.99361e-12\n",
      "exploration/Rewards Min                 -0.00720039\n",
      "exploration/Returns Mean                -0.335594\n",
      "exploration/Returns Std                  0.335176\n",
      "exploration/Returns Max                 -0.000417921\n",
      "exploration/Returns Min                 -0.670769\n",
      "exploration/Actions Mean                 0.0130269\n",
      "exploration/Actions Std                  0.0808837\n",
      "exploration/Actions Max                  0.268335\n",
      "exploration/Actions Min                 -0.250031\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.335594\n",
      "evaluation/num steps total          124875\n",
      "evaluation/num paths total             125\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -2.96012e-05\n",
      "evaluation/Rewards Std                   1.68919e-05\n",
      "evaluation/Rewards Max                  -4.50257e-11\n",
      "evaluation/Rewards Min                  -0.000138435\n",
      "evaluation/Returns Mean                 -0.0295716\n",
      "evaluation/Returns Std                   0.00140872\n",
      "evaluation/Returns Max                  -0.0278315\n",
      "evaluation/Returns Min                  -0.0320145\n",
      "evaluation/Actions Mean                  0.0163701\n",
      "evaluation/Actions Std                   0.00529459\n",
      "evaluation/Actions Max                   0.0372069\n",
      "evaluation/Actions Min                  -0.00636339\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0295716\n",
      "time/data storing (s)                    0.00578059\n",
      "time/evaluation sampling (s)             7.90962\n",
      "time/exploration sampling (s)            1.89186\n",
      "time/logging (s)                         0.0725002\n",
      "time/sac training (s)                   25.0781\n",
      "time/saving (s)                          0.00647459\n",
      "time/training (s)                        4.768e-05\n",
      "time/epoch (s)                          34.9643\n",
      "time/total (s)                         945.176\n",
      "Epoch                                   24\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:36:30.225858 UTC | [] Epoch 25 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   27000\n",
      "trainer/num train calls              26000\n",
      "trainer/QF1 Loss                         0.0174757\n",
      "trainer/QF2 Loss                         0.017409\n",
      "trainer/Policy Loss                     -2.08895\n",
      "trainer/Q1 Predictions Mean              2.07486\n",
      "trainer/Q1 Predictions Std               0.0715314\n",
      "trainer/Q1 Predictions Max               2.15186\n",
      "trainer/Q1 Predictions Min               1.68501\n",
      "trainer/Q2 Predictions Mean              2.07501\n",
      "trainer/Q2 Predictions Std               0.0711926\n",
      "trainer/Q2 Predictions Max               2.15043\n",
      "trainer/Q2 Predictions Min               1.69257\n",
      "trainer/Q Targets Mean                   2.06991\n",
      "trainer/Q Targets Std                    0.147611\n",
      "trainer/Q Targets Max                    2.15218\n",
      "trainer/Q Targets Min                   -0.00155753\n",
      "trainer/Log Pis Mean                     0.969528\n",
      "trainer/Log Pis Std                      0.745232\n",
      "trainer/Log Pis Max                      1.97135\n",
      "trainer/Log Pis Min                     -5.23555\n",
      "trainer/policy/mean Mean                 0.0174538\n",
      "trainer/policy/mean Std                  0.0367253\n",
      "trainer/policy/mean Max                  0.220786\n",
      "trainer/policy/mean Min                 -0.092815\n",
      "trainer/policy/normal/std Mean           0.0967953\n",
      "trainer/policy/normal/std Std            0.0126075\n",
      "trainer/policy/normal/std Max            0.132119\n",
      "trainer/policy/normal/std Min            0.0547133\n",
      "trainer/policy/normal/log_std Mean      -2.3442\n",
      "trainer/policy/normal/log_std Std        0.137287\n",
      "trainer/policy/normal/log_std Max       -2.02405\n",
      "trainer/policy/normal/log_std Min       -2.90565\n",
      "trainer/Alpha                            0.00166276\n",
      "trainer/Alpha Loss                      -0.195\n",
      "exploration/num steps total          27000\n",
      "exploration/num paths total             54\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000899978\n",
      "exploration/Rewards Std                  0.00128356\n",
      "exploration/Rewards Max                 -6.18087e-10\n",
      "exploration/Rewards Min                 -0.010137\n",
      "exploration/Returns Mean                -0.449989\n",
      "exploration/Returns Std                  0.449542\n",
      "exploration/Returns Max                 -0.000447235\n",
      "exploration/Returns Min                 -0.899531\n",
      "exploration/Actions Mean                 0.00380034\n",
      "exploration/Actions Std                  0.094791\n",
      "exploration/Actions Max                  0.318387\n",
      "exploration/Actions Min                 -0.285308\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.449989\n",
      "evaluation/num steps total          129870\n",
      "evaluation/num paths total             130\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -2.13881e-06\n",
      "evaluation/Rewards Std                   4.60331e-06\n",
      "evaluation/Rewards Max                  -6.96332e-14\n",
      "evaluation/Rewards Min                  -4.84832e-05\n",
      "evaluation/Returns Mean                 -0.00213668\n",
      "evaluation/Returns Std                   0.00193247\n",
      "evaluation/Returns Max                  -0.000555444\n",
      "evaluation/Returns Min                  -0.00548562\n",
      "evaluation/Actions Mean                  0.00247266\n",
      "evaluation/Actions Std                   0.00390821\n",
      "evaluation/Actions Max                   0.0220189\n",
      "evaluation/Actions Min                  -0.0127474\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.00213668\n",
      "time/data storing (s)                    0.053796\n",
      "time/evaluation sampling (s)             8.22553\n",
      "time/exploration sampling (s)            1.81399\n",
      "time/logging (s)                         0.0176576\n",
      "time/sac training (s)                   25.232\n",
      "time/saving (s)                          0.00666033\n",
      "time/training (s)                        4.84046e-05\n",
      "time/epoch (s)                          35.3497\n",
      "time/total (s)                         982.639\n",
      "Epoch                                   25\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:37:07.021816 UTC | [] Epoch 26 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   28000\n",
      "trainer/num train calls              27000\n",
      "trainer/QF1 Loss                         0.0151391\n",
      "trainer/QF2 Loss                         0.0151226\n",
      "trainer/Policy Loss                     -1.96352\n",
      "trainer/Q1 Predictions Mean              1.95466\n",
      "trainer/Q1 Predictions Std               0.0865651\n",
      "trainer/Q1 Predictions Max               2.04728\n",
      "trainer/Q1 Predictions Min               1.62358\n",
      "trainer/Q2 Predictions Mean              1.95401\n",
      "trainer/Q2 Predictions Std               0.0867156\n",
      "trainer/Q2 Predictions Max               2.03176\n",
      "trainer/Q2 Predictions Min               1.61125\n",
      "trainer/Q Targets Mean                   1.94128\n",
      "trainer/Q Targets Std                    0.151158\n",
      "trainer/Q Targets Max                    2.03134\n",
      "trainer/Q Targets Min                   -0.0325234\n",
      "trainer/Log Pis Mean                     1.03851\n",
      "trainer/Log Pis Std                      0.738056\n",
      "trainer/Log Pis Max                      2.08259\n",
      "trainer/Log Pis Min                     -2.36589\n",
      "trainer/policy/mean Mean                 0.00295035\n",
      "trainer/policy/mean Std                  0.0435793\n",
      "trainer/policy/mean Max                  0.285411\n",
      "trainer/policy/mean Min                 -0.055559\n",
      "trainer/policy/normal/std Mean           0.087358\n",
      "trainer/policy/normal/std Std            0.0118638\n",
      "trainer/policy/normal/std Max            0.124961\n",
      "trainer/policy/normal/std Min            0.0478485\n",
      "trainer/policy/normal/log_std Mean      -2.44698\n",
      "trainer/policy/normal/log_std Std        0.136866\n",
      "trainer/policy/normal/log_std Max       -2.07975\n",
      "trainer/policy/normal/log_std Min       -3.03972\n",
      "trainer/Alpha                            0.00160811\n",
      "trainer/Alpha Loss                       0.247717\n",
      "exploration/num steps total          28000\n",
      "exploration/num paths total             56\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000725451\n",
      "exploration/Rewards Std                  0.00100665\n",
      "exploration/Rewards Max                 -6.52287e-10\n",
      "exploration/Rewards Min                 -0.0122621\n",
      "exploration/Returns Mean                -0.362726\n",
      "exploration/Returns Std                  0.362514\n",
      "exploration/Returns Max                 -0.000211683\n",
      "exploration/Returns Min                 -0.725239\n",
      "exploration/Actions Mean                -0.0116674\n",
      "exploration/Actions Std                  0.0843705\n",
      "exploration/Actions Max                  0.242724\n",
      "exploration/Actions Min                 -0.350172\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.362726\n",
      "evaluation/num steps total          134865\n",
      "evaluation/num paths total             135\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -2.09043e-05\n",
      "evaluation/Rewards Std                   1.14366e-05\n",
      "evaluation/Rewards Max                  -3.13918e-12\n",
      "evaluation/Rewards Min                  -7.21251e-05\n",
      "evaluation/Returns Mean                 -0.0208834\n",
      "evaluation/Returns Std                   0.00127269\n",
      "evaluation/Returns Max                  -0.0196881\n",
      "evaluation/Returns Min                  -0.0228356\n",
      "evaluation/Actions Mean                 -0.0133939\n",
      "evaluation/Actions Std                   0.00544489\n",
      "evaluation/Actions Max                   0.0194762\n",
      "evaluation/Actions Min                  -0.0268561\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0208834\n",
      "time/data storing (s)                    0.00543126\n",
      "time/evaluation sampling (s)             7.88505\n",
      "time/exploration sampling (s)            1.83247\n",
      "time/logging (s)                         0.0182083\n",
      "time/sac training (s)                   24.7671\n",
      "time/saving (s)                          0.00658279\n",
      "time/training (s)                        5.32102e-05\n",
      "time/epoch (s)                          34.5149\n",
      "time/total (s)                        1019.37\n",
      "Epoch                                   26\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:37:43.722244 UTC | [] Epoch 27 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   29000\n",
      "trainer/num train calls              28000\n",
      "trainer/QF1 Loss                         0.000162112\n",
      "trainer/QF2 Loss                         0.000178924\n",
      "trainer/Policy Loss                     -1.85032\n",
      "trainer/Q1 Predictions Mean              1.83776\n",
      "trainer/Q1 Predictions Std               0.0806021\n",
      "trainer/Q1 Predictions Max               1.91467\n",
      "trainer/Q1 Predictions Min               1.45891\n",
      "trainer/Q2 Predictions Mean              1.83821\n",
      "trainer/Q2 Predictions Std               0.0805949\n",
      "trainer/Q2 Predictions Max               1.91426\n",
      "trainer/Q2 Predictions Min               1.46049\n",
      "trainer/Q Targets Mean                   1.82638\n",
      "trainer/Q Targets Std                    0.0816861\n",
      "trainer/Q Targets Max                    1.90292\n",
      "trainer/Q Targets Min                    1.44947\n",
      "trainer/Log Pis Mean                     0.893428\n",
      "trainer/Log Pis Std                      0.705867\n",
      "trainer/Log Pis Max                      1.95248\n",
      "trainer/Log Pis Min                     -2.29722\n",
      "trainer/policy/mean Mean                 0.0208234\n",
      "trainer/policy/mean Std                  0.0309498\n",
      "trainer/policy/mean Max                  0.238094\n",
      "trainer/policy/mean Min                 -0.0422938\n",
      "trainer/policy/normal/std Mean           0.0990043\n",
      "trainer/policy/normal/std Std            0.010705\n",
      "trainer/policy/normal/std Max            0.123047\n",
      "trainer/policy/normal/std Min            0.0566039\n",
      "trainer/policy/normal/log_std Mean      -2.31894\n",
      "trainer/policy/normal/log_std Std        0.115584\n",
      "trainer/policy/normal/log_std Max       -2.09519\n",
      "trainer/policy/normal/log_std Min       -2.87168\n",
      "trainer/Alpha                            0.0016235\n",
      "trainer/Alpha Loss                      -0.684533\n",
      "exploration/num steps total          29000\n",
      "exploration/num paths total             58\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000967716\n",
      "exploration/Rewards Std                  0.00138703\n",
      "exploration/Rewards Max                 -5.02268e-11\n",
      "exploration/Rewards Min                 -0.0143159\n",
      "exploration/Returns Mean                -0.483858\n",
      "exploration/Returns Std                  0.480858\n",
      "exploration/Returns Max                 -0.00299956\n",
      "exploration/Returns Min                 -0.964716\n",
      "exploration/Actions Mean                 0.0162682\n",
      "exploration/Actions Std                  0.0970181\n",
      "exploration/Actions Max                  0.378363\n",
      "exploration/Actions Min                 -0.258154\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.483858\n",
      "evaluation/num steps total          139860\n",
      "evaluation/num paths total             140\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.93665e-05\n",
      "evaluation/Rewards Std                   1.94422e-05\n",
      "evaluation/Rewards Max                  -2.1504e-10\n",
      "evaluation/Rewards Min                  -0.00018852\n",
      "evaluation/Returns Mean                 -0.0193471\n",
      "evaluation/Returns Std                   0.00431172\n",
      "evaluation/Returns Max                  -0.0155849\n",
      "evaluation/Returns Min                  -0.0260285\n",
      "evaluation/Actions Mean                  0.0126392\n",
      "evaluation/Actions Std                   0.0058237\n",
      "evaluation/Actions Max                   0.0434189\n",
      "evaluation/Actions Min                  -0.00649209\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0193471\n",
      "time/data storing (s)                    0.00536373\n",
      "time/evaluation sampling (s)             7.8202\n",
      "time/exploration sampling (s)            1.87748\n",
      "time/logging (s)                         0.0168063\n",
      "time/sac training (s)                   24.9687\n",
      "time/saving (s)                          0.00616853\n",
      "time/training (s)                        6.52671e-05\n",
      "time/epoch (s)                          34.6948\n",
      "time/total (s)                        1056.05\n",
      "Epoch                                   27\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:38:20.643016 UTC | [] Epoch 28 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   30000\n",
      "trainer/num train calls              29000\n",
      "trainer/QF1 Loss                         0.00010089\n",
      "trainer/QF2 Loss                         9.86936e-05\n",
      "trainer/Policy Loss                     -1.71066\n",
      "trainer/Q1 Predictions Mean              1.69925\n",
      "trainer/Q1 Predictions Std               0.0839767\n",
      "trainer/Q1 Predictions Max               1.78017\n",
      "trainer/Q1 Predictions Min               1.25414\n",
      "trainer/Q2 Predictions Mean              1.69939\n",
      "trainer/Q2 Predictions Std               0.083056\n",
      "trainer/Q2 Predictions Max               1.78265\n",
      "trainer/Q2 Predictions Min               1.26107\n",
      "trainer/Q Targets Mean                   1.7067\n",
      "trainer/Q Targets Std                    0.083178\n",
      "trainer/Q Targets Max                    1.78652\n",
      "trainer/Q Targets Min                    1.25779\n",
      "trainer/Log Pis Mean                     1.12943\n",
      "trainer/Log Pis Std                      0.661006\n",
      "trainer/Log Pis Max                      2.06732\n",
      "trainer/Log Pis Min                     -2.73277\n",
      "trainer/policy/mean Mean                 0.0234178\n",
      "trainer/policy/mean Std                  0.0280415\n",
      "trainer/policy/mean Max                  0.136898\n",
      "trainer/policy/mean Min                 -0.0629843\n",
      "trainer/policy/normal/std Mean           0.0825273\n",
      "trainer/policy/normal/std Std            0.0145158\n",
      "trainer/policy/normal/std Max            0.144829\n",
      "trainer/policy/normal/std Min            0.0503719\n",
      "trainer/policy/normal/log_std Mean      -2.50781\n",
      "trainer/policy/normal/log_std Std        0.156781\n",
      "trainer/policy/normal/log_std Max       -1.9322\n",
      "trainer/policy/normal/log_std Min       -2.98832\n",
      "trainer/Alpha                            0.0017349\n",
      "trainer/Alpha Loss                       0.822733\n",
      "exploration/num steps total          30000\n",
      "exploration/num paths total             60\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000550339\n",
      "exploration/Rewards Std                  0.000791093\n",
      "exploration/Rewards Max                 -3.84082e-09\n",
      "exploration/Rewards Min                 -0.00596556\n",
      "exploration/Returns Mean                -0.275169\n",
      "exploration/Returns Std                  0.273746\n",
      "exploration/Returns Max                 -0.00142352\n",
      "exploration/Returns Min                 -0.548915\n",
      "exploration/Actions Mean                 0.00781296\n",
      "exploration/Actions Std                  0.0737723\n",
      "exploration/Actions Max                  0.244245\n",
      "exploration/Actions Min                 -0.225145\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.275169\n",
      "evaluation/num steps total          144855\n",
      "evaluation/num paths total             145\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -8.60711e-06\n",
      "evaluation/Rewards Std                   7.93721e-06\n",
      "evaluation/Rewards Max                  -2.83311e-08\n",
      "evaluation/Rewards Min                  -6.7238e-05\n",
      "evaluation/Returns Mean                 -0.0085985\n",
      "evaluation/Returns Std                   0.00418417\n",
      "evaluation/Returns Max                  -0.00641402\n",
      "evaluation/Returns Min                  -0.016966\n",
      "evaluation/Actions Mean                  0.00868586\n",
      "evaluation/Actions Std                   0.00325989\n",
      "evaluation/Actions Max                   0.0259303\n",
      "evaluation/Actions Min                   0.000532217\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0085985\n",
      "time/data storing (s)                    0.00514724\n",
      "time/evaluation sampling (s)             7.91686\n",
      "time/exploration sampling (s)            1.87902\n",
      "time/logging (s)                         0.0693053\n",
      "time/sac training (s)                   24.7211\n",
      "time/saving (s)                          0.00632362\n",
      "time/training (s)                        4.11198e-05\n",
      "time/epoch (s)                          34.5978\n",
      "time/total (s)                        1093.01\n",
      "Epoch                                   28\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:38:57.763614 UTC | [] Epoch 29 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   31000\n",
      "trainer/num train calls              30000\n",
      "trainer/QF1 Loss                         9.62435e-05\n",
      "trainer/QF2 Loss                         0.000119158\n",
      "trainer/Policy Loss                     -1.61097\n",
      "trainer/Q1 Predictions Mean              1.59957\n",
      "trainer/Q1 Predictions Std               0.0657298\n",
      "trainer/Q1 Predictions Max               1.66646\n",
      "trainer/Q1 Predictions Min               1.29065\n",
      "trainer/Q2 Predictions Mean              1.60014\n",
      "trainer/Q2 Predictions Std               0.0659173\n",
      "trainer/Q2 Predictions Max               1.6671\n",
      "trainer/Q2 Predictions Min               1.30001\n",
      "trainer/Q Targets Mean                   1.60834\n",
      "trainer/Q Targets Std                    0.0665561\n",
      "trainer/Q Targets Max                    1.67351\n",
      "trainer/Q Targets Min                    1.29885\n",
      "trainer/Log Pis Mean                     0.998876\n",
      "trainer/Log Pis Std                      0.672109\n",
      "trainer/Log Pis Max                      1.69931\n",
      "trainer/Log Pis Min                     -3.17294\n",
      "trainer/policy/mean Mean                 0.00585851\n",
      "trainer/policy/mean Std                  0.0349847\n",
      "trainer/policy/mean Max                  0.261857\n",
      "trainer/policy/mean Min                 -0.0226821\n",
      "trainer/policy/normal/std Mean           0.0922372\n",
      "trainer/policy/normal/std Std            0.0133794\n",
      "trainer/policy/normal/std Max            0.143367\n",
      "trainer/policy/normal/std Min            0.0490092\n",
      "trainer/policy/normal/log_std Mean      -2.39345\n",
      "trainer/policy/normal/log_std Std        0.141329\n",
      "trainer/policy/normal/log_std Max       -1.94235\n",
      "trainer/policy/normal/log_std Min       -3.01575\n",
      "trainer/Alpha                            0.00185442\n",
      "trainer/Alpha Loss                      -0.00706953\n",
      "exploration/num steps total          31000\n",
      "exploration/num paths total             62\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000734666\n",
      "exploration/Rewards Std                  0.000991556\n",
      "exploration/Rewards Max                 -2.73873e-08\n",
      "exploration/Rewards Min                 -0.00653126\n",
      "exploration/Returns Mean                -0.367333\n",
      "exploration/Returns Std                  0.3673\n",
      "exploration/Returns Max                 -3.30075e-05\n",
      "exploration/Returns Min                 -0.734633\n",
      "exploration/Actions Mean                -0.0119474\n",
      "exploration/Actions Std                  0.0848759\n",
      "exploration/Actions Max                  0.255563\n",
      "exploration/Actions Min                 -0.253231\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.367333\n",
      "evaluation/num steps total          149850\n",
      "evaluation/num paths total             150\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.10794e-05\n",
      "evaluation/Rewards Std                   6.41146e-06\n",
      "evaluation/Rewards Max                  -9.20899e-12\n",
      "evaluation/Rewards Min                  -2.89205e-05\n",
      "evaluation/Returns Mean                 -0.0110683\n",
      "evaluation/Returns Std                   0.00268865\n",
      "evaluation/Returns Max                  -0.00762835\n",
      "evaluation/Returns Min                  -0.0138784\n",
      "evaluation/Actions Mean                 -0.0094945\n",
      "evaluation/Actions Std                   0.00454407\n",
      "evaluation/Actions Max                   0.0107052\n",
      "evaluation/Actions Min                  -0.017006\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0110683\n",
      "time/data storing (s)                    0.00553014\n",
      "time/evaluation sampling (s)             7.80673\n",
      "time/exploration sampling (s)            1.82591\n",
      "time/logging (s)                         0.0665177\n",
      "time/sac training (s)                   24.7826\n",
      "time/saving (s)                          0.00647145\n",
      "time/training (s)                        4.7354e-05\n",
      "time/epoch (s)                          34.4938\n",
      "time/total (s)                        1130.12\n",
      "Epoch                                   29\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:39:34.451469 UTC | [] Epoch 30 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   32000\n",
      "trainer/num train calls              31000\n",
      "trainer/QF1 Loss                         2.6581e-05\n",
      "trainer/QF2 Loss                         2.14141e-05\n",
      "trainer/Policy Loss                     -1.5202\n",
      "trainer/Q1 Predictions Mean              1.51094\n",
      "trainer/Q1 Predictions Std               0.0694005\n",
      "trainer/Q1 Predictions Max               1.57572\n",
      "trainer/Q1 Predictions Min               1.11062\n",
      "trainer/Q2 Predictions Mean              1.51113\n",
      "trainer/Q2 Predictions Std               0.069799\n",
      "trainer/Q2 Predictions Max               1.57719\n",
      "trainer/Q2 Predictions Min               1.11068\n",
      "trainer/Q Targets Mean                   1.51117\n",
      "trainer/Q Targets Std                    0.0674348\n",
      "trainer/Q Targets Max                    1.57464\n",
      "trainer/Q Targets Min                    1.14211\n",
      "trainer/Log Pis Mean                     1.11375\n",
      "trainer/Log Pis Std                      0.832288\n",
      "trainer/Log Pis Max                      1.92319\n",
      "trainer/Log Pis Min                     -3.68205\n",
      "trainer/policy/mean Mean                 0.0133435\n",
      "trainer/policy/mean Std                  0.0209279\n",
      "trainer/policy/mean Max                  0.153684\n",
      "trainer/policy/mean Min                 -0.09601\n",
      "trainer/policy/normal/std Mean           0.0785205\n",
      "trainer/policy/normal/std Std            0.00790377\n",
      "trainer/policy/normal/std Max            0.118519\n",
      "trainer/policy/normal/std Min            0.0469887\n",
      "trainer/policy/normal/log_std Mean      -2.54916\n",
      "trainer/policy/normal/log_std Std        0.0965916\n",
      "trainer/policy/normal/log_std Max       -2.13268\n",
      "trainer/policy/normal/log_std Min       -3.05785\n",
      "trainer/Alpha                            0.0017952\n",
      "trainer/Alpha Loss                       0.719179\n",
      "exploration/num steps total          32000\n",
      "exploration/num paths total             64\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000612952\n",
      "exploration/Rewards Std                  0.000812419\n",
      "exploration/Rewards Max                 -2.32831e-09\n",
      "exploration/Rewards Min                 -0.00684777\n",
      "exploration/Returns Mean                -0.306476\n",
      "exploration/Returns Std                  0.304077\n",
      "exploration/Returns Max                 -0.00239849\n",
      "exploration/Returns Min                 -0.610553\n",
      "exploration/Actions Mean                 0.0150366\n",
      "exploration/Actions Std                  0.0768337\n",
      "exploration/Actions Max                  0.261683\n",
      "exploration/Actions Min                 -0.224084\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.306476\n",
      "evaluation/num steps total          154845\n",
      "evaluation/num paths total             155\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -4.40946e-05\n",
      "evaluation/Rewards Std                   6.01987e-05\n",
      "evaluation/Rewards Max                  -1.02673e-13\n",
      "evaluation/Rewards Min                  -0.000259564\n",
      "evaluation/Returns Mean                 -0.0440505\n",
      "evaluation/Returns Std                   0.0177464\n",
      "evaluation/Returns Max                  -0.0212063\n",
      "evaluation/Returns Min                  -0.0748891\n",
      "evaluation/Actions Mean                  0.0151232\n",
      "evaluation/Actions Std                   0.0145683\n",
      "evaluation/Actions Max                   0.0509474\n",
      "evaluation/Actions Min                  -0.00611145\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0440505\n",
      "time/data storing (s)                    0.00546793\n",
      "time/evaluation sampling (s)             7.8292\n",
      "time/exploration sampling (s)            1.86535\n",
      "time/logging (s)                         0.0667667\n",
      "time/sac training (s)                   24.6921\n",
      "time/saving (s)                          0.00689813\n",
      "time/training (s)                        5.12321e-05\n",
      "time/epoch (s)                          34.4658\n",
      "time/total (s)                        1166.79\n",
      "Epoch                                   30\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:40:11.131019 UTC | [] Epoch 31 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   33000\n",
      "trainer/num train calls              32000\n",
      "trainer/QF1 Loss                         0.01668\n",
      "trainer/QF2 Loss                         0.0166642\n",
      "trainer/Policy Loss                     -1.41535\n",
      "trainer/Q1 Predictions Mean              1.40421\n",
      "trainer/Q1 Predictions Std               0.0714572\n",
      "trainer/Q1 Predictions Max               1.47279\n",
      "trainer/Q1 Predictions Min               0.912417\n",
      "trainer/Q2 Predictions Mean              1.40404\n",
      "trainer/Q2 Predictions Std               0.0713566\n",
      "trainer/Q2 Predictions Max               1.46878\n",
      "trainer/Q2 Predictions Min               0.907054\n",
      "trainer/Q Targets Mean                   1.4029\n",
      "trainer/Q Targets Std                    0.14344\n",
      "trainer/Q Targets Max                    1.48046\n",
      "trainer/Q Targets Min                   -0.00498076\n",
      "trainer/Log Pis Mean                     0.958606\n",
      "trainer/Log Pis Std                      0.736031\n",
      "trainer/Log Pis Max                      1.75317\n",
      "trainer/Log Pis Min                     -4.04064\n",
      "trainer/policy/mean Mean                 0.0066246\n",
      "trainer/policy/mean Std                  0.0283683\n",
      "trainer/policy/mean Max                  0.229909\n",
      "trainer/policy/mean Min                 -0.0170919\n",
      "trainer/policy/normal/std Mean           0.095557\n",
      "trainer/policy/normal/std Std            0.0110106\n",
      "trainer/policy/normal/std Max            0.18488\n",
      "trainer/policy/normal/std Min            0.0686479\n",
      "trainer/policy/normal/log_std Mean      -2.35355\n",
      "trainer/policy/normal/log_std Std        0.100989\n",
      "trainer/policy/normal/log_std Max       -1.68805\n",
      "trainer/policy/normal/log_std Min       -2.67876\n",
      "trainer/Alpha                            0.00174748\n",
      "trainer/Alpha Loss                      -0.262832\n",
      "exploration/num steps total          33000\n",
      "exploration/num paths total             66\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000905813\n",
      "exploration/Rewards Std                  0.0012957\n",
      "exploration/Rewards Max                 -5.32086e-11\n",
      "exploration/Rewards Min                 -0.01304\n",
      "exploration/Returns Mean                -0.452907\n",
      "exploration/Returns Std                  0.452824\n",
      "exploration/Returns Max                 -8.22151e-05\n",
      "exploration/Returns Min                 -0.905731\n",
      "exploration/Actions Mean                -0.0054978\n",
      "exploration/Actions Std                  0.0950153\n",
      "exploration/Actions Max                  0.270698\n",
      "exploration/Actions Min                 -0.361109\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.452907\n",
      "evaluation/num steps total          159840\n",
      "evaluation/num paths total             160\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -5.23702e-06\n",
      "evaluation/Rewards Std                   3.70022e-06\n",
      "evaluation/Rewards Max                  -1.71951e-13\n",
      "evaluation/Rewards Min                  -1.55644e-05\n",
      "evaluation/Returns Mean                 -0.00523178\n",
      "evaluation/Returns Std                   0.000966629\n",
      "evaluation/Returns Max                  -0.00450559\n",
      "evaluation/Returns Min                  -0.00702249\n",
      "evaluation/Actions Mean                 -0.00608379\n",
      "evaluation/Actions Std                   0.00391889\n",
      "evaluation/Actions Max                   0.00881977\n",
      "evaluation/Actions Min                  -0.0124757\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.00523178\n",
      "time/data storing (s)                    0.00561025\n",
      "time/evaluation sampling (s)             7.92591\n",
      "time/exploration sampling (s)            1.76948\n",
      "time/logging (s)                         0.016897\n",
      "time/sac training (s)                   24.5645\n",
      "time/saving (s)                          0.00635248\n",
      "time/training (s)                        4.26453e-05\n",
      "time/epoch (s)                          34.2888\n",
      "time/total (s)                        1203.41\n",
      "Epoch                                   31\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:40:47.919418 UTC | [] Epoch 32 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   34000\n",
      "trainer/num train calls              33000\n",
      "trainer/QF1 Loss                         8.0013e-05\n",
      "trainer/QF2 Loss                         8.2217e-05\n",
      "trainer/Policy Loss                     -1.31731\n",
      "trainer/Q1 Predictions Mean              1.30746\n",
      "trainer/Q1 Predictions Std               0.0788361\n",
      "trainer/Q1 Predictions Max               1.38518\n",
      "trainer/Q1 Predictions Min               0.825011\n",
      "trainer/Q2 Predictions Mean              1.30716\n",
      "trainer/Q2 Predictions Std               0.0789772\n",
      "trainer/Q2 Predictions Max               1.38035\n",
      "trainer/Q2 Predictions Min               0.821091\n",
      "trainer/Q Targets Mean                   1.31493\n",
      "trainer/Q Targets Std                    0.0785439\n",
      "trainer/Q Targets Max                    1.38903\n",
      "trainer/Q Targets Min                    0.839566\n",
      "trainer/Log Pis Mean                     0.814236\n",
      "trainer/Log Pis Std                      0.810057\n",
      "trainer/Log Pis Max                      1.74577\n",
      "trainer/Log Pis Min                     -4.62622\n",
      "trainer/policy/mean Mean                 0.00439168\n",
      "trainer/policy/mean Std                  0.0248405\n",
      "trainer/policy/mean Max                  0.125012\n",
      "trainer/policy/mean Min                 -0.0441048\n",
      "trainer/policy/normal/std Mean           0.0984005\n",
      "trainer/policy/normal/std Std            0.0101044\n",
      "trainer/policy/normal/std Max            0.174392\n",
      "trainer/policy/normal/std Min            0.0680555\n",
      "trainer/policy/normal/log_std Mean      -2.32344\n",
      "trainer/policy/normal/log_std Std        0.0953617\n",
      "trainer/policy/normal/log_std Max       -1.74645\n",
      "trainer/policy/normal/log_std Min       -2.68743\n",
      "trainer/Alpha                            0.00160519\n",
      "trainer/Alpha Loss                      -1.1953\n",
      "exploration/num steps total          34000\n",
      "exploration/num paths total             68\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000936597\n",
      "exploration/Rewards Std                  0.00133469\n",
      "exploration/Rewards Max                 -1.49448e-09\n",
      "exploration/Rewards Min                 -0.0111936\n",
      "exploration/Returns Mean                -0.468299\n",
      "exploration/Returns Std                  0.467536\n",
      "exploration/Returns Max                 -0.00076233\n",
      "exploration/Returns Min                 -0.935835\n",
      "exploration/Actions Mean                -0.00140889\n",
      "exploration/Actions Std                  0.0967677\n",
      "exploration/Actions Max                  0.298962\n",
      "exploration/Actions Min                 -0.334568\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.468299\n",
      "evaluation/num steps total          164835\n",
      "evaluation/num paths total             165\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -9.37369e-06\n",
      "evaluation/Rewards Std                   6.06029e-06\n",
      "evaluation/Rewards Max                  -1.71951e-13\n",
      "evaluation/Rewards Min                  -5.80394e-05\n",
      "evaluation/Returns Mean                 -0.00936431\n",
      "evaluation/Returns Std                   0.000519878\n",
      "evaluation/Returns Max                  -0.00883446\n",
      "evaluation/Returns Min                  -0.0101918\n",
      "evaluation/Actions Mean                 -0.00766949\n",
      "evaluation/Actions Std                   0.00590896\n",
      "evaluation/Actions Max                   0.0240914\n",
      "evaluation/Actions Min                  -0.0166537\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.00936431\n",
      "time/data storing (s)                    0.00536991\n",
      "time/evaluation sampling (s)             7.63208\n",
      "time/exploration sampling (s)            1.80141\n",
      "time/logging (s)                         0.0176744\n",
      "time/sac training (s)                   24.7121\n",
      "time/saving (s)                          0.00693988\n",
      "time/training (s)                        4.3584e-05\n",
      "time/epoch (s)                          34.1756\n",
      "time/total (s)                        1240.18\n",
      "Epoch                                   32\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:41:24.720535 UTC | [] Epoch 33 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   35000\n",
      "trainer/num train calls              34000\n",
      "trainer/QF1 Loss                         2.82021e-05\n",
      "trainer/QF2 Loss                         3.10189e-05\n",
      "trainer/Policy Loss                     -1.25093\n",
      "trainer/Q1 Predictions Mean              1.24246\n",
      "trainer/Q1 Predictions Std               0.0652638\n",
      "trainer/Q1 Predictions Max               1.31066\n",
      "trainer/Q1 Predictions Min               0.917522\n",
      "trainer/Q2 Predictions Mean              1.24274\n",
      "trainer/Q2 Predictions Std               0.0651507\n",
      "trainer/Q2 Predictions Max               1.30898\n",
      "trainer/Q2 Predictions Min               0.909977\n",
      "trainer/Q Targets Mean                   1.23817\n",
      "trainer/Q Targets Std                    0.066212\n",
      "trainer/Q Targets Max                    1.30452\n",
      "trainer/Q Targets Min                    0.893649\n",
      "trainer/Log Pis Mean                     1.01774\n",
      "trainer/Log Pis Std                      0.766455\n",
      "trainer/Log Pis Max                      1.84723\n",
      "trainer/Log Pis Min                     -3.5439\n",
      "trainer/policy/mean Mean                -0.00314084\n",
      "trainer/policy/mean Std                  0.0224655\n",
      "trainer/policy/mean Max                  0.102539\n",
      "trainer/policy/mean Min                 -0.0220683\n",
      "trainer/policy/normal/std Mean           0.0852566\n",
      "trainer/policy/normal/std Std            0.0116679\n",
      "trainer/policy/normal/std Max            0.162417\n",
      "trainer/policy/normal/std Min            0.0623695\n",
      "trainer/policy/normal/log_std Mean      -2.4695\n",
      "trainer/policy/normal/log_std Std        0.115496\n",
      "trainer/policy/normal/log_std Max       -1.81759\n",
      "trainer/policy/normal/log_std Min       -2.77468\n",
      "trainer/Alpha                            0.00167461\n",
      "trainer/Alpha Loss                       0.113396\n",
      "exploration/num steps total          35000\n",
      "exploration/num paths total             70\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000696103\n",
      "exploration/Rewards Std                  0.000953104\n",
      "exploration/Rewards Max                 -4.68214e-10\n",
      "exploration/Rewards Min                 -0.00816025\n",
      "exploration/Returns Mean                -0.348052\n",
      "exploration/Returns Std                  0.348034\n",
      "exploration/Returns Max                 -1.80679e-05\n",
      "exploration/Returns Min                 -0.696085\n",
      "exploration/Actions Mean                -0.0102219\n",
      "exploration/Actions Std                  0.0828043\n",
      "exploration/Actions Max                  0.207752\n",
      "exploration/Actions Min                 -0.285662\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.348052\n",
      "evaluation/num steps total          169830\n",
      "evaluation/num paths total             170\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.60751e-05\n",
      "evaluation/Rewards Std                   9.78301e-06\n",
      "evaluation/Rewards Max                  -4.61711e-12\n",
      "evaluation/Rewards Min                  -4.10706e-05\n",
      "evaluation/Returns Mean                 -0.016059\n",
      "evaluation/Returns Std                   0.00382511\n",
      "evaluation/Returns Max                  -0.0113062\n",
      "evaluation/Returns Min                  -0.0214575\n",
      "evaluation/Actions Mean                 -0.0108221\n",
      "evaluation/Actions Std                   0.00660549\n",
      "evaluation/Actions Max                   0.0188239\n",
      "evaluation/Actions Min                  -0.0202659\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.016059\n",
      "time/data storing (s)                    0.00514953\n",
      "time/evaluation sampling (s)             7.9186\n",
      "time/exploration sampling (s)            1.87685\n",
      "time/logging (s)                         0.016991\n",
      "time/sac training (s)                   24.8005\n",
      "time/saving (s)                          0.00561536\n",
      "time/training (s)                        4.04827e-05\n",
      "time/epoch (s)                          34.6238\n",
      "time/total (s)                        1276.97\n",
      "Epoch                                   33\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:42:01.831314 UTC | [] Epoch 34 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   36000\n",
      "trainer/num train calls              35000\n",
      "trainer/QF1 Loss                         0.00559847\n",
      "trainer/QF2 Loss                         0.00560174\n",
      "trainer/Policy Loss                     -1.16251\n",
      "trainer/Q1 Predictions Mean              1.15185\n",
      "trainer/Q1 Predictions Std               0.0732734\n",
      "trainer/Q1 Predictions Max               1.22558\n",
      "trainer/Q1 Predictions Min               0.842003\n",
      "trainer/Q2 Predictions Mean              1.15179\n",
      "trainer/Q2 Predictions Std               0.0745925\n",
      "trainer/Q2 Predictions Max               1.22423\n",
      "trainer/Q2 Predictions Min               0.82313\n",
      "trainer/Q Targets Mean                   1.14478\n",
      "trainer/Q Targets Std                    0.102788\n",
      "trainer/Q Targets Max                    1.21798\n",
      "trainer/Q Targets Min                   -0.00184304\n",
      "trainer/Log Pis Mean                     1.00302\n",
      "trainer/Log Pis Std                      0.707904\n",
      "trainer/Log Pis Max                      1.7502\n",
      "trainer/Log Pis Min                     -3.31676\n",
      "trainer/policy/mean Mean                -0.0234125\n",
      "trainer/policy/mean Std                  0.0238187\n",
      "trainer/policy/mean Max                  0.0907945\n",
      "trainer/policy/mean Min                 -0.0450555\n",
      "trainer/policy/normal/std Mean           0.0840548\n",
      "trainer/policy/normal/std Std            0.00932403\n",
      "trainer/policy/normal/std Max            0.170201\n",
      "trainer/policy/normal/std Min            0.0643336\n",
      "trainer/policy/normal/log_std Mean      -2.48113\n",
      "trainer/policy/normal/log_std Std        0.0933983\n",
      "trainer/policy/normal/log_std Max       -1.77077\n",
      "trainer/policy/normal/log_std Min       -2.74367\n",
      "trainer/Alpha                            0.0017282\n",
      "trainer/Alpha Loss                       0.0192021\n",
      "exploration/num steps total          36000\n",
      "exploration/num paths total             72\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000795054\n",
      "exploration/Rewards Std                  0.00111153\n",
      "exploration/Rewards Max                 -5.62199e-09\n",
      "exploration/Rewards Min                 -0.00816603\n",
      "exploration/Returns Mean                -0.397527\n",
      "exploration/Returns Std                  0.397486\n",
      "exploration/Returns Max                 -4.11927e-05\n",
      "exploration/Returns Min                 -0.795013\n",
      "exploration/Actions Mean                -0.0329195\n",
      "exploration/Actions Std                  0.0828664\n",
      "exploration/Actions Max                  0.271656\n",
      "exploration/Actions Min                 -0.285763\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.397527\n",
      "evaluation/num steps total          174825\n",
      "evaluation/num paths total             175\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -0.000149277\n",
      "evaluation/Rewards Std                   6.98554e-06\n",
      "evaluation/Rewards Max                  -0.000123861\n",
      "evaluation/Rewards Min                  -0.000168513\n",
      "evaluation/Returns Mean                 -0.149127\n",
      "evaluation/Returns Std                   0.00291269\n",
      "evaluation/Returns Max                  -0.14348\n",
      "evaluation/Returns Min                  -0.151224\n",
      "evaluation/Actions Mean                 -0.0386255\n",
      "evaluation/Actions Std                   0.000912787\n",
      "evaluation/Actions Max                  -0.0351938\n",
      "evaluation/Actions Min                  -0.0410503\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.149127\n",
      "time/data storing (s)                    0.00505484\n",
      "time/evaluation sampling (s)             7.97607\n",
      "time/exploration sampling (s)            1.83874\n",
      "time/logging (s)                         0.0182713\n",
      "time/sac training (s)                   25.2357\n",
      "time/saving (s)                          0.00661338\n",
      "time/training (s)                        4.78458e-05\n",
      "time/epoch (s)                          35.0805\n",
      "time/total (s)                        1314.06\n",
      "Epoch                                   34\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:42:39.258958 UTC | [] Epoch 35 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   37000\n",
      "trainer/num train calls              36000\n",
      "trainer/QF1 Loss                         4.67574e-05\n",
      "trainer/QF2 Loss                         4.20803e-05\n",
      "trainer/Policy Loss                     -1.07675\n",
      "trainer/Q1 Predictions Mean              1.0658\n",
      "trainer/Q1 Predictions Std               0.0718732\n",
      "trainer/Q1 Predictions Max               1.13562\n",
      "trainer/Q1 Predictions Min               0.735701\n",
      "trainer/Q2 Predictions Mean              1.06604\n",
      "trainer/Q2 Predictions Std               0.0718585\n",
      "trainer/Q2 Predictions Max               1.13601\n",
      "trainer/Q2 Predictions Min               0.744509\n",
      "trainer/Q Targets Mean                   1.07051\n",
      "trainer/Q Targets Std                    0.0718532\n",
      "trainer/Q Targets Max                    1.13999\n",
      "trainer/Q Targets Min                    0.733621\n",
      "trainer/Log Pis Mean                     1.0444\n",
      "trainer/Log Pis Std                      0.705517\n",
      "trainer/Log Pis Max                      1.66797\n",
      "trainer/Log Pis Min                     -4.21751\n",
      "trainer/policy/mean Mean                -0.0104915\n",
      "trainer/policy/mean Std                  0.022508\n",
      "trainer/policy/mean Max                  0.130858\n",
      "trainer/policy/mean Min                 -0.0321112\n",
      "trainer/policy/normal/std Mean           0.0850161\n",
      "trainer/policy/normal/std Std            0.0105415\n",
      "trainer/policy/normal/std Max            0.166278\n",
      "trainer/policy/normal/std Min            0.0650723\n",
      "trainer/policy/normal/log_std Mean      -2.47092\n",
      "trainer/policy/normal/log_std Std        0.103433\n",
      "trainer/policy/normal/log_std Max       -1.7941\n",
      "trainer/policy/normal/log_std Min       -2.73226\n",
      "trainer/Alpha                            0.00185539\n",
      "trainer/Alpha Loss                       0.279253\n",
      "exploration/num steps total          37000\n",
      "exploration/num paths total             74\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000731188\n",
      "exploration/Rewards Std                  0.00102615\n",
      "exploration/Rewards Max                 -5.98669e-09\n",
      "exploration/Rewards Min                 -0.00986172\n",
      "exploration/Returns Mean                -0.365594\n",
      "exploration/Returns Std                  0.365394\n",
      "exploration/Returns Max                 -0.000200023\n",
      "exploration/Returns Min                 -0.730988\n",
      "exploration/Actions Mean                -0.0255465\n",
      "exploration/Actions Std                  0.0816042\n",
      "exploration/Actions Max                  0.224019\n",
      "exploration/Actions Min                 -0.314034\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.365594\n",
      "evaluation/num steps total          179820\n",
      "evaluation/num paths total             180\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -5.26176e-05\n",
      "evaluation/Rewards Std                   1.7525e-05\n",
      "evaluation/Rewards Max                  -4.75907e-11\n",
      "evaluation/Rewards Min                  -9.25928e-05\n",
      "evaluation/Returns Mean                 -0.052565\n",
      "evaluation/Returns Std                   0.00836338\n",
      "evaluation/Returns Max                  -0.0389255\n",
      "evaluation/Returns Min                  -0.0605933\n",
      "evaluation/Actions Mean                 -0.0223153\n",
      "evaluation/Actions Std                   0.00531063\n",
      "evaluation/Actions Max                   0.00975192\n",
      "evaluation/Actions Min                  -0.0304291\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.052565\n",
      "time/data storing (s)                    0.00550868\n",
      "time/evaluation sampling (s)             8.18387\n",
      "time/exploration sampling (s)            1.80416\n",
      "time/logging (s)                         0.0665561\n",
      "time/sac training (s)                   25.0382\n",
      "time/saving (s)                          0.00548767\n",
      "time/training (s)                        3.98587e-05\n",
      "time/epoch (s)                          35.1038\n",
      "time/total (s)                        1351.53\n",
      "Epoch                                   35\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:43:16.041609 UTC | [] Epoch 36 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   38000\n",
      "trainer/num train calls              37000\n",
      "trainer/QF1 Loss                         6.7062e-06\n",
      "trainer/QF2 Loss                         9.1947e-06\n",
      "trainer/Policy Loss                     -1.01444\n",
      "trainer/Q1 Predictions Mean              1.00536\n",
      "trainer/Q1 Predictions Std               0.0635862\n",
      "trainer/Q1 Predictions Max               1.07176\n",
      "trainer/Q1 Predictions Min               0.69884\n",
      "trainer/Q2 Predictions Mean              1.00524\n",
      "trainer/Q2 Predictions Std               0.0635212\n",
      "trainer/Q2 Predictions Max               1.07157\n",
      "trainer/Q2 Predictions Min               0.703673\n",
      "trainer/Q Targets Mean                   1.00485\n",
      "trainer/Q Targets Std                    0.0637051\n",
      "trainer/Q Targets Max                    1.06998\n",
      "trainer/Q Targets Min                    0.692611\n",
      "trainer/Log Pis Mean                     0.981548\n",
      "trainer/Log Pis Std                      0.771211\n",
      "trainer/Log Pis Max                      1.63456\n",
      "trainer/Log Pis Min                     -5.05124\n",
      "trainer/policy/mean Mean                -0.0181518\n",
      "trainer/policy/mean Std                  0.0222101\n",
      "trainer/policy/mean Max                  0.0956614\n",
      "trainer/policy/mean Min                 -0.0494801\n",
      "trainer/policy/normal/std Mean           0.090955\n",
      "trainer/policy/normal/std Std            0.0127538\n",
      "trainer/policy/normal/std Max            0.168539\n",
      "trainer/policy/normal/std Min            0.0718434\n",
      "trainer/policy/normal/log_std Mean      -2.40508\n",
      "trainer/policy/normal/log_std Std        0.117177\n",
      "trainer/policy/normal/log_std Max       -1.78059\n",
      "trainer/policy/normal/log_std Min       -2.63327\n",
      "trainer/Alpha                            0.00175187\n",
      "trainer/Alpha Loss                      -0.117116\n",
      "exploration/num steps total          38000\n",
      "exploration/num paths total             76\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000861687\n",
      "exploration/Rewards Std                  0.0011897\n",
      "exploration/Rewards Max                 -6.37925e-10\n",
      "exploration/Rewards Min                 -0.00792376\n",
      "exploration/Returns Mean                -0.430844\n",
      "exploration/Returns Std                  0.4307\n",
      "exploration/Returns Max                 -0.000143927\n",
      "exploration/Returns Min                 -0.861543\n",
      "exploration/Actions Mean                -0.0321275\n",
      "exploration/Actions Std                  0.0870902\n",
      "exploration/Actions Max                  0.271413\n",
      "exploration/Actions Min                 -0.281492\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.430844\n",
      "evaluation/num steps total          184815\n",
      "evaluation/num paths total             185\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -8.36517e-05\n",
      "evaluation/Rewards Std                   2.64125e-05\n",
      "evaluation/Rewards Max                  -1.0537e-08\n",
      "evaluation/Rewards Min                  -0.000155184\n",
      "evaluation/Returns Mean                 -0.083568\n",
      "evaluation/Returns Std                   0.0100373\n",
      "evaluation/Returns Max                  -0.0680698\n",
      "evaluation/Returns Min                  -0.094901\n",
      "evaluation/Actions Mean                 -0.0282184\n",
      "evaluation/Actions Std                   0.00634329\n",
      "evaluation/Actions Max                   0.0148763\n",
      "evaluation/Actions Min                  -0.0393934\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.083568\n",
      "time/data storing (s)                    0.057581\n",
      "time/evaluation sampling (s)             7.69886\n",
      "time/exploration sampling (s)            1.83154\n",
      "time/logging (s)                         0.0169763\n",
      "time/sac training (s)                   24.9487\n",
      "time/saving (s)                          0.00635639\n",
      "time/training (s)                        4.6555e-05\n",
      "time/epoch (s)                          34.5601\n",
      "time/total (s)                        1388.24\n",
      "Epoch                                   36\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:43:53.141673 UTC | [] Epoch 37 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   39000\n",
      "trainer/num train calls              38000\n",
      "trainer/QF1 Loss                         2.56421e-05\n",
      "trainer/QF2 Loss                         2.85832e-05\n",
      "trainer/Policy Loss                     -0.945614\n",
      "trainer/Q1 Predictions Mean              0.934556\n",
      "trainer/Q1 Predictions Std               0.0709261\n",
      "trainer/Q1 Predictions Max               1.00138\n",
      "trainer/Q1 Predictions Min               0.566627\n",
      "trainer/Q2 Predictions Mean              0.934634\n",
      "trainer/Q2 Predictions Std               0.0712925\n",
      "trainer/Q2 Predictions Max               1.00237\n",
      "trainer/Q2 Predictions Min               0.554349\n",
      "trainer/Q Targets Mean                   0.931085\n",
      "trainer/Q Targets Std                    0.0712873\n",
      "trainer/Q Targets Max                    0.998106\n",
      "trainer/Q Targets Min                    0.559763\n",
      "trainer/Log Pis Mean                     1.05269\n",
      "trainer/Log Pis Std                      0.631304\n",
      "trainer/Log Pis Max                      1.94314\n",
      "trainer/Log Pis Min                     -2.21998\n",
      "trainer/policy/mean Mean                -0.00733965\n",
      "trainer/policy/mean Std                  0.0255573\n",
      "trainer/policy/mean Max                  0.134606\n",
      "trainer/policy/mean Min                 -0.0468117\n",
      "trainer/policy/normal/std Mean           0.0857043\n",
      "trainer/policy/normal/std Std            0.00908458\n",
      "trainer/policy/normal/std Max            0.148352\n",
      "trainer/policy/normal/std Min            0.053047\n",
      "trainer/policy/normal/log_std Mean      -2.46159\n",
      "trainer/policy/normal/log_std Std        0.093855\n",
      "trainer/policy/normal/log_std Max       -1.90816\n",
      "trainer/policy/normal/log_std Min       -2.93658\n",
      "trainer/Alpha                            0.00182251\n",
      "trainer/Alpha Loss                       0.332328\n",
      "exploration/num steps total          39000\n",
      "exploration/num paths total             78\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000718002\n",
      "exploration/Rewards Std                  0.000934374\n",
      "exploration/Rewards Max                 -4.52283e-09\n",
      "exploration/Rewards Min                 -0.00740331\n",
      "exploration/Returns Mean                -0.359001\n",
      "exploration/Returns Std                  0.358618\n",
      "exploration/Returns Max                 -0.000382441\n",
      "exploration/Returns Min                 -0.717619\n",
      "exploration/Actions Mean                -0.0117986\n",
      "exploration/Actions Std                  0.0839095\n",
      "exploration/Actions Max                  0.212643\n",
      "exploration/Actions Min                 -0.27209\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.359001\n",
      "evaluation/num steps total          189810\n",
      "evaluation/num paths total             190\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -2.96123e-05\n",
      "evaluation/Rewards Std                   2.16692e-05\n",
      "evaluation/Rewards Max                  -1.54756e-12\n",
      "evaluation/Rewards Min                  -0.000103636\n",
      "evaluation/Returns Mean                 -0.0295827\n",
      "evaluation/Returns Std                   0.00453391\n",
      "evaluation/Returns Max                  -0.0222675\n",
      "evaluation/Returns Min                  -0.0350061\n",
      "evaluation/Actions Mean                 -0.0154679\n",
      "evaluation/Actions Std                   0.00754106\n",
      "evaluation/Actions Max                   0.0237243\n",
      "evaluation/Actions Min                  -0.0321924\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0295827\n",
      "time/data storing (s)                    0.0053225\n",
      "time/evaluation sampling (s)             7.90757\n",
      "time/exploration sampling (s)            1.82763\n",
      "time/logging (s)                         0.0171305\n",
      "time/sac training (s)                   24.8306\n",
      "time/saving (s)                          0.00611425\n",
      "time/training (s)                        3.95961e-05\n",
      "time/epoch (s)                          34.5944\n",
      "time/total (s)                        1425.33\n",
      "Epoch                                   37\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:44:30.251934 UTC | [] Epoch 38 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   40000\n",
      "trainer/num train calls              39000\n",
      "trainer/QF1 Loss                         2.40641e-05\n",
      "trainer/QF2 Loss                         2.76929e-05\n",
      "trainer/Policy Loss                     -0.875867\n",
      "trainer/Q1 Predictions Mean              0.866637\n",
      "trainer/Q1 Predictions Std               0.0710121\n",
      "trainer/Q1 Predictions Max               0.931736\n",
      "trainer/Q1 Predictions Min               0.522665\n",
      "trainer/Q2 Predictions Mean              0.867062\n",
      "trainer/Q2 Predictions Std               0.0706497\n",
      "trainer/Q2 Predictions Max               0.932454\n",
      "trainer/Q2 Predictions Min               0.523643\n",
      "trainer/Q Targets Mean                   0.862804\n",
      "trainer/Q Targets Std                    0.0706096\n",
      "trainer/Q Targets Max                    0.927165\n",
      "trainer/Q Targets Min                    0.528085\n",
      "trainer/Log Pis Mean                     0.892638\n",
      "trainer/Log Pis Std                      0.837873\n",
      "trainer/Log Pis Max                      1.76377\n",
      "trainer/Log Pis Min                     -4.85358\n",
      "trainer/policy/mean Mean                -0.0326406\n",
      "trainer/policy/mean Std                  0.027084\n",
      "trainer/policy/mean Max                  0.102304\n",
      "trainer/policy/mean Min                 -0.0775324\n",
      "trainer/policy/normal/std Mean           0.0926988\n",
      "trainer/policy/normal/std Std            0.00736969\n",
      "trainer/policy/normal/std Max            0.133649\n",
      "trainer/policy/normal/std Min            0.0682686\n",
      "trainer/policy/normal/log_std Mean      -2.38124\n",
      "trainer/policy/normal/log_std Std        0.0736625\n",
      "trainer/policy/normal/log_std Max       -2.01254\n",
      "trainer/policy/normal/log_std Min       -2.68431\n",
      "trainer/Alpha                            0.00173688\n",
      "trainer/Alpha Loss                      -0.68236\n",
      "exploration/num steps total          40000\n",
      "exploration/num paths total             80\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.00101254\n",
      "exploration/Rewards Std                  0.00137651\n",
      "exploration/Rewards Max                 -3.63798e-11\n",
      "exploration/Rewards Min                 -0.0091193\n",
      "exploration/Returns Mean                -0.506269\n",
      "exploration/Returns Std                  0.503535\n",
      "exploration/Returns Max                 -0.00273443\n",
      "exploration/Returns Min                 -1.0098\n",
      "exploration/Actions Mean                -0.0432218\n",
      "exploration/Actions Std                  0.0908695\n",
      "exploration/Actions Max                  0.238456\n",
      "exploration/Actions Min                 -0.301982\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.506269\n",
      "evaluation/num steps total          194805\n",
      "evaluation/num paths total             195\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -0.000175795\n",
      "evaluation/Rewards Std                   4.70901e-05\n",
      "evaluation/Rewards Max                  -7.05143e-06\n",
      "evaluation/Rewards Min                  -0.000279568\n",
      "evaluation/Returns Mean                 -0.175619\n",
      "evaluation/Returns Std                   0.0105245\n",
      "evaluation/Returns Max                  -0.16296\n",
      "evaluation/Returns Min                  -0.191056\n",
      "evaluation/Actions Mean                 -0.041489\n",
      "evaluation/Actions Std                   0.00605037\n",
      "evaluation/Actions Max                  -0.0083973\n",
      "evaluation/Actions Min                  -0.0528741\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.175619\n",
      "time/data storing (s)                    0.00558083\n",
      "time/evaluation sampling (s)             7.91985\n",
      "time/exploration sampling (s)            1.80995\n",
      "time/logging (s)                         0.0691268\n",
      "time/sac training (s)                   25.1045\n",
      "time/saving (s)                          0.00586977\n",
      "time/training (s)                        4.19896e-05\n",
      "time/epoch (s)                          34.9149\n",
      "time/total (s)                        1462.48\n",
      "Epoch                                   38\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:45:07.154825 UTC | [] Epoch 39 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   41000\n",
      "trainer/num train calls              40000\n",
      "trainer/QF1 Loss                         0.0028761\n",
      "trainer/QF2 Loss                         0.00285946\n",
      "trainer/Policy Loss                     -0.810169\n",
      "trainer/Q1 Predictions Mean              0.800825\n",
      "trainer/Q1 Predictions Std               0.0724775\n",
      "trainer/Q1 Predictions Max               0.860778\n",
      "trainer/Q1 Predictions Min               0.534745\n",
      "trainer/Q2 Predictions Mean              0.801368\n",
      "trainer/Q2 Predictions Std               0.0728804\n",
      "trainer/Q2 Predictions Max               0.861593\n",
      "trainer/Q2 Predictions Min               0.532808\n",
      "trainer/Q Targets Mean                   0.798618\n",
      "trainer/Q Targets Std                    0.0877072\n",
      "trainer/Q Targets Max                    0.861125\n",
      "trainer/Q Targets Min                   -0.00126122\n",
      "trainer/Log Pis Mean                     0.916794\n",
      "trainer/Log Pis Std                      0.800154\n",
      "trainer/Log Pis Max                      1.70418\n",
      "trainer/Log Pis Min                     -4.05711\n",
      "trainer/policy/mean Mean                -0.00313218\n",
      "trainer/policy/mean Std                  0.0291224\n",
      "trainer/policy/mean Max                  0.155753\n",
      "trainer/policy/mean Min                 -0.0347349\n",
      "trainer/policy/normal/std Mean           0.0933634\n",
      "trainer/policy/normal/std Std            0.00656009\n",
      "trainer/policy/normal/std Max            0.126988\n",
      "trainer/policy/normal/std Min            0.0711991\n",
      "trainer/policy/normal/log_std Mean      -2.37354\n",
      "trainer/policy/normal/log_std Std        0.0665709\n",
      "trainer/policy/normal/log_std Max       -2.06366\n",
      "trainer/policy/normal/log_std Min       -2.64228\n",
      "trainer/Alpha                            0.00179387\n",
      "trainer/Alpha Loss                      -0.526146\n",
      "exploration/num steps total          41000\n",
      "exploration/num paths total             82\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000836393\n",
      "exploration/Rewards Std                  0.00117511\n",
      "exploration/Rewards Max                 -1.45252e-09\n",
      "exploration/Rewards Min                 -0.00847223\n",
      "exploration/Returns Mean                -0.418197\n",
      "exploration/Returns Std                  0.417782\n",
      "exploration/Returns Max                 -0.000414986\n",
      "exploration/Returns Min                 -0.835978\n",
      "exploration/Actions Mean                -0.0136738\n",
      "exploration/Actions Std                  0.0904265\n",
      "exploration/Actions Max                  0.270688\n",
      "exploration/Actions Min                 -0.291071\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.418197\n",
      "evaluation/num steps total          199800\n",
      "evaluation/num paths total             200\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -2.15803e-05\n",
      "evaluation/Rewards Std                   1.58208e-05\n",
      "evaluation/Rewards Max                  -1.36566e-12\n",
      "evaluation/Rewards Min                  -9.34192e-05\n",
      "evaluation/Returns Mean                 -0.0215587\n",
      "evaluation/Returns Std                   0.00127979\n",
      "evaluation/Returns Max                  -0.0199718\n",
      "evaluation/Returns Min                  -0.0230591\n",
      "evaluation/Actions Mean                 -0.01341\n",
      "evaluation/Actions Std                   0.0059979\n",
      "evaluation/Actions Max                   0.0113955\n",
      "evaluation/Actions Min                  -0.0305645\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0215587\n",
      "time/data storing (s)                    0.00545083\n",
      "time/evaluation sampling (s)             7.78755\n",
      "time/exploration sampling (s)            1.82452\n",
      "time/logging (s)                         0.0663577\n",
      "time/sac training (s)                   25.3064\n",
      "time/saving (s)                          0.00677962\n",
      "time/training (s)                        5.42086e-05\n",
      "time/epoch (s)                          34.9971\n",
      "time/total (s)                        1499.36\n",
      "Epoch                                   39\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:45:43.946484 UTC | [] Epoch 40 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   42000\n",
      "trainer/num train calls              41000\n",
      "trainer/QF1 Loss                         1.81808e-05\n",
      "trainer/QF2 Loss                         1.37625e-05\n",
      "trainer/Policy Loss                     -0.759087\n",
      "trainer/Q1 Predictions Mean              0.753896\n",
      "trainer/Q1 Predictions Std               0.0664551\n",
      "trainer/Q1 Predictions Max               0.802158\n",
      "trainer/Q1 Predictions Min               0.471197\n",
      "trainer/Q2 Predictions Mean              0.752269\n",
      "trainer/Q2 Predictions Std               0.0662751\n",
      "trainer/Q2 Predictions Max               0.80072\n",
      "trainer/Q2 Predictions Min               0.450807\n",
      "trainer/Q Targets Mean                   0.75111\n",
      "trainer/Q Targets Std                    0.0660713\n",
      "trainer/Q Targets Max                    0.799718\n",
      "trainer/Q Targets Min                    0.473134\n",
      "trainer/Log Pis Mean                     1.03319\n",
      "trainer/Log Pis Std                      0.572456\n",
      "trainer/Log Pis Max                      1.7971\n",
      "trainer/Log Pis Min                     -1.54335\n",
      "trainer/policy/mean Mean                -0.00603857\n",
      "trainer/policy/mean Std                  0.026421\n",
      "trainer/policy/mean Max                  0.139186\n",
      "trainer/policy/mean Min                 -0.0478582\n",
      "trainer/policy/normal/std Mean           0.0915286\n",
      "trainer/policy/normal/std Std            0.00775733\n",
      "trainer/policy/normal/std Max            0.130694\n",
      "trainer/policy/normal/std Min            0.0665058\n",
      "trainer/policy/normal/log_std Mean      -2.39434\n",
      "trainer/policy/normal/log_std Std        0.0787644\n",
      "trainer/policy/normal/log_std Max       -2.0349\n",
      "trainer/policy/normal/log_std Min       -2.71047\n",
      "trainer/Alpha                            0.00181542\n",
      "trainer/Alpha Loss                       0.209453\n",
      "exploration/num steps total          42000\n",
      "exploration/num paths total             84\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000845158\n",
      "exploration/Rewards Std                  0.00118668\n",
      "exploration/Rewards Max                 -3.35494e-09\n",
      "exploration/Rewards Min                 -0.00929036\n",
      "exploration/Returns Mean                -0.422579\n",
      "exploration/Returns Std                  0.421336\n",
      "exploration/Returns Max                 -0.00124288\n",
      "exploration/Returns Min                 -0.843915\n",
      "exploration/Actions Mean                -0.013101\n",
      "exploration/Actions Std                  0.0909942\n",
      "exploration/Actions Max                  0.238441\n",
      "exploration/Actions Min                 -0.304801\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.422579\n",
      "evaluation/num steps total          204795\n",
      "evaluation/num paths total             205\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -2.68726e-05\n",
      "evaluation/Rewards Std                   2.04447e-05\n",
      "evaluation/Rewards Max                  -1.00272e-11\n",
      "evaluation/Rewards Min                  -0.000107286\n",
      "evaluation/Returns Mean                 -0.0268457\n",
      "evaluation/Returns Std                   0.000834728\n",
      "evaluation/Returns Max                  -0.0252914\n",
      "evaluation/Returns Min                  -0.0276166\n",
      "evaluation/Actions Mean                 -0.0148894\n",
      "evaluation/Actions Std                   0.00685791\n",
      "evaluation/Actions Max                   0.0101915\n",
      "evaluation/Actions Min                  -0.0327546\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0268457\n",
      "time/data storing (s)                    0.00507887\n",
      "time/evaluation sampling (s)             7.79177\n",
      "time/exploration sampling (s)            1.81633\n",
      "time/logging (s)                         0.0722912\n",
      "time/sac training (s)                   24.7653\n",
      "time/saving (s)                          0.00587835\n",
      "time/training (s)                        4.52157e-05\n",
      "time/epoch (s)                          34.4567\n",
      "time/total (s)                        1536.15\n",
      "Epoch                                   40\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:46:21.355870 UTC | [] Epoch 41 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   43000\n",
      "trainer/num train calls              42000\n",
      "trainer/QF1 Loss                         8.68302e-06\n",
      "trainer/QF2 Loss                         7.43225e-06\n",
      "trainer/Policy Loss                     -0.70173\n",
      "trainer/Q1 Predictions Mean              0.696674\n",
      "trainer/Q1 Predictions Std               0.0605909\n",
      "trainer/Q1 Predictions Max               0.73992\n",
      "trainer/Q1 Predictions Min               0.308824\n",
      "trainer/Q2 Predictions Mean              0.696296\n",
      "trainer/Q2 Predictions Std               0.0602178\n",
      "trainer/Q2 Predictions Max               0.741111\n",
      "trainer/Q2 Predictions Min               0.310066\n",
      "trainer/Q Targets Mean                   0.695995\n",
      "trainer/Q Targets Std                    0.0611926\n",
      "trainer/Q Targets Max                    0.740488\n",
      "trainer/Q Targets Min                    0.308208\n",
      "trainer/Log Pis Mean                     1.00119\n",
      "trainer/Log Pis Std                      0.678273\n",
      "trainer/Log Pis Max                      1.88191\n",
      "trainer/Log Pis Min                     -4.6061\n",
      "trainer/policy/mean Mean                -0.0120305\n",
      "trainer/policy/mean Std                  0.0243113\n",
      "trainer/policy/mean Max                  0.151417\n",
      "trainer/policy/mean Min                 -0.0579804\n",
      "trainer/policy/normal/std Mean           0.0952263\n",
      "trainer/policy/normal/std Std            0.00562708\n",
      "trainer/policy/normal/std Max            0.124996\n",
      "trainer/policy/normal/std Min            0.0605539\n",
      "trainer/policy/normal/log_std Mean      -2.35326\n",
      "trainer/policy/normal/log_std Std        0.0597164\n",
      "trainer/policy/normal/log_std Max       -2.07948\n",
      "trainer/policy/normal/log_std Min       -2.80422\n",
      "trainer/Alpha                            0.00187259\n",
      "trainer/Alpha Loss                       0.00748818\n",
      "exploration/num steps total          43000\n",
      "exploration/num paths total             86\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000933066\n",
      "exploration/Rewards Std                  0.0012838\n",
      "exploration/Rewards Max                 -2.01071e-09\n",
      "exploration/Rewards Min                 -0.00954465\n",
      "exploration/Returns Mean                -0.466533\n",
      "exploration/Returns Std                  0.466228\n",
      "exploration/Returns Max                 -0.000305421\n",
      "exploration/Returns Min                 -0.932761\n",
      "exploration/Actions Mean                -0.0132202\n",
      "exploration/Actions Std                  0.0956864\n",
      "exploration/Actions Max                  0.300591\n",
      "exploration/Actions Min                 -0.308944\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.466533\n",
      "evaluation/num steps total          209790\n",
      "evaluation/num paths total             210\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -4.29075e-05\n",
      "evaluation/Rewards Std                   3.01356e-05\n",
      "evaluation/Rewards Max                  -8.31655e-10\n",
      "evaluation/Rewards Min                  -0.000176829\n",
      "evaluation/Returns Mean                 -0.0428646\n",
      "evaluation/Returns Std                   0.00416968\n",
      "evaluation/Returns Max                  -0.0367215\n",
      "evaluation/Returns Min                  -0.0480817\n",
      "evaluation/Actions Mean                 -0.0192096\n",
      "evaluation/Actions Std                   0.0077502\n",
      "evaluation/Actions Max                   0.00777059\n",
      "evaluation/Actions Min                  -0.0420511\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0428646\n",
      "time/data storing (s)                    0.00551263\n",
      "time/evaluation sampling (s)             7.93227\n",
      "time/exploration sampling (s)            1.78087\n",
      "time/logging (s)                         0.0705535\n",
      "time/sac training (s)                   25.3673\n",
      "time/saving (s)                          0.00692732\n",
      "time/training (s)                        4.88292e-05\n",
      "time/epoch (s)                          35.1635\n",
      "time/total (s)                        1573.54\n",
      "Epoch                                   41\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:46:58.226046 UTC | [] Epoch 42 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   44000\n",
      "trainer/num train calls              43000\n",
      "trainer/QF1 Loss                         2.90137e-05\n",
      "trainer/QF2 Loss                         4.80705e-05\n",
      "trainer/Policy Loss                     -0.648165\n",
      "trainer/Q1 Predictions Mean              0.643925\n",
      "trainer/Q1 Predictions Std               0.055672\n",
      "trainer/Q1 Predictions Max               0.680592\n",
      "trainer/Q1 Predictions Min               0.337831\n",
      "trainer/Q2 Predictions Mean              0.642235\n",
      "trainer/Q2 Predictions Std               0.0551157\n",
      "trainer/Q2 Predictions Max               0.677765\n",
      "trainer/Q2 Predictions Min               0.335229\n",
      "trainer/Q Targets Mean                   0.648687\n",
      "trainer/Q Targets Std                    0.0560276\n",
      "trainer/Q Targets Max                    0.683312\n",
      "trainer/Q Targets Min                    0.333365\n",
      "trainer/Log Pis Mean                     1.02261\n",
      "trainer/Log Pis Std                      0.593537\n",
      "trainer/Log Pis Max                      1.73795\n",
      "trainer/Log Pis Min                     -1.91322\n",
      "trainer/policy/mean Mean                -0.00643566\n",
      "trainer/policy/mean Std                  0.0209213\n",
      "trainer/policy/mean Max                  0.112645\n",
      "trainer/policy/mean Min                 -0.0492223\n",
      "trainer/policy/normal/std Mean           0.0949948\n",
      "trainer/policy/normal/std Std            0.0061272\n",
      "trainer/policy/normal/std Max            0.143983\n",
      "trainer/policy/normal/std Min            0.0702463\n",
      "trainer/policy/normal/log_std Mean      -2.35579\n",
      "trainer/policy/normal/log_std Std        0.0594542\n",
      "trainer/policy/normal/log_std Max       -1.93806\n",
      "trainer/policy/normal/log_std Min       -2.65575\n",
      "trainer/Alpha                            0.00183391\n",
      "trainer/Alpha Loss                       0.142471\n",
      "exploration/num steps total          44000\n",
      "exploration/num paths total             88\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000865253\n",
      "exploration/Rewards Std                  0.0011574\n",
      "exploration/Rewards Max                 -4.22873e-08\n",
      "exploration/Rewards Min                 -0.00812629\n",
      "exploration/Returns Mean                -0.432626\n",
      "exploration/Returns Std                  0.4326\n",
      "exploration/Returns Max                 -2.65906e-05\n",
      "exploration/Returns Min                 -0.865226\n",
      "exploration/Actions Mean                -0.0133489\n",
      "exploration/Actions Std                  0.0920562\n",
      "exploration/Actions Max                  0.266253\n",
      "exploration/Actions Min                 -0.285067\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.432626\n",
      "evaluation/num steps total          214785\n",
      "evaluation/num paths total             215\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.43511e-05\n",
      "evaluation/Rewards Std                   1.14991e-05\n",
      "evaluation/Rewards Max                  -1.04366e-10\n",
      "evaluation/Rewards Min                  -8.16902e-05\n",
      "evaluation/Returns Mean                 -0.0143367\n",
      "evaluation/Returns Std                   0.00282145\n",
      "evaluation/Returns Max                  -0.0113686\n",
      "evaluation/Returns Min                  -0.0194734\n",
      "evaluation/Actions Mean                 -0.0110068\n",
      "evaluation/Actions Std                   0.00472887\n",
      "evaluation/Actions Max                   0.00720302\n",
      "evaluation/Actions Min                  -0.0285815\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0143367\n",
      "time/data storing (s)                    0.00529555\n",
      "time/evaluation sampling (s)             7.79192\n",
      "time/exploration sampling (s)            1.81298\n",
      "time/logging (s)                         0.0175002\n",
      "time/sac training (s)                   24.9959\n",
      "time/saving (s)                          0.00646458\n",
      "time/training (s)                        5.1612e-05\n",
      "time/epoch (s)                          34.6301\n",
      "time/total (s)                        1610.34\n",
      "Epoch                                   42\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:47:35.235349 UTC | [] Epoch 43 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   45000\n",
      "trainer/num train calls              44000\n",
      "trainer/QF1 Loss                         7.09467e-06\n",
      "trainer/QF2 Loss                         3.91143e-06\n",
      "trainer/Policy Loss                     -0.600593\n",
      "trainer/Q1 Predictions Mean              0.593979\n",
      "trainer/Q1 Predictions Std               0.0603525\n",
      "trainer/Q1 Predictions Max               0.630872\n",
      "trainer/Q1 Predictions Min               0.230848\n",
      "trainer/Q2 Predictions Mean              0.593477\n",
      "trainer/Q2 Predictions Std               0.061096\n",
      "trainer/Q2 Predictions Max               0.6303\n",
      "trainer/Q2 Predictions Min               0.222401\n",
      "trainer/Q Targets Mean                   0.594031\n",
      "trainer/Q Targets Std                    0.0604066\n",
      "trainer/Q Targets Max                    0.63193\n",
      "trainer/Q Targets Min                    0.232383\n",
      "trainer/Log Pis Mean                     1.00699\n",
      "trainer/Log Pis Std                      0.65709\n",
      "trainer/Log Pis Max                      1.73879\n",
      "trainer/Log Pis Min                     -2.25615\n",
      "trainer/policy/mean Mean                -0.0218778\n",
      "trainer/policy/mean Std                  0.0197057\n",
      "trainer/policy/mean Max                  0.106129\n",
      "trainer/policy/mean Min                 -0.0655018\n",
      "trainer/policy/normal/std Mean           0.0886042\n",
      "trainer/policy/normal/std Std            0.00587114\n",
      "trainer/policy/normal/std Max            0.13433\n",
      "trainer/policy/normal/std Min            0.0610333\n",
      "trainer/policy/normal/log_std Mean      -2.42577\n",
      "trainer/policy/normal/log_std Std        0.0666941\n",
      "trainer/policy/normal/log_std Max       -2.00745\n",
      "trainer/policy/normal/log_std Min       -2.79634\n",
      "trainer/Alpha                            0.00181087\n",
      "trainer/Alpha Loss                       0.044118\n",
      "exploration/num steps total          45000\n",
      "exploration/num paths total             90\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000853987\n",
      "exploration/Rewards Std                  0.00121372\n",
      "exploration/Rewards Max                 -9.64883e-10\n",
      "exploration/Rewards Min                 -0.0102109\n",
      "exploration/Returns Mean                -0.426993\n",
      "exploration/Returns Std                  0.424363\n",
      "exploration/Returns Max                 -0.00263067\n",
      "exploration/Returns Min                 -0.851356\n",
      "exploration/Actions Mean                -0.0289915\n",
      "exploration/Actions Std                  0.087746\n",
      "exploration/Actions Max                  0.239331\n",
      "exploration/Actions Min                 -0.319546\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.426993\n",
      "evaluation/num steps total          219780\n",
      "evaluation/num paths total             220\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -6.84331e-05\n",
      "evaluation/Rewards Std                   3.43894e-05\n",
      "evaluation/Rewards Max                  -1.89199e-07\n",
      "evaluation/Rewards Min                  -0.00023778\n",
      "evaluation/Returns Mean                 -0.0683647\n",
      "evaluation/Returns Std                   0.00499686\n",
      "evaluation/Returns Max                  -0.0621353\n",
      "evaluation/Returns Min                  -0.0750911\n",
      "evaluation/Actions Mean                 -0.0253581\n",
      "evaluation/Actions Std                   0.00642626\n",
      "evaluation/Actions Max                  -0.00137548\n",
      "evaluation/Actions Min                  -0.0487627\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0683647\n",
      "time/data storing (s)                    0.00569304\n",
      "time/evaluation sampling (s)             7.91191\n",
      "time/exploration sampling (s)            1.88455\n",
      "time/logging (s)                         0.0178166\n",
      "time/sac training (s)                   24.9497\n",
      "time/saving (s)                          0.00593276\n",
      "time/training (s)                        3.91789e-05\n",
      "time/epoch (s)                          34.7757\n",
      "time/total (s)                        1647.34\n",
      "Epoch                                   43\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:48:12.454846 UTC | [] Epoch 44 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   46000\n",
      "trainer/num train calls              45000\n",
      "trainer/QF1 Loss                         0.00132591\n",
      "trainer/QF2 Loss                         0.00133095\n",
      "trainer/Policy Loss                     -0.548955\n",
      "trainer/Q1 Predictions Mean              0.539558\n",
      "trainer/Q1 Predictions Std               0.0607825\n",
      "trainer/Q1 Predictions Max               0.582045\n",
      "trainer/Q1 Predictions Min               0.20062\n",
      "trainer/Q2 Predictions Mean              0.54145\n",
      "trainer/Q2 Predictions Std               0.061017\n",
      "trainer/Q2 Predictions Max               0.583481\n",
      "trainer/Q2 Predictions Min               0.202625\n",
      "trainer/Q Targets Mean                   0.539657\n",
      "trainer/Q Targets Std                    0.0694789\n",
      "trainer/Q Targets Max                    0.583551\n",
      "trainer/Q Targets Min                   -0.00184304\n",
      "trainer/Log Pis Mean                     1.07459\n",
      "trainer/Log Pis Std                      0.558357\n",
      "trainer/Log Pis Max                      1.81453\n",
      "trainer/Log Pis Min                     -1.68605\n",
      "trainer/policy/mean Mean                -0.0153511\n",
      "trainer/policy/mean Std                  0.0186678\n",
      "trainer/policy/mean Max                  0.0718225\n",
      "trainer/policy/mean Min                 -0.0528952\n",
      "trainer/policy/normal/std Mean           0.0900351\n",
      "trainer/policy/normal/std Std            0.00544858\n",
      "trainer/policy/normal/std Max            0.115013\n",
      "trainer/policy/normal/std Min            0.0649246\n",
      "trainer/policy/normal/log_std Mean      -2.40939\n",
      "trainer/policy/normal/log_std Std        0.0606895\n",
      "trainer/policy/normal/log_std Max       -2.16271\n",
      "trainer/policy/normal/log_std Min       -2.73453\n",
      "trainer/Alpha                            0.0017515\n",
      "trainer/Alpha Loss                       0.473414\n",
      "exploration/num steps total          46000\n",
      "exploration/num paths total             92\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000805188\n",
      "exploration/Rewards Std                  0.00106869\n",
      "exploration/Rewards Max                 -1.28253e-13\n",
      "exploration/Rewards Min                 -0.00787701\n",
      "exploration/Returns Mean                -0.402594\n",
      "exploration/Returns Std                  0.402224\n",
      "exploration/Returns Max                 -0.000369772\n",
      "exploration/Returns Min                 -0.804818\n",
      "exploration/Actions Mean                -0.018378\n",
      "exploration/Actions Std                  0.0878301\n",
      "exploration/Actions Max                  0.253292\n",
      "exploration/Actions Min                 -0.28066\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.402594\n",
      "evaluation/num steps total          224775\n",
      "evaluation/num paths total             225\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -3.95523e-05\n",
      "evaluation/Rewards Std                   2.77998e-05\n",
      "evaluation/Rewards Max                  -2.73873e-10\n",
      "evaluation/Rewards Min                  -0.000173749\n",
      "evaluation/Returns Mean                 -0.0395127\n",
      "evaluation/Returns Std                   0.00385588\n",
      "evaluation/Returns Max                  -0.0339891\n",
      "evaluation/Returns Min                  -0.0444184\n",
      "evaluation/Actions Mean                 -0.0186713\n",
      "evaluation/Actions Std                   0.00684883\n",
      "evaluation/Actions Max                   0.00275163\n",
      "evaluation/Actions Min                  -0.0416832\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0395127\n",
      "time/data storing (s)                    0.00571349\n",
      "time/evaluation sampling (s)             7.98251\n",
      "time/exploration sampling (s)            1.82037\n",
      "time/logging (s)                         0.0688526\n",
      "time/sac training (s)                   25.304\n",
      "time/saving (s)                          0.0064098\n",
      "time/training (s)                        4.24385e-05\n",
      "time/epoch (s)                          35.1879\n",
      "time/total (s)                        1684.59\n",
      "Epoch                                   44\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:48:49.512877 UTC | [] Epoch 45 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   47000\n",
      "trainer/num train calls              46000\n",
      "trainer/QF1 Loss                         6.51703e-06\n",
      "trainer/QF2 Loss                         6.54723e-06\n",
      "trainer/Policy Loss                     -0.506933\n",
      "trainer/Q1 Predictions Mean              0.498492\n",
      "trainer/Q1 Predictions Std               0.0589351\n",
      "trainer/Q1 Predictions Max               0.537817\n",
      "trainer/Q1 Predictions Min               0.21591\n",
      "trainer/Q2 Predictions Mean              0.498917\n",
      "trainer/Q2 Predictions Std               0.0589652\n",
      "trainer/Q2 Predictions Max               0.538065\n",
      "trainer/Q2 Predictions Min               0.217881\n",
      "trainer/Q Targets Mean                   0.500048\n",
      "trainer/Q Targets Std                    0.0589302\n",
      "trainer/Q Targets Max                    0.539817\n",
      "trainer/Q Targets Min                    0.217496\n",
      "trainer/Log Pis Mean                     1.08858\n",
      "trainer/Log Pis Std                      0.724884\n",
      "trainer/Log Pis Max                      1.75155\n",
      "trainer/Log Pis Min                     -4.71556\n",
      "trainer/policy/mean Mean                -0.00754735\n",
      "trainer/policy/mean Std                  0.0166444\n",
      "trainer/policy/mean Max                  0.116571\n",
      "trainer/policy/mean Min                 -0.035229\n",
      "trainer/policy/normal/std Mean           0.0841234\n",
      "trainer/policy/normal/std Std            0.00529235\n",
      "trainer/policy/normal/std Max            0.124129\n",
      "trainer/policy/normal/std Min            0.0624567\n",
      "trainer/policy/normal/log_std Mean      -2.47731\n",
      "trainer/policy/normal/log_std Std        0.059868\n",
      "trainer/policy/normal/log_std Max       -2.08643\n",
      "trainer/policy/normal/log_std Min       -2.77328\n",
      "trainer/Alpha                            0.0017706\n",
      "trainer/Alpha Loss                       0.561291\n",
      "exploration/num steps total          47000\n",
      "exploration/num paths total             94\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000716872\n",
      "exploration/Rewards Std                  0.00101876\n",
      "exploration/Rewards Max                 -1.97872e-09\n",
      "exploration/Rewards Min                 -0.00789619\n",
      "exploration/Returns Mean                -0.358436\n",
      "exploration/Returns Std                  0.354941\n",
      "exploration/Returns Max                 -0.00349493\n",
      "exploration/Returns Min                 -0.713377\n",
      "exploration/Actions Mean                -0.0151992\n",
      "exploration/Actions Std                  0.0832929\n",
      "exploration/Actions Max                  0.239106\n",
      "exploration/Actions Min                 -0.281002\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.358436\n",
      "evaluation/num steps total          229770\n",
      "evaluation/num paths total             230\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.63441e-05\n",
      "evaluation/Rewards Std                   1.12458e-05\n",
      "evaluation/Rewards Max                  -1.88288e-10\n",
      "evaluation/Rewards Min                  -8.30131e-05\n",
      "evaluation/Returns Mean                 -0.0163278\n",
      "evaluation/Returns Std                   0.00026454\n",
      "evaluation/Returns Max                  -0.0159167\n",
      "evaluation/Returns Min                  -0.0166887\n",
      "evaluation/Actions Mean                 -0.0117757\n",
      "evaluation/Actions Std                   0.00497734\n",
      "evaluation/Actions Max                   0.010073\n",
      "evaluation/Actions Min                  -0.028812\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0163278\n",
      "time/data storing (s)                    0.0580369\n",
      "time/evaluation sampling (s)             7.8265\n",
      "time/exploration sampling (s)            1.70892\n",
      "time/logging (s)                         0.0161939\n",
      "time/sac training (s)                   25.0513\n",
      "time/saving (s)                          0.0589898\n",
      "time/training (s)                        9.20799e-05\n",
      "time/epoch (s)                          34.7201\n",
      "time/total (s)                        1721.58\n",
      "Epoch                                   45\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:49:26.547052 UTC | [] Epoch 46 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   48000\n",
      "trainer/num train calls              47000\n",
      "trainer/QF1 Loss                         7.44064e-06\n",
      "trainer/QF2 Loss                         1.1868e-05\n",
      "trainer/Policy Loss                     -0.467175\n",
      "trainer/Q1 Predictions Mean              0.459122\n",
      "trainer/Q1 Predictions Std               0.0593151\n",
      "trainer/Q1 Predictions Max               0.495705\n",
      "trainer/Q1 Predictions Min               0.170239\n",
      "trainer/Q2 Predictions Mean              0.457805\n",
      "trainer/Q2 Predictions Std               0.0593636\n",
      "trainer/Q2 Predictions Max               0.493469\n",
      "trainer/Q2 Predictions Min               0.16962\n",
      "trainer/Q Targets Mean                   0.46058\n",
      "trainer/Q Targets Std                    0.060059\n",
      "trainer/Q Targets Max                    0.497457\n",
      "trainer/Q Targets Min                    0.169562\n",
      "trainer/Log Pis Mean                     0.960221\n",
      "trainer/Log Pis Std                      0.702895\n",
      "trainer/Log Pis Max                      1.54101\n",
      "trainer/Log Pis Min                     -3.45668\n",
      "trainer/policy/mean Mean                 0.0115572\n",
      "trainer/policy/mean Std                  0.0137528\n",
      "trainer/policy/mean Max                  0.0961016\n",
      "trainer/policy/mean Min                 -0.0232426\n",
      "trainer/policy/normal/std Mean           0.0948269\n",
      "trainer/policy/normal/std Std            0.00458026\n",
      "trainer/policy/normal/std Max            0.121465\n",
      "trainer/policy/normal/std Min            0.0654728\n",
      "trainer/policy/normal/log_std Mean      -2.35692\n",
      "trainer/policy/normal/log_std Std        0.0500365\n",
      "trainer/policy/normal/log_std Max       -2.10813\n",
      "trainer/policy/normal/log_std Min       -2.72612\n",
      "trainer/Alpha                            0.0016963\n",
      "trainer/Alpha Loss                      -0.253762\n",
      "exploration/num steps total          48000\n",
      "exploration/num paths total             96\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000919028\n",
      "exploration/Rewards Std                  0.00123833\n",
      "exploration/Rewards Max                 -1.62396e-09\n",
      "exploration/Rewards Min                 -0.00848288\n",
      "exploration/Returns Mean                -0.459514\n",
      "exploration/Returns Std                  0.458506\n",
      "exploration/Returns Max                 -0.00100753\n",
      "exploration/Returns Min                 -0.91802\n",
      "exploration/Actions Mean                 0.00833552\n",
      "exploration/Actions Std                  0.0955028\n",
      "exploration/Actions Max                  0.273581\n",
      "exploration/Actions Min                 -0.291254\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.459514\n",
      "evaluation/num steps total          234765\n",
      "evaluation/num paths total             235\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.36945e-05\n",
      "evaluation/Rewards Std                   9.54626e-06\n",
      "evaluation/Rewards Max                  -7.29546e-10\n",
      "evaluation/Rewards Min                  -6.13731e-05\n",
      "evaluation/Returns Mean                 -0.0136808\n",
      "evaluation/Returns Std                   0.00124516\n",
      "evaluation/Returns Max                  -0.0120993\n",
      "evaluation/Returns Min                  -0.015216\n",
      "evaluation/Actions Mean                  0.0110256\n",
      "evaluation/Actions Std                   0.00392194\n",
      "evaluation/Actions Max                   0.0247736\n",
      "evaluation/Actions Min                  -0.00182471\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0136808\n",
      "time/data storing (s)                    0.00573397\n",
      "time/evaluation sampling (s)             8.00005\n",
      "time/exploration sampling (s)            1.88613\n",
      "time/logging (s)                         0.0702315\n",
      "time/sac training (s)                   25.1878\n",
      "time/saving (s)                          0.00659166\n",
      "time/training (s)                        4.88833e-05\n",
      "time/epoch (s)                          35.1565\n",
      "time/total (s)                        1758.66\n",
      "Epoch                                   46\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:50:03.540444 UTC | [] Epoch 47 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   49000\n",
      "trainer/num train calls              48000\n",
      "trainer/QF1 Loss                         5.20964e-06\n",
      "trainer/QF2 Loss                         5.72925e-06\n",
      "trainer/Policy Loss                     -0.432256\n",
      "trainer/Q1 Predictions Mean              0.425025\n",
      "trainer/Q1 Predictions Std               0.0496893\n",
      "trainer/Q1 Predictions Max               0.45754\n",
      "trainer/Q1 Predictions Min               0.0750412\n",
      "trainer/Q2 Predictions Mean              0.426485\n",
      "trainer/Q2 Predictions Std               0.0496037\n",
      "trainer/Q2 Predictions Max               0.459184\n",
      "trainer/Q2 Predictions Min               0.0745392\n",
      "trainer/Q Targets Mean                   0.425477\n",
      "trainer/Q Targets Std                    0.0498527\n",
      "trainer/Q Targets Max                    0.458516\n",
      "trainer/Q Targets Min                    0.0691212\n",
      "trainer/Log Pis Mean                     0.943735\n",
      "trainer/Log Pis Std                      0.707857\n",
      "trainer/Log Pis Max                      1.49272\n",
      "trainer/Log Pis Min                     -2.30734\n",
      "trainer/policy/mean Mean                 0.0184572\n",
      "trainer/policy/mean Std                  0.0113948\n",
      "trainer/policy/mean Max                  0.110508\n",
      "trainer/policy/mean Min                 -0.0164107\n",
      "trainer/policy/normal/std Mean           0.0915049\n",
      "trainer/policy/normal/std Std            0.00299113\n",
      "trainer/policy/normal/std Max            0.114793\n",
      "trainer/policy/normal/std Min            0.0730282\n",
      "trainer/policy/normal/log_std Mean      -2.3919\n",
      "trainer/policy/normal/log_std Std        0.0328325\n",
      "trainer/policy/normal/log_std Max       -2.16463\n",
      "trainer/policy/normal/log_std Min       -2.61691\n",
      "trainer/Alpha                            0.00167195\n",
      "trainer/Alpha Loss                      -0.359746\n",
      "exploration/num steps total          49000\n",
      "exploration/num paths total             98\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000850637\n",
      "exploration/Rewards Std                  0.00119848\n",
      "exploration/Rewards Max                 -1.16133e-09\n",
      "exploration/Rewards Min                 -0.00875558\n",
      "exploration/Returns Mean                -0.425318\n",
      "exploration/Returns Std                  0.425251\n",
      "exploration/Returns Max                 -6.76125e-05\n",
      "exploration/Returns Min                 -0.850569\n",
      "exploration/Actions Mean                 0.0142052\n",
      "exploration/Actions Std                  0.0911295\n",
      "exploration/Actions Max                  0.289189\n",
      "exploration/Actions Min                 -0.295898\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.425318\n",
      "evaluation/num steps total          239760\n",
      "evaluation/num paths total             240\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -3.746e-05\n",
      "evaluation/Rewards Std                   1.63806e-05\n",
      "evaluation/Rewards Max                  -4.03654e-06\n",
      "evaluation/Rewards Min                  -0.000109594\n",
      "evaluation/Returns Mean                 -0.0374226\n",
      "evaluation/Returns Std                   0.00202799\n",
      "evaluation/Returns Max                  -0.0346729\n",
      "evaluation/Returns Min                  -0.0409316\n",
      "evaluation/Actions Mean                  0.0189137\n",
      "evaluation/Actions Std                   0.00410756\n",
      "evaluation/Actions Max                   0.033105\n",
      "evaluation/Actions Min                   0.00635334\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0374226\n",
      "time/data storing (s)                    0.0538873\n",
      "time/evaluation sampling (s)             7.88336\n",
      "time/exploration sampling (s)            1.84555\n",
      "time/logging (s)                         0.0176436\n",
      "time/sac training (s)                   25.3415\n",
      "time/saving (s)                          0.00662564\n",
      "time/training (s)                        4.18909e-05\n",
      "time/epoch (s)                          35.1486\n",
      "time/total (s)                        1795.58\n",
      "Epoch                                   47\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:50:40.542167 UTC | [] Epoch 48 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   50000\n",
      "trainer/num train calls              49000\n",
      "trainer/QF1 Loss                         4.72672e-06\n",
      "trainer/QF2 Loss                         4.91745e-06\n",
      "trainer/Policy Loss                     -0.392389\n",
      "trainer/Q1 Predictions Mean              0.384255\n",
      "trainer/Q1 Predictions Std               0.0597101\n",
      "trainer/Q1 Predictions Max               0.422285\n",
      "trainer/Q1 Predictions Min               0.0433704\n",
      "trainer/Q2 Predictions Mean              0.383861\n",
      "trainer/Q2 Predictions Std               0.0594611\n",
      "trainer/Q2 Predictions Max               0.422154\n",
      "trainer/Q2 Predictions Min               0.0501942\n",
      "trainer/Q Targets Mean                   0.383322\n",
      "trainer/Q Targets Std                    0.0593873\n",
      "trainer/Q Targets Max                    0.420991\n",
      "trainer/Q Targets Min                    0.0452112\n",
      "trainer/Log Pis Mean                     1.01748\n",
      "trainer/Log Pis Std                      0.681462\n",
      "trainer/Log Pis Max                      1.73876\n",
      "trainer/Log Pis Min                     -4.30679\n",
      "trainer/policy/mean Mean                -0.00495087\n",
      "trainer/policy/mean Std                  0.0175853\n",
      "trainer/policy/mean Max                  0.125475\n",
      "trainer/policy/mean Min                 -0.0607966\n",
      "trainer/policy/normal/std Mean           0.0903044\n",
      "trainer/policy/normal/std Std            0.00367918\n",
      "trainer/policy/normal/std Max            0.113586\n",
      "trainer/policy/normal/std Min            0.068353\n",
      "trainer/policy/normal/log_std Mean      -2.40546\n",
      "trainer/policy/normal/log_std Std        0.0428556\n",
      "trainer/policy/normal/log_std Max       -2.1752\n",
      "trainer/policy/normal/log_std Min       -2.68307\n",
      "trainer/Alpha                            0.00167798\n",
      "trainer/Alpha Loss                       0.111694\n",
      "exploration/num steps total          50000\n",
      "exploration/num paths total            100\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000844472\n",
      "exploration/Rewards Std                  0.00109909\n",
      "exploration/Rewards Max                 -9.20899e-10\n",
      "exploration/Rewards Min                 -0.00800333\n",
      "exploration/Returns Mean                -0.422236\n",
      "exploration/Returns Std                  0.4206\n",
      "exploration/Returns Max                 -0.00163534\n",
      "exploration/Returns Min                 -0.842836\n",
      "exploration/Actions Mean                -0.00262481\n",
      "exploration/Actions Std                  0.0918576\n",
      "exploration/Actions Max                  0.226916\n",
      "exploration/Actions Min                 -0.282902\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.422236\n",
      "evaluation/num steps total          244755\n",
      "evaluation/num paths total             245\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -5.76522e-06\n",
      "evaluation/Rewards Std                   4.93306e-06\n",
      "evaluation/Rewards Max                  -1.05103e-11\n",
      "evaluation/Rewards Min                  -2.8675e-05\n",
      "evaluation/Returns Mean                 -0.00575945\n",
      "evaluation/Returns Std                   0.000507326\n",
      "evaluation/Returns Max                  -0.00506719\n",
      "evaluation/Returns Min                  -0.00637177\n",
      "evaluation/Actions Mean                 -0.00652953\n",
      "evaluation/Actions Std                   0.00387523\n",
      "evaluation/Actions Max                   0.00517154\n",
      "evaluation/Actions Min                  -0.0169337\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.00575945\n",
      "time/data storing (s)                    0.00548257\n",
      "time/evaluation sampling (s)             8.20186\n",
      "time/exploration sampling (s)            1.71097\n",
      "time/logging (s)                         0.0769357\n",
      "time/sac training (s)                   24.8379\n",
      "time/saving (s)                          0.00643353\n",
      "time/training (s)                        4.12874e-05\n",
      "time/epoch (s)                          34.8396\n",
      "time/total (s)                        1832.63\n",
      "Epoch                                   48\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:51:17.630828 UTC | [] Epoch 49 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   51000\n",
      "trainer/num train calls              50000\n",
      "trainer/QF1 Loss                         0.000565941\n",
      "trainer/QF2 Loss                         0.00056618\n",
      "trainer/Policy Loss                     -0.355663\n",
      "trainer/Q1 Predictions Mean              0.34967\n",
      "trainer/Q1 Predictions Std               0.0546187\n",
      "trainer/Q1 Predictions Max               0.382465\n",
      "trainer/Q1 Predictions Min               0.0486348\n",
      "trainer/Q2 Predictions Mean              0.35069\n",
      "trainer/Q2 Predictions Std               0.0546402\n",
      "trainer/Q2 Predictions Max               0.384564\n",
      "trainer/Q2 Predictions Min               0.0544943\n",
      "trainer/Q Targets Mean                   0.351289\n",
      "trainer/Q Targets Std                    0.0592314\n",
      "trainer/Q Targets Max                    0.386397\n",
      "trainer/Q Targets Min                   -0.000109907\n",
      "trainer/Log Pis Mean                     1.02871\n",
      "trainer/Log Pis Std                      0.692004\n",
      "trainer/Log Pis Max                      1.59416\n",
      "trainer/Log Pis Min                     -2.75031\n",
      "trainer/policy/mean Mean                -0.00302058\n",
      "trainer/policy/mean Std                  0.0135405\n",
      "trainer/policy/mean Max                  0.0965561\n",
      "trainer/policy/mean Min                 -0.0474518\n",
      "trainer/policy/normal/std Mean           0.0882835\n",
      "trainer/policy/normal/std Std            0.00320429\n",
      "trainer/policy/normal/std Max            0.110475\n",
      "trainer/policy/normal/std Min            0.0734977\n",
      "trainer/policy/normal/log_std Mean      -2.42785\n",
      "trainer/policy/normal/log_std Std        0.0357641\n",
      "trainer/policy/normal/log_std Max       -2.20297\n",
      "trainer/policy/normal/log_std Min       -2.6105\n",
      "trainer/Alpha                            0.00168925\n",
      "trainer/Alpha Loss                       0.183279\n",
      "exploration/num steps total          51000\n",
      "exploration/num paths total            102\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000852861\n",
      "exploration/Rewards Std                  0.00114298\n",
      "exploration/Rewards Max                 -3.27418e-10\n",
      "exploration/Rewards Min                 -0.00796238\n",
      "exploration/Returns Mean                -0.426431\n",
      "exploration/Returns Std                  0.426281\n",
      "exploration/Returns Max                 -0.000149768\n",
      "exploration/Returns Min                 -0.852711\n",
      "exploration/Actions Mean                -0.00511775\n",
      "exploration/Actions Std                  0.0922086\n",
      "exploration/Actions Max                  0.282177\n",
      "exploration/Actions Min                 -0.266455\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.426431\n",
      "evaluation/num steps total          249750\n",
      "evaluation/num paths total             250\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -5.35346e-06\n",
      "evaluation/Rewards Std                   6.59457e-06\n",
      "evaluation/Rewards Max                  -2.98783e-13\n",
      "evaluation/Rewards Min                  -4.44804e-05\n",
      "evaluation/Returns Mean                 -0.0053481\n",
      "evaluation/Returns Std                   0.00195522\n",
      "evaluation/Returns Max                  -0.00293312\n",
      "evaluation/Returns Min                  -0.00726951\n",
      "evaluation/Actions Mean                 -0.00405761\n",
      "evaluation/Actions Std                   0.00608854\n",
      "evaluation/Actions Max                   0.0141903\n",
      "evaluation/Actions Min                  -0.0210904\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0053481\n",
      "time/data storing (s)                    0.00553883\n",
      "time/evaluation sampling (s)             7.92271\n",
      "time/exploration sampling (s)            1.88474\n",
      "time/logging (s)                         0.0162392\n",
      "time/sac training (s)                   24.6385\n",
      "time/saving (s)                          0.00577254\n",
      "time/training (s)                        4.17754e-05\n",
      "time/epoch (s)                          34.4735\n",
      "time/total (s)                        1869.64\n",
      "Epoch                                   49\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:51:54.651349 UTC | [] Epoch 50 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   52000\n",
      "trainer/num train calls              51000\n",
      "trainer/QF1 Loss                         6.18297e-06\n",
      "trainer/QF2 Loss                         4.88076e-06\n",
      "trainer/Policy Loss                     -0.324654\n",
      "trainer/Q1 Predictions Mean              0.31685\n",
      "trainer/Q1 Predictions Std               0.0581957\n",
      "trainer/Q1 Predictions Max               0.351336\n",
      "trainer/Q1 Predictions Min              -0.0375815\n",
      "trainer/Q2 Predictions Mean              0.318199\n",
      "trainer/Q2 Predictions Std               0.0583802\n",
      "trainer/Q2 Predictions Max               0.353746\n",
      "trainer/Q2 Predictions Min              -0.0402105\n",
      "trainer/Q Targets Mean                   0.318002\n",
      "trainer/Q Targets Std                    0.0591693\n",
      "trainer/Q Targets Max                    0.35289\n",
      "trainer/Q Targets Min                   -0.0481785\n",
      "trainer/Log Pis Mean                     0.95901\n",
      "trainer/Log Pis Std                      0.7812\n",
      "trainer/Log Pis Max                      1.56154\n",
      "trainer/Log Pis Min                     -3.42667\n",
      "trainer/policy/mean Mean                -0.00351798\n",
      "trainer/policy/mean Std                  0.0124352\n",
      "trainer/policy/mean Max                  0.0611982\n",
      "trainer/policy/mean Min                 -0.0616243\n",
      "trainer/policy/normal/std Mean           0.090257\n",
      "trainer/policy/normal/std Std            0.00383109\n",
      "trainer/policy/normal/std Max            0.127089\n",
      "trainer/policy/normal/std Min            0.0750034\n",
      "trainer/policy/normal/log_std Mean      -2.40595\n",
      "trainer/policy/normal/log_std Std        0.0408648\n",
      "trainer/policy/normal/log_std Max       -2.06287\n",
      "trainer/policy/normal/log_std Min       -2.59022\n",
      "trainer/Alpha                            0.00168337\n",
      "trainer/Alpha Loss                      -0.2618\n",
      "exploration/num steps total          52000\n",
      "exploration/num paths total            104\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000856621\n",
      "exploration/Rewards Std                  0.00120896\n",
      "exploration/Rewards Max                 -4.15925e-10\n",
      "exploration/Rewards Min                 -0.0123315\n",
      "exploration/Returns Mean                -0.428311\n",
      "exploration/Returns Std                  0.427875\n",
      "exploration/Returns Max                 -0.000435622\n",
      "exploration/Returns Min                 -0.856186\n",
      "exploration/Actions Mean                -0.00452784\n",
      "exploration/Actions Std                  0.092443\n",
      "exploration/Actions Max                  0.28521\n",
      "exploration/Actions Min                 -0.351163\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.428311\n",
      "evaluation/num steps total          254745\n",
      "evaluation/num paths total             255\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.90239e-06\n",
      "evaluation/Rewards Std                   3.42835e-06\n",
      "evaluation/Rewards Max                  -1.71951e-13\n",
      "evaluation/Rewards Min                  -3.2335e-05\n",
      "evaluation/Returns Mean                 -0.00190049\n",
      "evaluation/Returns Std                   0.00180187\n",
      "evaluation/Returns Max                  -0.000585637\n",
      "evaluation/Returns Min                  -0.00542725\n",
      "evaluation/Actions Mean                 -0.00282069\n",
      "evaluation/Actions Std                   0.0033268\n",
      "evaluation/Actions Max                   0.00990002\n",
      "evaluation/Actions Min                  -0.0179819\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.00190049\n",
      "time/data storing (s)                    0.00576114\n",
      "time/evaluation sampling (s)             7.77689\n",
      "time/exploration sampling (s)            1.82057\n",
      "time/logging (s)                         0.0656637\n",
      "time/sac training (s)                   25.1783\n",
      "time/saving (s)                          0.00670137\n",
      "time/training (s)                        4.83599e-05\n",
      "time/epoch (s)                          34.854\n",
      "time/total (s)                        1906.7\n",
      "Epoch                                   50\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:52:31.747538 UTC | [] Epoch 51 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   53000\n",
      "trainer/num train calls              52000\n",
      "trainer/QF1 Loss                         2.54746e-06\n",
      "trainer/QF2 Loss                         1.79948e-06\n",
      "trainer/Policy Loss                     -0.288554\n",
      "trainer/Q1 Predictions Mean              0.282884\n",
      "trainer/Q1 Predictions Std               0.0617545\n",
      "trainer/Q1 Predictions Max               0.319879\n",
      "trainer/Q1 Predictions Min              -0.0115703\n",
      "trainer/Q2 Predictions Mean              0.282653\n",
      "trainer/Q2 Predictions Std               0.0617237\n",
      "trainer/Q2 Predictions Max               0.320682\n",
      "trainer/Q2 Predictions Min              -0.0103918\n",
      "trainer/Q Targets Mean                   0.282856\n",
      "trainer/Q Targets Std                    0.0620434\n",
      "trainer/Q Targets Max                    0.320659\n",
      "trainer/Q Targets Min                   -0.0120979\n",
      "trainer/Log Pis Mean                     1.05091\n",
      "trainer/Log Pis Std                      0.620036\n",
      "trainer/Log Pis Max                      1.73403\n",
      "trainer/Log Pis Min                     -2.07084\n",
      "trainer/policy/mean Mean                -0.00620479\n",
      "trainer/policy/mean Std                  0.0178614\n",
      "trainer/policy/mean Max                  0.112863\n",
      "trainer/policy/mean Min                 -0.0494156\n",
      "trainer/policy/normal/std Mean           0.0879625\n",
      "trainer/policy/normal/std Std            0.00433741\n",
      "trainer/policy/normal/std Max            0.112219\n",
      "trainer/policy/normal/std Min            0.0662155\n",
      "trainer/policy/normal/log_std Mean      -2.43207\n",
      "trainer/policy/normal/log_std Std        0.0496695\n",
      "trainer/policy/normal/log_std Max       -2.1873\n",
      "trainer/policy/normal/log_std Min       -2.71484\n",
      "trainer/Alpha                            0.00169157\n",
      "trainer/Alpha Loss                       0.324905\n",
      "exploration/num steps total          53000\n",
      "exploration/num paths total            106\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000819605\n",
      "exploration/Rewards Std                  0.00113635\n",
      "exploration/Rewards Max                 -6.69175e-11\n",
      "exploration/Rewards Min                 -0.00896027\n",
      "exploration/Returns Mean                -0.409803\n",
      "exploration/Returns Std                  0.409791\n",
      "exploration/Returns Max                 -1.17873e-05\n",
      "exploration/Returns Min                 -0.819593\n",
      "exploration/Actions Mean                -0.0167534\n",
      "exploration/Actions Std                  0.0889684\n",
      "exploration/Actions Max                  0.261518\n",
      "exploration/Actions Min                 -0.299337\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.409803\n",
      "evaluation/num steps total          259740\n",
      "evaluation/num paths total             260\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.29138e-05\n",
      "evaluation/Rewards Std                   7.91915e-06\n",
      "evaluation/Rewards Max                  -4.10694e-13\n",
      "evaluation/Rewards Min                  -4.01921e-05\n",
      "evaluation/Returns Mean                 -0.0129009\n",
      "evaluation/Returns Std                   0.000188086\n",
      "evaluation/Returns Max                  -0.0125529\n",
      "evaluation/Returns Min                  -0.0130587\n",
      "evaluation/Actions Mean                 -0.0106222\n",
      "evaluation/Actions Std                   0.0040382\n",
      "evaluation/Actions Max                   0.00441917\n",
      "evaluation/Actions Min                  -0.020048\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0129009\n",
      "time/data storing (s)                    0.00548311\n",
      "time/evaluation sampling (s)             8.21716\n",
      "time/exploration sampling (s)            1.81731\n",
      "time/logging (s)                         0.0698285\n",
      "time/sac training (s)                   24.9493\n",
      "time/saving (s)                          0.00661869\n",
      "time/training (s)                        4.78774e-05\n",
      "time/epoch (s)                          35.0657\n",
      "time/total (s)                        1943.78\n",
      "Epoch                                   51\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:53:08.637271 UTC | [] Epoch 52 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   54000\n",
      "trainer/num train calls              53000\n",
      "trainer/QF1 Loss                         1.64689e-06\n",
      "trainer/QF2 Loss                         4.30477e-06\n",
      "trainer/Policy Loss                     -0.271453\n",
      "trainer/Q1 Predictions Mean              0.265472\n",
      "trainer/Q1 Predictions Std               0.0458952\n",
      "trainer/Q1 Predictions Max               0.291579\n",
      "trainer/Q1 Predictions Min              -0.0431057\n",
      "trainer/Q2 Predictions Mean              0.266683\n",
      "trainer/Q2 Predictions Std               0.0454316\n",
      "trainer/Q2 Predictions Max               0.292626\n",
      "trainer/Q2 Predictions Min              -0.0335336\n",
      "trainer/Q Targets Mean                   0.26496\n",
      "trainer/Q Targets Std                    0.0455258\n",
      "trainer/Q Targets Max                    0.291622\n",
      "trainer/Q Targets Min                   -0.0402866\n",
      "trainer/Log Pis Mean                     1.05285\n",
      "trainer/Log Pis Std                      0.641872\n",
      "trainer/Log Pis Max                      1.5723\n",
      "trainer/Log Pis Min                     -2.24504\n",
      "trainer/policy/mean Mean                -0.0144317\n",
      "trainer/policy/mean Std                  0.00985058\n",
      "trainer/policy/mean Max                  0.0630472\n",
      "trainer/policy/mean Min                 -0.0343872\n",
      "trainer/policy/normal/std Mean           0.0870999\n",
      "trainer/policy/normal/std Std            0.00186157\n",
      "trainer/policy/normal/std Max            0.101739\n",
      "trainer/policy/normal/std Min            0.0803761\n",
      "trainer/policy/normal/log_std Mean      -2.44093\n",
      "trainer/policy/normal/log_std Std        0.0211646\n",
      "trainer/policy/normal/log_std Max       -2.28535\n",
      "trainer/policy/normal/log_std Min       -2.52104\n",
      "trainer/Alpha                            0.0016624\n",
      "trainer/Alpha Loss                       0.338236\n",
      "exploration/num steps total          54000\n",
      "exploration/num paths total            108\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000755567\n",
      "exploration/Rewards Std                  0.000999152\n",
      "exploration/Rewards Max                 -3.24022e-09\n",
      "exploration/Rewards Min                 -0.00751695\n",
      "exploration/Returns Mean                -0.377784\n",
      "exploration/Returns Std                  0.377164\n",
      "exploration/Returns Max                 -0.000619025\n",
      "exploration/Returns Min                 -0.754948\n",
      "exploration/Actions Mean                -0.0178537\n",
      "exploration/Actions Std                  0.0850701\n",
      "exploration/Actions Max                  0.231605\n",
      "exploration/Actions Min                 -0.274171\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.377784\n",
      "evaluation/num steps total          264735\n",
      "evaluation/num paths total             265\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -2.78607e-05\n",
      "evaluation/Rewards Std                   1.37541e-05\n",
      "evaluation/Rewards Max                  -6.66095e-11\n",
      "evaluation/Rewards Min                  -7.8433e-05\n",
      "evaluation/Returns Mean                 -0.0278328\n",
      "evaluation/Returns Std                   0.0015168\n",
      "evaluation/Returns Max                  -0.0250867\n",
      "evaluation/Returns Min                  -0.0292388\n",
      "evaluation/Actions Mean                 -0.0158972\n",
      "evaluation/Actions Std                   0.00508785\n",
      "evaluation/Actions Max                   0.00283848\n",
      "evaluation/Actions Min                  -0.0280059\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0278328\n",
      "time/data storing (s)                    0.00504468\n",
      "time/evaluation sampling (s)             7.72335\n",
      "time/exploration sampling (s)            1.81068\n",
      "time/logging (s)                         0.0191472\n",
      "time/sac training (s)                   24.6148\n",
      "time/saving (s)                          0.00726352\n",
      "time/training (s)                        5.49667e-05\n",
      "time/epoch (s)                          34.1804\n",
      "time/total (s)                        1980.61\n",
      "Epoch                                   52\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:53:45.849414 UTC | [] Epoch 53 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   55000\n",
      "trainer/num train calls              54000\n",
      "trainer/QF1 Loss                         1.54447e-05\n",
      "trainer/QF2 Loss                         1.44347e-05\n",
      "trainer/Policy Loss                     -0.237296\n",
      "trainer/Q1 Predictions Mean              0.231771\n",
      "trainer/Q1 Predictions Std               0.0517514\n",
      "trainer/Q1 Predictions Max               0.25905\n",
      "trainer/Q1 Predictions Min              -0.100829\n",
      "trainer/Q2 Predictions Mean              0.231214\n",
      "trainer/Q2 Predictions Std               0.0516193\n",
      "trainer/Q2 Predictions Max               0.258855\n",
      "trainer/Q2 Predictions Min              -0.0820088\n",
      "trainer/Q Targets Mean                   0.234734\n",
      "trainer/Q Targets Std                    0.0523319\n",
      "trainer/Q Targets Max                    0.262248\n",
      "trainer/Q Targets Min                   -0.0937966\n",
      "trainer/Log Pis Mean                     0.926333\n",
      "trainer/Log Pis Std                      0.735755\n",
      "trainer/Log Pis Max                      1.5396\n",
      "trainer/Log Pis Min                     -2.681\n",
      "trainer/policy/mean Mean                -0.00322063\n",
      "trainer/policy/mean Std                  0.0111869\n",
      "trainer/policy/mean Max                  0.0582946\n",
      "trainer/policy/mean Min                 -0.045489\n",
      "trainer/policy/normal/std Mean           0.0908813\n",
      "trainer/policy/normal/std Std            0.00257408\n",
      "trainer/policy/normal/std Max            0.106718\n",
      "trainer/policy/normal/std Min            0.0787842\n",
      "trainer/policy/normal/log_std Mean      -2.3986\n",
      "trainer/policy/normal/log_std Std        0.028241\n",
      "trainer/policy/normal/log_std Max       -2.23757\n",
      "trainer/policy/normal/log_std Min       -2.54104\n",
      "trainer/Alpha                            0.00166218\n",
      "trainer/Alpha Loss                      -0.471439\n",
      "exploration/num steps total          55000\n",
      "exploration/num paths total            110\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.00083709\n",
      "exploration/Rewards Std                  0.0011918\n",
      "exploration/Rewards Max                 -5.13012e-13\n",
      "exploration/Rewards Min                 -0.009053\n",
      "exploration/Returns Mean                -0.418545\n",
      "exploration/Returns Std                  0.418541\n",
      "exploration/Returns Max                 -4.29996e-06\n",
      "exploration/Returns Min                 -0.837086\n",
      "exploration/Actions Mean                -0.0085208\n",
      "exploration/Actions Std                  0.091095\n",
      "exploration/Actions Max                  0.300882\n",
      "exploration/Actions Min                 -0.281473\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.418545\n",
      "evaluation/num steps total          269730\n",
      "evaluation/num paths total             270\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -4.55567e-06\n",
      "evaluation/Rewards Std                   5.73167e-06\n",
      "evaluation/Rewards Max                  -5.11591e-14\n",
      "evaluation/Rewards Min                  -3.87036e-05\n",
      "evaluation/Returns Mean                 -0.00455112\n",
      "evaluation/Returns Std                   0.0019747\n",
      "evaluation/Returns Max                  -0.00200201\n",
      "evaluation/Returns Min                  -0.00691403\n",
      "evaluation/Actions Mean                 -0.00416567\n",
      "evaluation/Actions Std                   0.00531074\n",
      "evaluation/Actions Max                   0.0129502\n",
      "evaluation/Actions Min                  -0.0196732\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.00455112\n",
      "time/data storing (s)                    0.00566994\n",
      "time/evaluation sampling (s)             7.92103\n",
      "time/exploration sampling (s)            1.7856\n",
      "time/logging (s)                         0.0707352\n",
      "time/sac training (s)                   25.4926\n",
      "time/saving (s)                          0.00690123\n",
      "time/training (s)                        5.36647e-05\n",
      "time/epoch (s)                          35.2826\n",
      "time/total (s)                        2017.85\n",
      "Epoch                                   53\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:54:22.725125 UTC | [] Epoch 54 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   56000\n",
      "trainer/num train calls              55000\n",
      "trainer/QF1 Loss                         3.32506e-06\n",
      "trainer/QF2 Loss                         2.27503e-06\n",
      "trainer/Policy Loss                     -0.213727\n",
      "trainer/Q1 Predictions Mean              0.20752\n",
      "trainer/Q1 Predictions Std               0.0451319\n",
      "trainer/Q1 Predictions Max               0.234495\n",
      "trainer/Q1 Predictions Min              -0.0333631\n",
      "trainer/Q2 Predictions Mean              0.207902\n",
      "trainer/Q2 Predictions Std               0.0449503\n",
      "trainer/Q2 Predictions Max               0.234225\n",
      "trainer/Q2 Predictions Min              -0.0284088\n",
      "trainer/Q Targets Mean                   0.208994\n",
      "trainer/Q Targets Std                    0.0451224\n",
      "trainer/Q Targets Max                    0.235577\n",
      "trainer/Q Targets Min                   -0.0300729\n",
      "trainer/Log Pis Mean                     0.992098\n",
      "trainer/Log Pis Std                      0.668383\n",
      "trainer/Log Pis Max                      1.61072\n",
      "trainer/Log Pis Min                     -2.15153\n",
      "trainer/policy/mean Mean                -0.0122051\n",
      "trainer/policy/mean Std                  0.0106336\n",
      "trainer/policy/mean Max                  0.0549472\n",
      "trainer/policy/mean Min                 -0.0367504\n",
      "trainer/policy/normal/std Mean           0.0945007\n",
      "trainer/policy/normal/std Std            0.00263796\n",
      "trainer/policy/normal/std Max            0.114308\n",
      "trainer/policy/normal/std Min            0.0797116\n",
      "trainer/policy/normal/log_std Mean      -2.35954\n",
      "trainer/policy/normal/log_std Std        0.0278365\n",
      "trainer/policy/normal/log_std Max       -2.16886\n",
      "trainer/policy/normal/log_std Min       -2.52934\n",
      "trainer/Alpha                            0.00166446\n",
      "trainer/Alpha Loss                      -0.0505582\n",
      "exploration/num steps total          56000\n",
      "exploration/num paths total            112\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000929938\n",
      "exploration/Rewards Std                  0.00131982\n",
      "exploration/Rewards Max                 -6.41739e-10\n",
      "exploration/Rewards Min                 -0.0122939\n",
      "exploration/Returns Mean                -0.464969\n",
      "exploration/Returns Std                  0.464746\n",
      "exploration/Returns Max                 -0.000222734\n",
      "exploration/Returns Min                 -0.929715\n",
      "exploration/Actions Mean                -0.0122253\n",
      "exploration/Actions Std                  0.0956552\n",
      "exploration/Actions Max                  0.332719\n",
      "exploration/Actions Min                 -0.350627\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.464969\n",
      "evaluation/num steps total          274725\n",
      "evaluation/num paths total             275\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.84866e-05\n",
      "evaluation/Rewards Std                   9.14889e-06\n",
      "evaluation/Rewards Max                  -1.10424e-09\n",
      "evaluation/Rewards Min                  -5.46515e-05\n",
      "evaluation/Returns Mean                 -0.0184681\n",
      "evaluation/Returns Std                   0.000717694\n",
      "evaluation/Returns Max                  -0.0175697\n",
      "evaluation/Returns Min                  -0.0193392\n",
      "evaluation/Actions Mean                 -0.0131103\n",
      "evaluation/Actions Std                   0.00360347\n",
      "evaluation/Actions Max                  -0.000105077\n",
      "evaluation/Actions Min                  -0.0233776\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0184681\n",
      "time/data storing (s)                    0.00567499\n",
      "time/evaluation sampling (s)             7.80602\n",
      "time/exploration sampling (s)            1.81575\n",
      "time/logging (s)                         0.0174289\n",
      "time/sac training (s)                   25.145\n",
      "time/saving (s)                          0.00656053\n",
      "time/training (s)                        4.52604e-05\n",
      "time/epoch (s)                          34.7965\n",
      "time/total (s)                        2054.66\n",
      "Epoch                                   54\n",
      "----------------------------------  ----------------\n",
      "2021-03-01 09:54:59.644199 UTC | [] Epoch 55 finished\n",
      "----------------------------------  ----------------\n",
      "replay_buffer/size                   57000\n",
      "trainer/num train calls              56000\n",
      "trainer/QF1 Loss                         1.97376e-06\n",
      "trainer/QF2 Loss                         1.3677e-06\n",
      "trainer/Policy Loss                     -0.190466\n",
      "trainer/Q1 Predictions Mean              0.187109\n",
      "trainer/Q1 Predictions Std               0.0452101\n",
      "trainer/Q1 Predictions Max               0.211474\n",
      "trainer/Q1 Predictions Min              -0.100662\n",
      "trainer/Q2 Predictions Mean              0.186145\n",
      "trainer/Q2 Predictions Std               0.0457165\n",
      "trainer/Q2 Predictions Max               0.21042\n",
      "trainer/Q2 Predictions Min              -0.0994154\n",
      "trainer/Q Targets Mean                   0.186169\n",
      "trainer/Q Targets Std                    0.0451013\n",
      "trainer/Q Targets Max                    0.210289\n",
      "trainer/Q Targets Min                   -0.0993869\n",
      "trainer/Log Pis Mean                     1.00299\n",
      "trainer/Log Pis Std                      0.673466\n",
      "trainer/Log Pis Max                      1.55365\n",
      "trainer/Log Pis Min                     -1.89058\n",
      "trainer/policy/mean Mean                -0.0114847\n",
      "trainer/policy/mean Std                  0.00774059\n",
      "trainer/policy/mean Max                  0.0532129\n",
      "trainer/policy/mean Min                 -0.0291699\n",
      "trainer/policy/normal/std Mean           0.0895567\n",
      "trainer/policy/normal/std Std            0.00202692\n",
      "trainer/policy/normal/std Max            0.107996\n",
      "trainer/policy/normal/std Min            0.0828244\n",
      "trainer/policy/normal/log_std Mean      -2.41313\n",
      "trainer/policy/normal/log_std Std        0.0222339\n",
      "trainer/policy/normal/log_std Max       -2.22566\n",
      "trainer/policy/normal/log_std Min       -2.49103\n",
      "trainer/Alpha                            0.00165331\n",
      "trainer/Alpha Loss                       0.0191187\n",
      "exploration/num steps total          57000\n",
      "exploration/num paths total            114\n",
      "exploration/path length Mean           500\n",
      "exploration/path length Std            499\n",
      "exploration/path length Max            999\n",
      "exploration/path length Min              1\n",
      "exploration/Rewards Mean                -0.000836377\n",
      "exploration/Rewards Std                  0.00114697\n",
      "exploration/Rewards Max                 -6.08921e-11\n",
      "exploration/Rewards Min                 -0.00831817\n",
      "exploration/Returns Mean                -0.418188\n",
      "exploration/Returns Std                  0.417784\n",
      "exploration/Returns Max                 -0.00040462\n",
      "exploration/Returns Min                 -0.835972\n",
      "exploration/Actions Mean                -0.0150441\n",
      "exploration/Actions Std                  0.0902078\n",
      "exploration/Actions Max                  0.286762\n",
      "exploration/Actions Min                 -0.288412\n",
      "exploration/Num Paths                    2\n",
      "exploration/Average Returns             -0.418188\n",
      "evaluation/num steps total          279720\n",
      "evaluation/num paths total             280\n",
      "evaluation/path length Mean            999\n",
      "evaluation/path length Std               0\n",
      "evaluation/path length Max             999\n",
      "evaluation/path length Min             999\n",
      "evaluation/Rewards Mean                 -1.70261e-05\n",
      "evaluation/Rewards Std                   1.12548e-05\n",
      "evaluation/Rewards Max                   0\n",
      "evaluation/Rewards Min                  -5.65513e-05\n",
      "evaluation/Returns Mean                 -0.0170091\n",
      "evaluation/Returns Std                   0.000522704\n",
      "evaluation/Returns Max                  -0.0165798\n",
      "evaluation/Returns Min                  -0.0180296\n",
      "evaluation/Actions Mean                 -0.0121239\n",
      "evaluation/Actions Std                   0.00482427\n",
      "evaluation/Actions Max                   0.00226789\n",
      "evaluation/Actions Min                  -0.0237805\n",
      "evaluation/Num Paths                     5\n",
      "evaluation/Average Returns              -0.0170091\n",
      "time/data storing (s)                    0.0059111\n",
      "time/evaluation sampling (s)             7.97906\n",
      "time/exploration sampling (s)            1.83187\n",
      "time/logging (s)                         0.0175668\n",
      "time/sac training (s)                   24.9154\n",
      "time/saving (s)                          0.00648873\n",
      "time/training (s)                        4.68604e-05\n",
      "time/epoch (s)                          34.7563\n",
      "time/total (s)                        2091.57\n",
      "Epoch                                   55\n",
      "----------------------------------  ----------------\n"
     ]
    }
   ],
   "source": [
    "variant = dict(\n",
    "        algorithm=\"SAC\",\n",
    "        version=\"normal\",\n",
    "        layer_size=256,\n",
    "        replay_buffer_size=int(1E6),\n",
    "        algorithm_kwargs=dict(\n",
    "            num_epochs=10,#3000,\n",
    "            num_eval_steps_per_epoch=5000,\n",
    "            num_trains_per_train_loop=1000,\n",
    "            num_expl_steps_per_train_loop=1000,\n",
    "            min_num_steps_before_training=1000,\n",
    "            max_path_length=1000,\n",
    "            batch_size=256,\n",
    "        ),\n",
    "        trainer_kwargs=dict(\n",
    "            discount=0.99,\n",
    "            soft_target_tau=5e-3,\n",
    "            target_update_period=1,\n",
    "            policy_lr=3E-4,\n",
    "            qf_lr=3E-4,\n",
    "            reward_scale=1,\n",
    "            use_automatic_entropy_tuning=True,\n",
    "        ),\n",
    "    )\n",
    "setup_logger('example-sac-car', variant=variant,log_dir=\"./rlkit_out/\")\n",
    "ptu.set_gpu_mode(True)  # optionally set the GPU (default=False)\n",
    "experiment(variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
